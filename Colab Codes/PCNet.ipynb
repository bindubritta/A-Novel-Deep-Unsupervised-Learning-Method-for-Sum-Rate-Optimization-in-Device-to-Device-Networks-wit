{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**2. Codes for analyzing the PCNet/PCNet+ model**"
      ],
      "metadata": {
        "id": "dp0eV7rtKFFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 PCNet Model: For training with a given background noise power**"
      ],
      "metadata": {
        "id": "StPhFrl8kYQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Reference\n",
        "# F. Liang, C. Shen, W. Yu, and F. Wu, “Towards optimal power control via\n",
        "# ensembling deep neural networks,” IEEE Transactions on Communications, vol. 68,\n",
        "# no. 3, pp. 1760–1776, 2020, doi: https://doi.org/10.1109/TCOMM.2019.2957482."
      ],
      "metadata": {
        "id": "4Dp-WDKoKX8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Number of transmitter-receiver pairs\n",
        "K = 5\n",
        "\n",
        "## Variances for noise signals\n",
        "sigma_sqr_noise = np.array([1e-0, 1e-0, 1e-0, 1e-0, 1e-0], dtype = float)\n",
        "\n",
        "## Minimum rate for the achievable SINR of multiple concurrent transmissions\n",
        "SINR_P_min = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype = float)\n",
        "\n",
        "## Maximum transmit power\n",
        "p_max = 1.0"
      ],
      "metadata": {
        "id": "s7p8E3GvOucd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading a CSV file (F_H_2D.csv) for feasible H matrices that was uploaded to\n",
        "## Google Collab's session storage.\n",
        "from numpy import loadtxt\n",
        "\n",
        "## Reading an array from the file\n",
        "F_H_2D_L = np.loadtxt('F_H_2D.csv', delimiter = ',', dtype = str)\n",
        "\n",
        "## Reshaping the array from 2D to 3D\n",
        "F_H_3D = F_H_2D_L.reshape(F_H_2D_L.shape[0], F_H_2D_L.shape[1] // K, K)\n",
        "F_H_3D_size = F_H_3D.shape[0]"
      ],
      "metadata": {
        "id": "6wSz2ELCoUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting string data to complex data and removing the initial whitespace\n",
        "F_H_list = []\n",
        "for k in range(F_H_3D_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    for j in range(K):  # Total columns\n",
        "      F_H_temp = complex(F_H_3D[k][i][j].strip())\n",
        "      F_H_list.append(F_H_temp)\n",
        "F_H_array = np.array(F_H_list)\n",
        "F_H = F_H_array.reshape((F_H_3D_size, K, K)) # H_size X row X column_count\n",
        "print(F_H.shape)\n",
        "F_H_size = F_H.shape[0]\n",
        "# print(F_H)"
      ],
      "metadata": {
        "id": "GBpEySUjofeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de42c54-3fef-40e4-8058-853e80c62ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba as nb\n",
        "\n",
        "## Function to compute the square of the absolute value of an array of complex numbers\n",
        "@nb.vectorize([nb.float64(nb.complex128),nb.float32(nb.complex64)])\n",
        "def cmplx_abs_sqr(cmplx_var):\n",
        "  return cmplx_var.real**2 + cmplx_var.imag**2"
      ],
      "metadata": {
        "id": "uMG7e1joA3JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the matrix A (K x K)\n",
        "def generate_A(F_H_size, K, SINR_P_min, F_H):\n",
        "  Aij_list = []\n",
        "  F_H_abs_sqr = cmplx_abs_sqr(F_H)\n",
        "\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      Aj_list =[]\n",
        "      for j in range(K): # Total columns\n",
        "        if i==j:\n",
        "          A = F_H_abs_sqr[k,i,j]\n",
        "        else:\n",
        "          A = np.multiply(-SINR_P_min[i], F_H_abs_sqr[k,i,j])\n",
        "        Aj_list.append(A)\n",
        "      Aij_list.append(Aj_list)\n",
        "  Aij_array = np.array(Aij_list)\n",
        "  Aij = Aij_array.reshape((F_H_size, K, K)) # H_size X row X column\n",
        "  return Aij"
      ],
      "metadata": {
        "id": "3F5C3yW2EqIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A\n",
        "A = generate_A(F_H_size, K, SINR_P_min, F_H)\n",
        "print(A.shape)\n",
        "# print(A)"
      ],
      "metadata": {
        "id": "vpAMgcBJErlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e659eb-a197-4551-adfd-0c1f6641f8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the vector b (K x 1)\n",
        "def generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H):\n",
        "  bi_list = []\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows, i.e., total transmitters\n",
        "      b = np.multiply(SINR_P_min[i], sigma_sqr_noise[i])\n",
        "      bi_list.append(b)\n",
        "  bi_array = np.array(bi_list)\n",
        "  bi = bi_array.reshape((F_H_size, K, 1)) # H_size X row X column\n",
        "  return bi"
      ],
      "metadata": {
        "id": "gI9bSsL3GDai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create vector b\n",
        "b = generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H)\n",
        "print(b.shape)\n",
        "# print(b)"
      ],
      "metadata": {
        "id": "oJ207qN9GEdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c94f98-aaa9-496e-df15-11ceb1f54d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A_inv, i.e., the pseudo inverse of matrix A\n",
        "A_inv = np.linalg.pinv(A)\n",
        "print(A_inv.shape)\n",
        "# print(A_inv)"
      ],
      "metadata": {
        "id": "MIIDTUBlGJGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03e4b31-40a4-4296-86b3-9e5f8634cac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector p_hat = (A_inv x b)\n",
        "p_hat = np.matmul(A_inv, b)\n",
        "print(p_hat.shape)\n",
        "# print(p_hat)"
      ],
      "metadata": {
        "id": "GEuukR7BGMG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505bf3a4-cbc6-4de7-d5ef-ba52b6a560d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Function to compute the square of the absolute value of an array of complex numbers\n",
        "# import numba as nb\n",
        "\n",
        "# @nb.vectorize([nb.float64(nb.complex128),nb.float32(nb.complex64)])\n",
        "# def cmplx_abs_sqr(cmplx_var):\n",
        "#   return cmplx_var.real**2 + cmplx_var.imag**2"
      ],
      "metadata": {
        "id": "Fx9vomtzPuQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split datasets for training, validation, and testing.\n",
        "\n",
        "def split(np_array):\n",
        "  # data_size = np_array.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  # test_e_indx = valid_e_indx + test_data_size - 2\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_array.shape[1]\n",
        "  column_count = np_array.shape[2]\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = complex, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_array[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_array[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_array[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "_sRYFgyasz-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split F_H Matrix\n",
        "F_H_S = split(F_H)\n",
        "train_input_F_H = F_H_S[0]\n",
        "valid_input_F_H = F_H_S[1]\n",
        "test_input_F_H = F_H_S[2]\n",
        "test_data_F_H = F_H_S[3]"
      ],
      "metadata": {
        "id": "82SQ9k2Es62S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcf6ad1-1acb-4fcd-8368-f9f8299850d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 5) (25000, 5, 5) (25000, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split p_hat Matrix\n",
        "p_hat_S = split(p_hat)\n",
        "train_input_p_hat = p_hat_S[0]\n",
        "valid_input_p_hat = p_hat_S[1]\n",
        "test_input_p_hat = p_hat_S[2]\n",
        "test_data_p_hat = p_hat_S[3]"
      ],
      "metadata": {
        "id": "HOTyn5Klb7I1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb2a96f-82d1-42d5-9cab-e9b572475d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 1) (25000, 5, 1) (25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the DNN model - The Sequential model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "## from tensorflow.keras import layers # shows warning\n",
        "from keras.api._v2.keras import layers\n",
        "\n",
        "model = keras.Sequential(name = \"sequential_model\")\n",
        "\n",
        "model.add(keras.Input(shape = (K,K), name = \"hij_inputs\"))\n",
        "model.add(layers.Flatten(name = \"flatten_layer_hij\"))\n",
        "\n",
        "model.add(layers.Dense(units = 2*K*K, activation = 'relu', input_shape = (K*K,), name = \"dense_layer_1\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(units = K*K, activation = 'relu', input_shape = (2*K*K,), name = \"dense_layer_2\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(units = K, activation = 'sigmoid', input_shape = (K*K,), name = \"P_hat\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sKd9OWCctGhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fea647c-540d-47bd-9f57-acd07320a327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_layer_hij (Flatten)  (None, 25)               0         \n",
            "                                                                 \n",
            " dense_layer_1 (Dense)       (None, 50)                1300      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 50)               200       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_layer_2 (Dense)       (None, 25)                1275      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 25)               100       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " P_hat (Dense)               (None, 5)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,005\n",
            "Trainable params: 2,855\n",
            "Non-trainable params: 150\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the model as a graph\n",
        "# keras.utils.plot_model(model, \"Sequential_Model.png\")"
      ],
      "metadata": {
        "id": "FOA7BGUlti-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Display the input and output shapes of each layer\n",
        "# keras.utils.plot_model(model, \"Sequential_Model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "vWcXGiwttoGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert sigma_sqr_noise from numpy array to tensor\n",
        "sigma_sqr_noise_t = tf.convert_to_tensor(sigma_sqr_noise, dtype = float)\n",
        "tf.print(sigma_sqr_noise_t)"
      ],
      "metadata": {
        "id": "9EepXpjhASWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a827e114-c530-4a7e-b90c-2ac147f43955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert SINR_P_min from numpy array to tensor\n",
        "SINR_P_min_t = tf.convert_to_tensor(SINR_P_min, dtype = float)\n",
        "tf.print(SINR_P_min_t)"
      ],
      "metadata": {
        "id": "e62igGZlH2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9470c82-0086-419c-ae74-e05300ac09e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5 0.5 0.5 0.5 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The customized loss function that penalizes the constraint violation\n",
        "def custom_loss(y_true, y_pred):\n",
        "  p = tf.math.multiply(p_max, y_pred)\n",
        "  hij = tf.reshape(y_true[:,0:K*K], (-1,K,K))\n",
        "  hij_abs_sqr = tf.math.square(tf.math.abs(hij))\n",
        "\n",
        "  lambda_l = 5.0\n",
        "  R_P = 0.0\n",
        "  pnlty_f_CV = 0.0\n",
        "\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0.0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = tf.math.multiply(p[:,j], hij_abs_sqr[:,i,j])\n",
        "      ph = tf.math.add(ph, ph_j)\n",
        "\n",
        "    numr = tf.math.multiply(p[:,i], hij_abs_sqr[:,i,i])\n",
        "    dnumr = tf.math.add(sigma_sqr_noise_t[i], tf.math.subtract(ph, numr))\n",
        "    SINR_i = tf.math.divide(numr, dnumr)\n",
        "    R_P = tf.math.add(R_P, (tf.math.log(1 + SINR_i)/tf.math.log(2.0)))\n",
        "    pnlty_f_CV = tf.math.add(pnlty_f_CV,\n",
        "                             tf.nn.relu((tf.math.log(1 + SINR_P_min_t[i])/tf.math.log(2.0))\n",
        "                                      - (tf.math.log(1 + SINR_i)/tf.math.log(2.0))))\n",
        "\n",
        "  loss = tf.math.add(-R_P, tf.math.multiply(lambda_l, pnlty_f_CV))\n",
        "  loss = tf.reduce_mean(loss) # batch mean\n",
        "  return loss"
      ],
      "metadata": {
        "id": "uueHZcGyty_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build and compile the DNN model\n",
        "## Training and Testing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "optA = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer = optA, loss = custom_loss)\n",
        "\n",
        "history = model.fit(train_input_F_H, train_input_F_H, epochs = 50, validation_data = (valid_input_F_H, valid_input_F_H), batch_size = 1000)\n",
        "\n",
        "plt.plot(history.epoch, history.history['loss'], color = \"blue\", label = \"Training\")\n",
        "plt.plot(history.epoch, history.history['val_loss'], color=\"black\", label = \"Validation\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VFP_5h2wt3ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0ac4f4-343c-45f3-f63b-d507a2accc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "200/200 [==============================] - 4s 8ms/step - loss: -0.7100 - val_loss: -1.4531\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -1.4159 - val_loss: -1.5933\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -1.8481 - val_loss: -1.8802\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.1470 - val_loss: -2.1737\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.3679 - val_loss: -2.3936\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.5343 - val_loss: -2.5555\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.6622 - val_loss: -2.6864\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.7649 - val_loss: -2.7872\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.8499 - val_loss: -2.8680\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.9217 - val_loss: -2.9445\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -2.9828 - val_loss: -3.0046\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.0342 - val_loss: -3.0561\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.0771 - val_loss: -3.1004\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.1131 - val_loss: -3.1299\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.1430 - val_loss: -3.1619\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.1678 - val_loss: -3.1808\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.1887 - val_loss: -3.2008\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 1s 5ms/step - loss: -3.2063 - val_loss: -3.2169\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2210 - val_loss: -3.2294\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 1s 5ms/step - loss: -3.2337 - val_loss: -3.2401\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2445 - val_loss: -3.2503\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2539 - val_loss: -3.2587\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2622 - val_loss: -3.2654\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2697 - val_loss: -3.2723\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2761 - val_loss: -3.2786\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2820 - val_loss: -3.2831\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 1s 5ms/step - loss: -3.2870 - val_loss: -3.2878\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2917 - val_loss: -3.2921\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.2961 - val_loss: -3.2964\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3000 - val_loss: -3.2992\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3033 - val_loss: -3.3023\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3066 - val_loss: -3.3063\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3094 - val_loss: -3.3093\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3121 - val_loss: -3.3113\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3146 - val_loss: -3.3146\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3170 - val_loss: -3.3170\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 1s 5ms/step - loss: -3.3190 - val_loss: -3.3188\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3210 - val_loss: -3.3203\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3227 - val_loss: -3.3221\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3244 - val_loss: -3.3239\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3259 - val_loss: -3.3255\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3274 - val_loss: -3.3267\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3287 - val_loss: -3.3276\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3299 - val_loss: -3.3294\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3310 - val_loss: -3.3304\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3321 - val_loss: -3.3315\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3330 - val_loss: -3.3322\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3339 - val_loss: -3.3334\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3348 - val_loss: -3.3342\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 1s 4ms/step - loss: -3.3356 - val_loss: -3.3350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability and\n",
        "## finding indexes of test_input_F_H matrix with the hij set that do not satisfy\n",
        "## constraint on the minimum SINR_P_min rate but satisfy the maximum transmit\n",
        "## power p_max\n",
        "\n",
        "output_P_hat_temp = p_max * model.predict(test_input_F_H)\n",
        "output_P_hat = output_P_hat_temp.reshape((output_P_hat_temp.shape[0], output_P_hat_temp.shape[1], 1)) # test_input_F_H_size X row X column\n",
        "output_P_hat_size = output_P_hat.shape[0]\n",
        "test_data_F_H_abs_sqr = cmplx_abs_sqr(test_data_F_H)\n",
        "\n",
        "indx_n = []\n",
        "count_v = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(output_P_hat[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(output_P_hat[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise[i] + ph - numr\n",
        "    SINR_out = np.divide(numr, dnumr)\n",
        "    if np.round(SINR_out, decimals = 3) < SINR_P_min[i]:\n",
        "      indx_n.append(k)\n",
        "      count_v = count_v + 1\n",
        "      # print(SINR_out)\n",
        "      break\n",
        "\n",
        "violation_prb = (count_v / output_P_hat_size) * 100\n",
        "print(\"Constraints Violation Probability: {:.2f}%\".format(violation_prb))\n",
        "# print(len(indx_n))\n",
        "# print(indx_n)"
      ],
      "metadata": {
        "id": "TppB89ypt7gC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ac3478-e146-4d50-8dd5-da5bc66a440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 1ms/step\n",
            "Constraints Violation Probability: 80.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to calculate the average sum rate\n",
        "# Here, p_model is the output of DNN, and it is a 2D array.\n",
        "import math\n",
        "\n",
        "def average_sum_rate(hij, p_model, sigma_sqr_noise, K):\n",
        "  R = 0\n",
        "  hij_size = hij.shape[0]\n",
        "  hij_abs_sqr = cmplx_abs_sqr(hij)\n",
        "\n",
        "  for k in range(hij_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      phn = 0\n",
        "      for j in range(K):  # Total columns\n",
        "        phn_j = np.multiply(p_model[k,j], hij_abs_sqr[k,i,j])\n",
        "        phn = phn + phn_j\n",
        "\n",
        "      numr_s = np.multiply(p_model[k,i], hij_abs_sqr[k,i,i])\n",
        "      dnumr_s = sigma_sqr_noise[i] + phn - numr_s\n",
        "      R_temp = math.log2(1 + np.divide(numr_s, dnumr_s))\n",
        "      R = R + R_temp\n",
        "\n",
        "  return (R/hij_size)"
      ],
      "metadata": {
        "id": "Sj6wlXCWuAgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the curated power vector p_tilda\n",
        "# p_tilda = test_input_p_hat when SINR_P_min is not met\n",
        "# p_tilda = output_P_hat when SINR_P_min is met\n",
        "\n",
        "p_tilda = np.empty((output_P_hat_size, K, 1), dtype = float, order = 'C')\n",
        "\n",
        "i = 0\n",
        "for j in range(output_P_hat_size):\n",
        "  if (i < len(indx_n)) and (j == indx_n[i]):\n",
        "    p_tilda[j] = (test_input_p_hat[j] * p_max) / np.amax(test_input_p_hat[j])\n",
        "    i = i + 1\n",
        "  else:\n",
        "    p_tilda[j] = output_P_hat[j]\n",
        "\n",
        "print(p_tilda.shape)\n",
        "# print(p_tilda)"
      ],
      "metadata": {
        "id": "F6Jg6f6qr1AE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f151bbf9-df2f-4b64-94b8-7922592392c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking p_tilda, i.e., the power for test_data_F_H for negative values\n",
        "## and Hit Rate i.e. percentage for 0 <= p_tilda <= p_max\n",
        "count_p_t = 0\n",
        "count_n_t = 0\n",
        "\n",
        "for n in range(output_P_hat_size):\n",
        "  P_max = np.amax(p_tilda[n])\n",
        "  if np.round(P_max, decimals = 3) <= 1:\n",
        "    count_p_t = count_p_t + 1\n",
        "\n",
        "  if np.any(p_tilda[n] < 0):\n",
        "    count_n_t = count_n_t + 1\n",
        "    print(n,'\\n')\n",
        "    print(p_tilda)\n",
        "\n",
        "p_tilda_hit_rate = (count_p_t / output_P_hat_size) * 100\n",
        "print(\"Hit Rate for Power p_tilda: {:.2f}%\".format(p_tilda_hit_rate))\n",
        "print(\"Negative power count: \", count_n_t)"
      ],
      "metadata": {
        "id": "hYk4oLMZxSev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd53b20-748e-4e7c-c235-ac77ac8a986b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rate for Power p_tilda: 100.00%\n",
            "Negative power count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability for p_tilda on the SINR_P_min\n",
        "# indx_t = []\n",
        "count_v_t = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(p_tilda[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(p_tilda[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise[i] + ph - numr\n",
        "    SINR_out_t = np.divide(numr, dnumr)\n",
        "    # if k == 24463:\n",
        "    #   print(SINR_out_t)\n",
        "\n",
        "    if np.round(SINR_out_t, decimals = 2) < SINR_P_min[i]:\n",
        "      # indx_t.append(k)\n",
        "      count_v_t = count_v_t + 1\n",
        "      break\n",
        "\n",
        "violation_prb_t = (count_v_t / output_P_hat_size) * 100\n",
        "print(\"SINR_P_min Constraints Violation Probability for p_tilda: {:.2f}%\".format(violation_prb_t))"
      ],
      "metadata": {
        "id": "yM20Ni9dxK-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3fcc4e-6569-435b-9ba8-d523e71078f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SINR_P_min Constraints Violation Probability for p_tilda: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DNN Sum Rate for test_data_F_H\n",
        "sumrate_s_F_H = average_sum_rate(test_data_F_H, p_tilda, sigma_sqr_noise, K)\n",
        "print(\"Total Average Sum Rate for all H matrices: {:.3f} Bit/Second/Hertz\".format(sumrate_s_F_H))"
      ],
      "metadata": {
        "id": "UQvyTY2POnuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4fe784-7fe4-4c1e-8675-7ba83d95ba8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Average Sum Rate for all H matrices: 3.273 Bit/Second/Hertz\n"
          ]
        }
      ]
    }
  ]
}