{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3. Codes for analyzing the Proposed Model**"
      ],
      "metadata": {
        "id": "dp0eV7rtKFFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 For training with a given background noise power**"
      ],
      "metadata": {
        "id": "Qb9Rmt8Y-DLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Number of transmitter-receiver pairs\n",
        "K = 5\n",
        "\n",
        "## Variances for noise signals\n",
        "sigma_sqr_noise = np.array([1e-0, 1e-0, 1e-0, 1e-0, 1e-0], dtype = float)\n",
        "\n",
        "## Minimum rate for the achievable SINR of multiple concurrent transmissions\n",
        "SINR_P_min = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
        "\n",
        "## Maximum transmit power\n",
        "p_max = 1.0"
      ],
      "metadata": {
        "id": "s7p8E3GvOucd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading a NumPy array from a CSV file\n",
        "# Loading F_H array from a CSV file\n",
        "from numpy import loadtxt\n",
        "\n",
        "## Reading an array from the file\n",
        "# If we want to read a file from our local drive, we have to first upload it to Collab's session storage.\n",
        "F_H_2D_L = np.loadtxt('F_H_2D.csv', delimiter = ',', dtype = str)\n",
        "\n",
        "## Reshaping the array from 2D to 3D\n",
        "F_H_3D = F_H_2D_L.reshape(F_H_2D_L.shape[0], F_H_2D_L.shape[1] // K, K)\n",
        "F_H_3D_size = F_H_3D.shape[0]"
      ],
      "metadata": {
        "id": "6wSz2ELCoUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting string data to complex data and removing the initial whitespace\n",
        "F_H_list = []\n",
        "for k in range(F_H_3D_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    for j in range(K):  # Total columns\n",
        "      F_H_temp = complex(F_H_3D[k][i][j].strip())\n",
        "      F_H_list.append(F_H_temp)\n",
        "F_H_array = np.array(F_H_list)\n",
        "F_H = F_H_array.reshape((F_H_3D_size, K, K)) # H_size X row X column_count\n",
        "print(F_H.shape)\n",
        "F_H_size = F_H.shape[0]\n",
        "# print(F_H)"
      ],
      "metadata": {
        "id": "GBpEySUjofeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5a1c6c-aa3f-4409-cbe6-c9ab4ded214b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba as nb\n",
        "\n",
        "## Function to compute the square of the absolute value of an array of complex numbers\n",
        "@nb.vectorize([nb.float64(nb.complex128),nb.float32(nb.complex64)])\n",
        "def cmplx_abs_sqr(cmplx_var):\n",
        "  return cmplx_var.real**2 + cmplx_var.imag**2"
      ],
      "metadata": {
        "id": "Fx9vomtzPuQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the matrix A (K x K)\n",
        "def generate_A(F_H_size, K, SINR_P_min, F_H):\n",
        "  Aij_list = []\n",
        "  F_H_abs_sqr = cmplx_abs_sqr(F_H)\n",
        "\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      Aj_list =[]\n",
        "      for j in range(K): # Total columns\n",
        "        if i==j:\n",
        "          A = F_H_abs_sqr[k,i,j]\n",
        "        else:\n",
        "          A = np.multiply(-SINR_P_min[i], F_H_abs_sqr[k,i,j])\n",
        "        Aj_list.append(A)\n",
        "      Aij_list.append(Aj_list)\n",
        "  Aij_array = np.array(Aij_list)\n",
        "  Aij = Aij_array.reshape((F_H_size, K, K)) # H_size X row X column\n",
        "  return Aij"
      ],
      "metadata": {
        "id": "L8cJY-gXhM1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A\n",
        "A = generate_A(F_H_size, K, SINR_P_min, F_H)\n",
        "print(A.shape)\n",
        "# print(A)"
      ],
      "metadata": {
        "id": "n84VL_CUhQVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b6aff4-2e31-4ee0-b873-efed3e2adfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the vector b (K x 1)\n",
        "def generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H):\n",
        "  bi_list = []\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows, i.e., total transmitters\n",
        "      b = np.multiply(SINR_P_min[i], sigma_sqr_noise[i])\n",
        "      bi_list.append(b)\n",
        "  bi_array = np.array(bi_list)\n",
        "  bi = bi_array.reshape((F_H_size, K, 1)) # H_size X row X column\n",
        "  return bi"
      ],
      "metadata": {
        "id": "-6-F2dBOhZrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create vector b\n",
        "b = generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H)\n",
        "print(b.shape)\n",
        "# print(b)"
      ],
      "metadata": {
        "id": "zybS1lEDhaoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fd8eb7-8e06-4647-e1fb-e3f9bb6fd7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A_inv, i.e., the pseudo inverse of matrix A\n",
        "A_inv = np.linalg.pinv(A)\n",
        "A_inv[A_inv<0] = 0\n",
        "print(A_inv.shape)\n",
        "# print(A_inv)"
      ],
      "metadata": {
        "id": "2UKeHq34hgj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd6647a-5ec5-4828-a866-143e523740c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector p_hat = (A_inv x b)\n",
        "p_hat = np.matmul(A_inv, b)\n",
        "print(p_hat.shape)\n",
        "# print(p_hat)"
      ],
      "metadata": {
        "id": "eaoUS6EFwQGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce55e8e1-2cac-4c12-c3fe-d630b90de69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert p_max_array to (K x 1) vector\n",
        "p_max_array = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype = float)\n",
        "p_max_vector = p_max_array.reshape((K, 1)) # row X column\n",
        "print(p_max_vector)"
      ],
      "metadata": {
        "id": "uBRgwNBgx192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94f44d7-1353-4b16-df52-67c7b10368d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector X = (p_max_vector - p_hat)\n",
        "X = p_max_vector - p_hat\n",
        "print(X.shape)\n",
        "# print(X)"
      ],
      "metadata": {
        "id": "jKEF6bOyrU-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5571abd6-4cd8-42e8-c5df-0bab6f5ce2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector beta = MIN[(p_max_vector - p_hat) / A_inv_cv]\n",
        "beta_list = []\n",
        "\n",
        "for k in range(F_H_size):\n",
        "  for i in range(K):  # Total columns\n",
        "    ak = A_inv[k,:,i]\n",
        "    akr = ak.reshape((K, 1)) # row X column\n",
        "    with np.errstate(divide='ignore'):\n",
        "      beta_w = np.where(akr != 0.0, np.divide(X[k], akr), np.inf)\n",
        "      # beta_w = np.divide(X[k], akr)\n",
        "    # beta_w = np.divide(X[k], akr)\n",
        "    beta_min = np.amin(beta_w)\n",
        "    beta_list.append(beta_min)\n",
        "\n",
        "beta_array = np.array(beta_list)\n",
        "beta = beta_array.reshape((F_H_size, K, 1)) # H_size X row X column_count\n",
        "print(beta.shape)\n",
        "beta_size = beta.shape[0]\n",
        "# print(beta)"
      ],
      "metadata": {
        "id": "0yYHOU4Voal2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e2c6ef-86f6-40b7-df1b-0c6401d4277a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250005, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split datasets for training, validation, and testing.\n",
        "def split(np_array):\n",
        "  # data_size = np_array.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_array.shape[1]\n",
        "  column_count = np_array.shape[2]\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = complex, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_array[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_array[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_array[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "_sRYFgyasz-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split F_H matrix\n",
        "F_H_S = split(F_H)\n",
        "train_input_F_H = F_H_S[0]\n",
        "valid_input_F_H = F_H_S[1]\n",
        "test_input_F_H = F_H_S[2]\n",
        "test_data_F_H = F_H_S[3]"
      ],
      "metadata": {
        "id": "82SQ9k2Es62S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e995ae7a-f259-40b8-fc18-4a8ab2e80215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 5) (25000, 5, 5) (25000, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split A_inv matrix\n",
        "A_inv_S = split(A_inv)\n",
        "train_input_A_inv = A_inv_S[0]\n",
        "valid_input_A_inv = A_inv_S[1]\n",
        "test_input_A_inv = A_inv_S[2]\n",
        "test_data_A_inv = A_inv_S[3]"
      ],
      "metadata": {
        "id": "zYDqkyj4hrvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f87236-a74b-4965-d126-288929b0fb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 5) (25000, 5, 5) (25000, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split b vector\n",
        "b_S = split(b)\n",
        "train_input_b = b_S[0]\n",
        "valid_input_b = b_S[1]\n",
        "test_input_b = b_S[2]\n",
        "test_data_b = b_S[3]"
      ],
      "metadata": {
        "id": "0VFev8m6hure",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11dc1ee-520e-40ed-fc4e-0eeadd994aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 1) (25000, 5, 1) (25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split X vector\n",
        "X_S = split(X)\n",
        "train_input_X = X_S[0]\n",
        "valid_input_X = X_S[1]\n",
        "test_input_X = X_S[2]\n",
        "test_data_X = X_S[3]"
      ],
      "metadata": {
        "id": "aHOqCgops5TZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cc17c8-4eb1-48dc-8804-b3b2affc6d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 1) (25000, 5, 1) (25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split beta vector\n",
        "beta_S = split(beta)\n",
        "train_input_beta = beta_S[0]\n",
        "valid_input_beta = beta_S[1]\n",
        "test_input_beta = beta_S[2]\n",
        "test_data_beta = beta_S[3]"
      ],
      "metadata": {
        "id": "7z4m623F17X6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9c55dc-500f-4b84-859c-f84557024d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 1) (25000, 5, 1) (25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split p_hat vector\n",
        "p_hat_S = split(p_hat)\n",
        "train_input_p_hat = p_hat_S[0]\n",
        "valid_input_p_hat = p_hat_S[1]\n",
        "test_input_p_hat = p_hat_S[2]\n",
        "test_data_p_hat = p_hat_S[3]"
      ],
      "metadata": {
        "id": "vjStduzPwW-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552fdc1d-86d7-4f17-d8ea-91dd1bfed6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 5, 1) (25000, 5, 1) (25000, 5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the DNN model - The Functional API\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "## from tensorflow.keras import layers # shows warning\n",
        "from keras.api._v2.keras import layers\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "hij_inputs = keras.Input(shape=(K,K), name = \"hij_inputs\")\n",
        "f1 = layers.Flatten(name = \"flatten_layer_hij\")(hij_inputs)\n",
        "\n",
        "d1 = layers.Dense(2*K*K, activation=\"relu\", name = \"dense_layer_1\")(f1)\n",
        "b1 = layers.BatchNormalization(name = \"batch_norm_layer_1\")(d1)\n",
        "\n",
        "d2 = layers.Dense(K*K, activation=\"relu\", name = \"dense_layer_2\")(b1)\n",
        "b2 = layers.BatchNormalization(name = \"batch_norm_layer_2\")(d2)\n",
        "\n",
        "# meu = layers.Dense(K, activation=\"relu\", name = \"meu\")(b2)\n",
        "meu = layers.Dense(K, activation=\"sigmoid\", name = \"meu\")(b2)\n",
        "\n",
        "A_inv_inputs = keras.Input(shape=(K,K), name = \"A_inv_inputs\")\n",
        "f2 = layers.Flatten(name = \"flatten_layer_A_inv\")(A_inv_inputs)\n",
        "\n",
        "X_inputs = keras.Input(shape=(K,1), name = \"X_inputs\")\n",
        "f3 = layers.Flatten(name = \"flatten_layer_X\")(X_inputs)\n",
        "\n",
        "beta_inputs = keras.Input(shape=(K,1), name = \"beta_inputs\")\n",
        "f4 = layers.Flatten(name = \"flatten_layer_beta\")(beta_inputs)\n",
        "\n",
        "p_hat_inputs = keras.Input(shape=(K,1), name = \"p_hat_inputs\")\n",
        "f5 = layers.Flatten(name = \"flatten_layer_p_hat\")(p_hat_inputs)\n",
        "\n",
        "def custom_layer(tensor):\n",
        "  t_A_inv = tensor[0]\n",
        "  t_X = tensor[1]\n",
        "  t_beta = tensor[2]\n",
        "  t_p_hat = tensor[3]\n",
        "  t_meu = tensor[4]\n",
        "\n",
        "  A_inv_cl = tf.reshape(t_A_inv[:,0:K*K], (-1,K,K))\n",
        "  X_cl = tf.reshape(t_X[:,0:K*1], (-1,K,1))\n",
        "  beta_cl = tf.reshape(t_beta[:,0:K*1], (-1,K,1))\n",
        "  p_hat_cl = tf.reshape(t_p_hat[:,0:K*1], (-1,K,1))\n",
        "  meu_cl = tf.reshape(t_meu[:,0:K*1], (-1,K,1))\n",
        "\n",
        "  meu_ewm = tf.math.multiply(beta_cl, meu_cl)\n",
        "\n",
        "  alpha_dnumr = tf.matmul(A_inv_cl, meu_ewm)\n",
        "  alpha_whole = tf.divide(X_cl, alpha_dnumr)\n",
        "  alpha = tf.reduce_min(alpha_whole, axis = 1, keepdims = True)\n",
        "  max_p = tf.constant([1.0])\n",
        "  alpha = tf.math.minimum(max_p, alpha)\n",
        "  meu_P = tf.multiply(meu_ewm, alpha)\n",
        "\n",
        "  Z_cl = tf.matmul(A_inv_cl, meu_P)\n",
        "  P_hat_cl = tf.add(p_hat_cl, Z_cl)\n",
        "  P_hat_cl_Norm = tf.math.divide(P_hat_cl, tf.reduce_max(P_hat_cl, axis = 1, keepdims = True))\n",
        "\n",
        "  # return P_hat_cl\n",
        "  return P_hat_cl_Norm\n",
        "\n",
        "lambda_layer = tf.keras.layers.Lambda(custom_layer, name=\"lambda_layer\")([f2, f3, f4, f5, meu])\n",
        "f6 = layers.Flatten(name = \"flatten_layer_output\")(lambda_layer)\n",
        "\n",
        "model = keras.Model(inputs = [hij_inputs, A_inv_inputs, X_inputs, beta_inputs, p_hat_inputs], outputs = f6, name = \"functional_api\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EGjbOfESh0At",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cdd3f4-43d5-4bf1-f3af-d4748cdced0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_api\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " hij_inputs (InputLayer)        [(None, 5, 5)]       0           []                               \n",
            "                                                                                                  \n",
            " flatten_layer_hij (Flatten)    (None, 25)           0           ['hij_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " dense_layer_1 (Dense)          (None, 50)           1300        ['flatten_layer_hij[0][0]']      \n",
            "                                                                                                  \n",
            " batch_norm_layer_1 (BatchNorma  (None, 50)          200         ['dense_layer_1[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " dense_layer_2 (Dense)          (None, 25)           1275        ['batch_norm_layer_1[0][0]']     \n",
            "                                                                                                  \n",
            " A_inv_inputs (InputLayer)      [(None, 5, 5)]       0           []                               \n",
            "                                                                                                  \n",
            " X_inputs (InputLayer)          [(None, 5, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " beta_inputs (InputLayer)       [(None, 5, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " p_hat_inputs (InputLayer)      [(None, 5, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " batch_norm_layer_2 (BatchNorma  (None, 25)          100         ['dense_layer_2[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " flatten_layer_A_inv (Flatten)  (None, 25)           0           ['A_inv_inputs[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_layer_X (Flatten)      (None, 5)            0           ['X_inputs[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_layer_beta (Flatten)   (None, 5)            0           ['beta_inputs[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_layer_p_hat (Flatten)  (None, 5)            0           ['p_hat_inputs[0][0]']           \n",
            "                                                                                                  \n",
            " meu (Dense)                    (None, 5)            130         ['batch_norm_layer_2[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_layer (Lambda)          (None, 5, 1)         0           ['flatten_layer_A_inv[0][0]',    \n",
            "                                                                  'flatten_layer_X[0][0]',        \n",
            "                                                                  'flatten_layer_beta[0][0]',     \n",
            "                                                                  'flatten_layer_p_hat[0][0]',    \n",
            "                                                                  'meu[0][0]']                    \n",
            "                                                                                                  \n",
            " flatten_layer_output (Flatten)  (None, 5)           0           ['lambda_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,005\n",
            "Trainable params: 2,855\n",
            "Non-trainable params: 150\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Plot the model as a graph\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model.png\")"
      ],
      "metadata": {
        "id": "FOA7BGUlti-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Display the input and output shapes of each layer\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "vWcXGiwttoGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert sigma_sqr_noise from numpy array to tensor\n",
        "sigma_sqr_noise_t = tf.convert_to_tensor(sigma_sqr_noise, dtype = float)\n",
        "tf.print(sigma_sqr_noise_t)"
      ],
      "metadata": {
        "id": "j5V8r3SYAy_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5134b26e-d278-4344-dec8-ed750fac5701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The customized loss function\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  # p = y_pred\n",
        "  p = tf.math.multiply(p_max, y_pred)\n",
        "  hij = tf.reshape(y_true[:,0:K*K], (-1,K,K))\n",
        "  hij_abs_sqr = tf.math.square(tf.math.abs(hij))\n",
        "\n",
        "  R_P = 0.0\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0.0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = tf.math.multiply(p[:,j], hij_abs_sqr[:,i,j])\n",
        "      ph = tf.math.add(ph, ph_j)\n",
        "\n",
        "    numr = tf.math.multiply(p[:,i], hij_abs_sqr[:,i,i])\n",
        "    dnumr = tf.math.add(sigma_sqr_noise_t[i], tf.math.subtract(ph, numr))\n",
        "    SINR_i = tf.math.divide(numr, dnumr)\n",
        "    R_P = tf.math.add(R_P, (tf.math.log(1 + SINR_i)/tf.math.log(2.0)))\n",
        "\n",
        "  loss = -R_P\n",
        "  loss = tf.reduce_mean(loss) # batch mean\n",
        "  return loss"
      ],
      "metadata": {
        "id": "eNwToz_dVG56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build and compile the DNN model\n",
        "## Training and Testing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "optA = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer = optA, loss = custom_loss)\n",
        "\n",
        "train_input = [train_input_F_H, train_input_A_inv, train_input_X, train_input_beta, train_input_p_hat]\n",
        "valid_input = [valid_input_F_H, valid_input_A_inv, valid_input_X, valid_input_beta, valid_input_p_hat]\n",
        "\n",
        "history = model.fit(train_input, train_input_F_H, epochs = 50, validation_data = (valid_input, valid_input_F_H), batch_size = 1000)\n",
        "\n",
        "plt.plot(history.epoch, history.history['loss'], color = \"blue\", label = \"Training\")\n",
        "plt.plot(history.epoch, history.history['val_loss'], color=\"black\", label = \"Validation\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VFP_5h2wt3ri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9dd8370-bd51-4b8b-88d8-0030b7b1abb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "200/200 [==============================] - 7s 15ms/step - loss: -3.3391 - val_loss: -3.3461\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.3770 - val_loss: -3.3869\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4058 - val_loss: -3.4162\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4259 - val_loss: -3.4332\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4391 - val_loss: -3.4436\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4475 - val_loss: -3.4500\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4531 - val_loss: -3.4541\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4569 - val_loss: -3.4573\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4598 - val_loss: -3.4598\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4621 - val_loss: -3.4617\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4640 - val_loss: -3.4634\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4657 - val_loss: -3.4647\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4671 - val_loss: -3.4659\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4685 - val_loss: -3.4671\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4697 - val_loss: -3.4684\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4708 - val_loss: -3.4694\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4717 - val_loss: -3.4705\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4724 - val_loss: -3.4711\n",
            "Epoch 19/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4731 - val_loss: -3.4720\n",
            "Epoch 20/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4736 - val_loss: -3.4727\n",
            "Epoch 21/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4740 - val_loss: -3.4731\n",
            "Epoch 22/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4745 - val_loss: -3.4737\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 2s 12ms/step - loss: -3.4748 - val_loss: -3.4740\n",
            "Epoch 24/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4752 - val_loss: -3.4743\n",
            "Epoch 25/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4755 - val_loss: -3.4746\n",
            "Epoch 26/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4758 - val_loss: -3.4751\n",
            "Epoch 27/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4761 - val_loss: -3.4752\n",
            "Epoch 28/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4764 - val_loss: -3.4756\n",
            "Epoch 29/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4767 - val_loss: -3.4758\n",
            "Epoch 30/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4770 - val_loss: -3.4761\n",
            "Epoch 31/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4772 - val_loss: -3.4763\n",
            "Epoch 32/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4775 - val_loss: -3.4766\n",
            "Epoch 33/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4778 - val_loss: -3.4770\n",
            "Epoch 34/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4781 - val_loss: -3.4772\n",
            "Epoch 35/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4784 - val_loss: -3.4775\n",
            "Epoch 36/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4787 - val_loss: -3.4777\n",
            "Epoch 37/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4790 - val_loss: -3.4781\n",
            "Epoch 38/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4793 - val_loss: -3.4783\n",
            "Epoch 39/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4795 - val_loss: -3.4786\n",
            "Epoch 40/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4798 - val_loss: -3.4786\n",
            "Epoch 41/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4800 - val_loss: -3.4790\n",
            "Epoch 42/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4803 - val_loss: -3.4793\n",
            "Epoch 43/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4804 - val_loss: -3.4794\n",
            "Epoch 44/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4807 - val_loss: -3.4796\n",
            "Epoch 45/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4809 - val_loss: -3.4798\n",
            "Epoch 46/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4811 - val_loss: -3.4801\n",
            "Epoch 47/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4813 - val_loss: -3.4803\n",
            "Epoch 48/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4815 - val_loss: -3.4805\n",
            "Epoch 49/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4817 - val_loss: -3.4806\n",
            "Epoch 50/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4819 - val_loss: -3.4808\n",
            "Epoch 51/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4821 - val_loss: -3.4811\n",
            "Epoch 52/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4823 - val_loss: -3.4813\n",
            "Epoch 53/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4825 - val_loss: -3.4815\n",
            "Epoch 54/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4827 - val_loss: -3.4816\n",
            "Epoch 55/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4829 - val_loss: -3.4818\n",
            "Epoch 56/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4831 - val_loss: -3.4818\n",
            "Epoch 57/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4833 - val_loss: -3.4821\n",
            "Epoch 58/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4834 - val_loss: -3.4821\n",
            "Epoch 59/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4836 - val_loss: -3.4823\n",
            "Epoch 60/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4837 - val_loss: -3.4826\n",
            "Epoch 61/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4839 - val_loss: -3.4827\n",
            "Epoch 62/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4840 - val_loss: -3.4828\n",
            "Epoch 63/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4841 - val_loss: -3.4830\n",
            "Epoch 64/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4843 - val_loss: -3.4831\n",
            "Epoch 65/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4844 - val_loss: -3.4832\n",
            "Epoch 66/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4845 - val_loss: -3.4832\n",
            "Epoch 67/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4846 - val_loss: -3.4834\n",
            "Epoch 68/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4847 - val_loss: -3.4835\n",
            "Epoch 69/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4848 - val_loss: -3.4836\n",
            "Epoch 70/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4849 - val_loss: -3.4837\n",
            "Epoch 71/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4850 - val_loss: -3.4838\n",
            "Epoch 72/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4851 - val_loss: -3.4839\n",
            "Epoch 73/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4852 - val_loss: -3.4840\n",
            "Epoch 74/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4853 - val_loss: -3.4841\n",
            "Epoch 75/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4853 - val_loss: -3.4842\n",
            "Epoch 76/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4855 - val_loss: -3.4842\n",
            "Epoch 77/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4855 - val_loss: -3.4843\n",
            "Epoch 78/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4856 - val_loss: -3.4844\n",
            "Epoch 79/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4857 - val_loss: -3.4845\n",
            "Epoch 80/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4858 - val_loss: -3.4846\n",
            "Epoch 81/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4858 - val_loss: -3.4847\n",
            "Epoch 82/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4859 - val_loss: -3.4847\n",
            "Epoch 83/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4860 - val_loss: -3.4846\n",
            "Epoch 84/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4861 - val_loss: -3.4849\n",
            "Epoch 85/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4861 - val_loss: -3.4849\n",
            "Epoch 86/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4862 - val_loss: -3.4849\n",
            "Epoch 87/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4862 - val_loss: -3.4850\n",
            "Epoch 88/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4863 - val_loss: -3.4851\n",
            "Epoch 89/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4863 - val_loss: -3.4851\n",
            "Epoch 90/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4865 - val_loss: -3.4853\n",
            "Epoch 91/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4865 - val_loss: -3.4853\n",
            "Epoch 92/100\n",
            "200/200 [==============================] - 2s 10ms/step - loss: -3.4866 - val_loss: -3.4854\n",
            "Epoch 93/100\n",
            "200/200 [==============================] - 1s 7ms/step - loss: -3.4866 - val_loss: -3.4854\n",
            "Epoch 94/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4867 - val_loss: -3.4854\n",
            "Epoch 95/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4867 - val_loss: -3.4855\n",
            "Epoch 96/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4868 - val_loss: -3.4855\n",
            "Epoch 97/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4868 - val_loss: -3.4856\n",
            "Epoch 98/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: -3.4869 - val_loss: -3.4857\n",
            "Epoch 99/100\n",
            "200/200 [==============================] - 2s 11ms/step - loss: -3.4869 - val_loss: -3.4857\n",
            "Epoch 100/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: -3.4870 - val_loss: -3.4857\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABa2UlEQVR4nO3deXhU9d3+8fdkksxkT0hCEiAJEJYEBdxAE7RKUVGQxWJRRNCKxfaHVREooUqrtYq71IJdfaA8Ujd29UFBEcpuQFDWAAFJCEuA7Psy5/fHOEMiEEKYZDLhfl3XuSYzc86Zz5y05PZzvud7TIZhGIiIiIjIOXm5uwARERGRlkxhSURERKQeCksiIiIi9VBYEhEREamHwpKIiIhIPRSWREREROqhsCQiIiJSD293F9Aa2Gw2jh49SlBQECaTyd3liIiISAMYhkFRURHt2rXDy+v8/SOFJRc4evQosbGx7i5DREREGiErK4sOHTqc932FJRcICgoC7Ac7ODjYzdWIiIhIQxQWFhIbG+v8O34+Cksu4Dj1FhwcrLAkIiLiYS40hEYDvEVERETqobAkIiIiUg+FJREREZF6aMySiIhILTU1NVRVVbm7DHEBHx8fzGbzJe9HYUlERAT7nDvHjx8nPz/f3aWIC4WGhhIdHX1J8yAqLImIiIAzKLVt2xZ/f39NMuzhDMOgtLSUnJwcAGJiYhq9L4UlERG57NXU1DiDUnh4uLvLERfx8/MDICcnh7Zt2zb6lJwGeIuIyGXPMUbJ39/fzZWIqzl+p5cyDk1hSURE5Ac69db6uOJ3qrAkIiIiUg+FJREREZF6KCyJiIiIU8eOHZk5c2aD11+9ejUmk6lVT7ngMWFp6NChxMXFYbVaiYmJYcyYMRw9erTebR599FESEhLw8/MjMjKSYcOGsXfv3nOue/r0aTp06NCifuEnT8KBA1BW5u5KRESkpTGZTPUuzz77bKP2m5aWxvjx4xu8fkpKCseOHSMkJKRRn+cJPCYs9e/fnw8//JD09HQWLlxIRkYG99xzT73bXHvttcyZM4c9e/bw+eefYxgGt99+OzU1NWetO27cOHr16tVU5TdK377QtSt8+627KxERkZbm2LFjzmXmzJkEBwfXeW3y5MnOdQ3DoLq6ukH7jYyMvKirAn19fS950seWzmPC0sSJE7nhhhuIj48nJSWF1NRUNm3aVO+lgOPHj+cnP/kJHTt25JprruFPf/oTWVlZfP/993XW++tf/0p+fn6d/2G1BEFB9seiIvfWISJyuTEMKClxz2IYDasxOjrauYSEhGAymZzP9+7dS1BQEMuXL+faa6/FYrGwbt06MjIyGDZsGFFRUQQGBtKnTx+++OKLOvv98Wk4k8nEv/71L+6++278/f3p2rUry5Ytc77/49Nwc+fOJTQ0lM8//5ykpCQCAwO54447OHbsmHOb6upqHn/8cUJDQwkPD2fq1Kk8+OCDDB8+vLG/siblMWGpttzcXObPn09KSgo+Pj4N2qakpIQ5c+bQqVMnYmNjna/v3r2bP/7xj8ybNw8vr4YdjoqKCgoLC+ssTSE42P7YRLsXEZHzKC2FwED3LKWlrvseqampvPTSS+zZs4devXpRXFzMoEGD+PLLL9m2bRt33HEHQ4YMITMzs979PPfcc4wcOZLvvvuOQYMGMXr0aHJzc+s5fqW89tpr/O///i///e9/yczMrNOQePnll5k/fz5z5sxh/fr1FBYWsmTJEld9bZfzqLA0depUAgICCA8PJzMzk6VLl15wm7fffpvAwEACAwNZvnw5K1euxNfXF7CHnlGjRvHqq68SFxfX4DpmzJhBSEiIc6kdvlzJEZbUWRIRkcb44x//yG233UZCQgJt2rShd+/ePProo1x55ZV07dqV559/noSEhDqdonN56KGHGDVqFF26dOHFF1+kuLiYr7/++rzrV1VV8be//Y3rrruOa665hscee4wvv/zS+f5f/vIXpk2bxt13301iYiKzZs0iNDTUVV/b5dwallJTUy84QK32gOwpU6awbds2VqxYgdlsZuzYsRgX6FeOHj2abdu2sWbNGrp168bIkSMpLy8HYNq0aSQlJfHAAw9cVN3Tpk2joKDAuWRlZV38l28Ax2k4dZZERJqXvz8UF7tnceUk4tddd12d58XFxUyePJmkpCRCQ0MJDAxkz549F+ws1R7TGxAQQHBwsPOea+fi7+9PQkKC83lMTIxz/YKCAk6cOEHfvn2d75vNZq699tqL+m7Nya33hps0aRIPPfRQvet07tzZ+XNERAQRERF069aNpKQkYmNj2bRpE8nJyefd3tH96dq1KzfccANhYWEsXryYUaNGsWrVKnbs2MGCBQsAnMErIiKCp59+mueee+6c+7RYLFgslov8thdPp+FERNzDZIKAAHdXcekCfvQlJk+ezMqVK3nttdfo0qULfn5+3HPPPVRWVta7nx8PeTGZTNhstota/0LNjZbMrWEpMjKSyMjIRm3r+CVVVFQ0eBvDMDAMw7nNwoULKat1XX5aWhoPP/wwa9eurZOI3UUDvEVExJXWr1/PQw89xN133w3YO00/vuipqYWEhBAVFUVaWho/+clPAPuNjL/55huuuuqqZq2lodwalhpq8+bNpKWlceONNxIWFkZGRgbTp08nISHB2VXKzs5mwIABzJs3j759+3Lw4EE++OADbr/9diIjIzly5AgvvfQSfn5+DBo0COCsQHTq1CkAZ3vS3dRZEhERV+ratSuLFi1iyJAhmEwmpk+fXm+HqKn85je/YcaMGXTp0oXExET+8pe/kJeX12KnH/CIAd7+/v4sWrSIAQMG0L17d+ecSGvWrHGeDquqqiI9PZ3SHy4jsFqtrF27lkGDBtGlSxfuvfdegoKC2LBhA23btnXn12kwdZZERMSV3njjDcLCwkhJSWHIkCEMHDiQa665ptnrmDp1KqNGjWLs2LEkJycTGBjIwIEDsVqtzV5LQ5gMTz6J2EIUFhYSEhJCQUEBwY52kAv8858wfjwMGQIXuFBBREQuQXl5OYcOHaJTp04t9g92a2az2UhKSmLkyJE8//zzLt13fb/bhv799ojTcJcrdZZERKQ1Onz4MCtWrODmm2+moqKCWbNmcejQIe6//353l3ZOHnEa7nKlMUsiItIaeXl5MXfuXPr06UO/fv3YsWMHX3zxBUlJSe4u7ZzUWWrB1FkSEZHWKDY2lvXr17u7jAZTZ6kFU2dJRETE/RSWWjDd7kRERMT9FJZaMB+fcuAUpaWVVFe7uxoREZHLk8JSC3bjjUlAJLCN4mJ3VyMiInJ5UlhqwYKCAn/4qUjjlkRERNxEYakFC3JcDqewJCIiTeCWW27hySefdD7v2LEjM2fOrHcbk8nEkiVLLvmzXbWf5qCw1IIFBjo6S8Ua5C0iInUMGTKEO+6445zvrV27FpPJxHfffXdR+0xLS2P8+PGuKM/p2WefPecNco8dO8add97p0s9qKgpLLZg6SyIicj7jxo1j5cqVHDly5Kz35syZw3XXXUevXr0uap+RkZH4+/u7qsR6RUdHO+/v2tIpLLVgZ8KSOksiIlLXXXfdRWRkJHPnzq3zenFxMR999BHDhw9n1KhRtG/fHn9/f3r27Ml7771X7z5/fBpu//79/OQnP8FqtdKjRw9Wrlx51jZTp06lW7du+Pv707lzZ6ZPn05VVRUAc+fO5bnnnuPbb7/FZDJhMpmc9f74NNyOHTv46U9/ip+fH+Hh4YwfP57iWlc3PfTQQwwfPpzXXnuNmJgYwsPDmTBhgvOzmpJm8G7BzpyGU2dJRKQ5GYZBaWmpWz7b398fk8l0wfW8vb0ZO3Ysc+fO5emnn3Zu89FHH1FTU8MDDzzARx99xNSpUwkODubTTz9lzJgxJCQk0Ldv3wvu32az8bOf/YyoqCg2b95MQUFBnfFNDkFBQcydO5d27dqxY8cOfvnLXxIUFMRvf/tb7r33Xnbu3Mlnn33GF198AUBISMhZ+ygpKWHgwIEkJyeTlpZGTk4OjzzyCI899lidMPjVV18RExPDV199xYEDB7j33nu56qqr+OUvf3nB73MpFJZaMHWWRETco7S0tNZ/sDav4uJiAgICGrTuww8/zKuvvsqaNWu45ZZbAPspuBEjRhAfH8/kyZOd6/7mN7/h888/58MPP2xQWPriiy/Yu3cvn3/+Oe3atQPgxRdfPGuc0TPPPOP8uWPHjkyePJn333+f3/72t/j5+REYGIi3tzfR0dHn/az//Oc/lJeXM2/ePOd3nzVrFkOGDOHll18mKioKgLCwMGbNmoXZbCYxMZHBgwfz5ZdfNnlY0mm4FkydJRERqU9iYiIpKSn8z//8DwAHDhxg7dq1jBs3jpqaGp5//nl69uxJmzZtCAwM5PPPPyczM7NB+96zZw+xsbHOoASQnJx81noffPAB/fr1Izo6msDAQJ555pkGf0btz+rdu3edkNivXz9sNhvp6enO16644grMZrPzeUxMDDk5ORf1WY2hzlILVnuAtzpLIiLNx9/fv854meb+7Isxbtw4fvOb3zB79mzmzJlDQkICN998My+//DJ//vOfmTlzJj179iQgIIAnn3ySyspKl9W6ceNGRo8ezXPPPcfAgQMJCQnh/fff5/XXX3fZZ9Tm4+NT57nJZMJmszXJZ9WmsNSC1Z46QJ0lEZHmYzKZGnwqzN1GjhzJE088wX/+8x/mzZvHr3/9a0wmE+vXr2fYsGE88MADgH0M0r59++jRo0eD9puUlERWVhbHjh0jJiYGgE2bNtVZZ8OGDcTHx/P00087Xzt8+HCddXx9fampqbngZ82dO5eSkhLncV+/fj1eXl507969QfU2JZ2Ga8HUWRIRkQsJDAzk3nvvZdq0aRw7doyHHnoIgK5du7Jy5Uo2bNjAnj17ePTRRzlx4kSD93vrrbfSrVs3HnzwQb799lvWrl1bJxQ5PiMzM5P333+fjIwM3nrrLRYvXlxnnY4dO3Lo0CG2b9/OqVOnqKioOOuzRo8ejdVq5cEHH2Tnzp189dVX/OY3v2HMmDHO8UrupLDUgtUe4K3OkoiInM+4cePIy8tj4MCBzjFGzzzzDNdccw0DBw7klltuITo6muHDhzd4n15eXixevJiysjL69u3LI488wgsvvFBnnaFDhzJx4kQee+wxrrrqKjZs2MD06dPrrDNixAjuuOMO+vfvT2Rk5DmnL/D39+fzzz8nNzeXPn36cM899zBgwABmzZp18QejCZgMwzDcXYSnKywsJCQkhIKCAoKDg12233Xr1nHTTTcBXbj55v2sXu2yXYuISC3l5eUcOnSITp06YbVa3V2OuFB9v9uG/v1WZ6kFU2dJRETE/RSWWjBNHSAiIuJ+Ckst2JnOUgmFhU1/aaSIiIicTWGpBas9e2xhYYkbKxEREbl8KSy1YH5+fnh52X9FFRVFNMO9AkVELmu65qn1ccXvVGGpBTOZTLo/nIhIM3DMDO2um+dK03H8Tn88+/fF0AzeLVxgYCAFBQU4JqZs08bdFYmItD5ms5nQ0FDnfcb8/f0xmUxurkouhWEYlJaWkpOTQ2hoaJ17yl0shaUWrvYs3roiTkSk6URHRwM0y41ZpfmEhoY6f7eNpbDUwtW+P5xOw4mINB2TyURMTAxt27alSoNEWwUfH59L6ig5KCy1cOosiYg0L7PZ7JI/sNJ6aIB3C6cB3iIiIu6lsNTCaRZvERER91JYauHUWRIREXEvhaUWTp0lERER91JYauE0wFtERMS9PCYsDR06lLi4OKxWKzExMYwZM4ajR4/Wu82jjz5KQkICfn5+REZGMmzYMPbu3XvWenPnzqVXr15YrVbatm3LhAkTmuprXDRNHSAiIuJeHhOW+vfvz4cffkh6ejoLFy4kIyODe+65p95trr32WubMmcOePXv4/PPPMQyD22+/nZqaGuc6b7zxBk8//TSpqans2rWLL774goEDBzb112kwdZZERETcy2R46F0Dly1bxvDhw6moqGjw/V6+++47evfuzYEDB0hISCAvL4/27dvz8ccfM2DAgEbXUlhYSEhICAUFBQQHBzd6P+fy3nvvcf/99wM/ZfDgL/nkE5fuXkRE5LLV0L/fHtNZqi03N5f58+eTkpLS4KBUUlLCnDlz6NSpE7GxsQCsXLkSm81GdnY2SUlJdOjQgZEjR5KVlVXvvioqKigsLKyzNBUN8BYREXEvjwpLU6dOJSAggPDwcDIzM1m6dOkFt3n77bcJDAwkMDCQ5cuXs3LlSnx9fQE4ePAgNpuNF198kZkzZ7JgwQJyc3O57bbbqKysPO8+Z8yYQUhIiHNxhK+moKkDRERE3MutYSk1NRWTyVTvUntA9pQpU9i2bRsrVqzAbDYzduxYLnQWcfTo0Wzbto01a9bQrVs3Ro4cSXl5OQA2m42qqireeustBg4cyA033MB7773H/v37+eqrr867z2nTplFQUOBcLtSJuhTqLImIiLiXW+8NN2nSJB566KF61+ncubPz54iICCIiIujWrRtJSUnExsayadMmkpOTz7u9o/vTtWtXbrjhBsLCwli8eDGjRo0iJiYGgB49ejjXj4yMJCIigszMzPPu02KxYLFYGvgtL03tAd7qLImIiDQ/t4alyMhIIiMjG7WtzWYD7OOHGsowDAzDcG7Tr18/ANLT0+nQoQNgHw916tQp4uPjG1WXq9U+DVdQYAAmd5YjIiJy2fGIMUubN29m1qxZbN++ncOHD7Nq1SpGjRpFQkKCs6uUnZ1NYmIiX3/9NWAfjzRjxgy2bt1KZmYmGzZs4Oc//zl+fn4MGjQIgG7dujFs2DCeeOIJNmzYwM6dO3nwwQdJTEykf//+bvu+tZ05DVdDZWU59QylEhERkSbgEWHJ39+fRYsWMWDAALp37864cePo1asXa9ascZ4Oq6qqIj09ndLSUgCsVitr165l0KBBdOnShXvvvZegoCA2bNhA27ZtnfueN28e119/PYMHD+bmm2/Gx8eHzz77rMFX2TW1gICAWs80yFtERKS5eew8Sy1JU86zBPbAZA+BGRw61JmOHV3+ESIiIpedVj3P0uWm9rglXREnIiLSvBSWPICmDxAREXEfhSUPoOkDRERE3EdhyQPoNJyIiIj7KCx5gNqn4dRZEhERaV4KSx5AnSURERH3UVjyAOosiYiIuI/CkgdQZ0lERMR9FJY8gK6GExERcR+FJQ+geZZERETcR2HJA9Q+DafOkoiISPNSWPIA6iyJiIi4j8KSB1BnSURExH0UljyAOksiIiLuo7DkATR1gIiIiPsoLHkATR0gIiLiPgpLHqD2abjKSqiocGs5IiIilxWFJQ9wprNUAVSpuyQiItKMFJY8wJnOEmjckoiISPNSWPIAvr6++Pr6/vBM0weIiIg0J4UlD1F7kLc6SyIiIs1HYclDnDkVp86SiIhIc1JY8hDqLImIiLiHwpKHqD19gDpLIiIizUdhyUNoFm8RERH3UFjyEOosiYiIuIfCkodQZ0lERMQ9FJY8hAZ4i4iIuIfCkofQ1AEiIiLuobDkIdRZEhERcQ+FJQ9Re4C3wpKIiEjzUVjyELUHeBcUuLUUERGRy4rCkoeo3VnKzXVrKSIiIpcVhSUPUbuzlJfn1lJEREQuKx4TloYOHUpcXBxWq5WYmBjGjBnD0aNH693m0UcfJSEhAT8/PyIjIxk2bBh79+6ts05aWhoDBgwgNDSUsLAwBg4cyLffftuUX6VRag/wLi+HsjK3liMiInLZ8Jiw1L9/fz788EPS09NZuHAhGRkZ3HPPPfVuc+211zJnzhz27NnD559/jmEY3H777dTU1ABQXFzMHXfcQVxcHJs3b2bdunUEBQUxcOBAqqqqmuNrNVjtqQMAnYoTERFpJibDMAx3F9EYy5YtY/jw4VRUVODj49Ogbb777jt69+7NgQMHSEhIYMuWLfTp04fMzExiY2MB2LFjB7169WL//v106dKlQfstLCwkJCSEgoICgoODG/2d6pORkUGXLl0wmQIxjCK++w569mySjxIREbksNPTvt8d0lmrLzc1l/vz5pKSkNDgolZSUMGfOHDp16uQMRt27dyc8PJx33nmHyspKysrKeOedd0hKSqJjx47n3VdFRQWFhYV1lqbm6CwZRjFg07glERGRZuJRYWnq1KkEBAQQHh5OZmYmS5cuveA2b7/9NoGBgQQGBrJ8+XJWrlyJr68vYB8HtHr1at599138/PwIDAzks88+Y/ny5Xh7e593nzNmzCAkJMS5OMJXUzozZgmgVKfhREREmolbw1Jqaiomk6nepfaA7ClTprBt2zZWrFiB2Wxm7NixXOgs4ujRo9m2bRtr1qyhW7dujBw5kvLycgDKysoYN24c/fr1Y9OmTaxfv54rr7ySwYMHU1bPCOpp06ZRUFDgXLKyslxzQOrh5+eHl5fj16XpA0RERJqLW8csnTx5ktOnT9e7TufOnZ2doNqOHDlCbGwsGzZsIDk5uUGfV1lZSVhYGP/6178YNWoU77zzDr/73e84duyYM4g41nnnnXe47777GrTf5hizBBASEvLDKb99vPpqVyZPbrKPEhERafUa+vf7/OeamkFkZCSRkZGN2tZmswH28UMNZRgGhmE4tyktLcXLywuTyeRcx/Hcsf+WJCgo6IewpM6SiIhIc/GIMUubN29m1qxZbN++ncOHD7Nq1SpGjRpFQkKCs6uUnZ1NYmIiX3/9NQAHDx5kxowZbN26lczMTDZs2MDPf/5z/Pz8GDRoEAC33XYbeXl5TJgwgT179rBr1y5+8Ytf4O3tTf/+/d32fc+n9izeGuAtIiLSPDwiLPn7+7No0SIGDBhA9+7dGTduHL169WLNmjVYLBYAqqqqSE9Pp7S0FACr1cratWsZNGgQXbp04d577yUoKIgNGzbQtm1bABITE/n444/57rvvSE5O5qabbuLo0aN89tlnxMTEuO37nk/tWbzVWRIREWkeHjvPUkvSXGOW+vfvz+rVq4H3uPXW+1i5ssk+SkREpNVr1fMsXa7UWRIREWl+CksepPb94RSWREREmofCkgepfX84DfAWERFpHgpLHqR2Z6mgAKqr3VqOiIjIZUFhyYPUnjoAID/fbaWIiIhcNhSWPIijs+TjUwygcUsiIiLNQGHJgzjCkre3vbOksCQiItL0FJY8iOM0nLe3vbOkQd4iIiJNT2HJgzg6SyZTIaDOkoiISHNQWPIgoaGhANhs+YDCkoiISHNQWPIgYWFhAFRX28+/KSyJiIg0PYUlD9KmTRsAKiryAENhSUREpBkoLHkQR2fJMGqAIg3wFhERaQYKSx7Ez88Pq9X6w7NcdZZERESagcKSh3GcioM8hSUREZFmoLDkYRyn4tRZEhERaR4KSx6mdmdJY5ZERESansKSh/lxZ8kw3FqOiIhIq6ew5GFqd5aqq6G42K3liIiItHoKSx7G0Vkym+0DljRuSUREpGkpLHkYR2fJ11ezeIuIiDQHhSUP4+gs+fjYU5IGeYuIiDQthSUP4+gsmUzqLImIiDQHhSUPU/tqOFBYEhERaWoKSx7G0VmqrlZnSUREpDkoLHkYR2epqkqdJRERkeagsORhHJ2lysoioEoDvEVERJqYwpKHCQ0NrfUsX50lERGRJqaw5GHMZjMhISE/PNPNdEVERJqawpIHOnNFXJ7CkoiISBNTWPJAZ+4Pp86SiIhIU1NY8kC1b6arAd4iIiJNS2HJA9WemLKkBCoq3FqOiIhIq6aw5IF+fMsTdZdERESajsKSB3J0lnx9NTGliIhIU/OYsDR06FDi4uKwWq3ExMQwZswYjh492qBtDcPgzjvvxGQysWTJkjrvZWZmMnjwYPz9/Wnbti1Tpkyhurq6Cb6B6zg6S76+uuWJiIhIU/OYsNS/f38+/PBD0tPTWbhwIRkZGdxzzz0N2nbmzJmYTKazXq+pqWHw4MFUVlayYcMG/v3vfzN37lx+//vfu7p8l3J0lsxme0rSaTgREZGm4+3uAhpq4sSJzp/j4+NJTU1l+PDhVFVV4ePjc97ttm/fzuuvv86WLVuIiYmp896KFSvYvXs3X3zxBVFRUVx11VU8//zzTJ06lWeffRZfX99z7rOiooKKWqOqCwsLL/HbXZzaV8OBOksiIiJNyWM6S7Xl5uYyf/58UlJS6g1KpaWl3H///cyePZvo6Oiz3t+4cSM9e/YkKirK+drAgQMpLCxk165d593vjBkzCAkJcS6xsbGX9oUukqOzZLNpzJKIiEhT86iwNHXqVAICAggPDyczM5OlS5fWu/7EiRNJSUlh2LBh53z/+PHjdYIS4Hx+/Pjx8+532rRpFBQUOJesrKyL/CaXxtFZqq5WZ0lERKSpuTUspaamYjKZ6l327t3rXH/KlCls27aNFStWYDabGTt2LIZhnHPfy5YtY9WqVcycOdPldVssFoKDg+sszcnRWaqoyAUMhSUREZEm5NYxS5MmTeKhhx6qd53OnTs7f46IiCAiIoJu3bqRlJREbGwsmzZtIjk5+aztVq1aRUZGBqGhoXVeHzFiBDfddBOrV68mOjqar7/+us77J06cADjnabuWwtFZqqmpBErJywtwb0EiIiKtmFvDUmRkJJGRkY3a1mazAdQZaF1bamoqjzzySJ3XevbsyZtvvsmQIUMASE5O5oUXXiAnJ4e2bdsCsHLlSoKDg+nRo0ej6moOAQEBeHt7/zDFQR65uQpLIiIiTcUjrobbvHkzaWlp3HjjjYSFhZGRkcH06dNJSEhwdpWys7MZMGAA8+bNo2/fvkRHR5+zOxQXF0enTp0AuP322+nRowdjxozhlVde4fjx4zzzzDNMmDABi8XSrN/xYphMJtq0aUNOTg72m+l2cHdJIiIirZZHDPD29/dn0aJFDBgwgO7duzNu3Dh69erFmjVrnKGmqqqK9PR0SktLG7xfs9nMJ598gtlsJjk5mQceeICxY8fyxz/+sam+isucuT9cnsYsiYiINCGP6Cz17NmTVatW1btOx44dzzvY2+Fc78fHx/N///d/l1SfO5yZaylXk1KKiIg0IY/oLMnZaneW8vLghyFcIiIi4mIKSx6qdmfJMKCgwK3liIiItFoKSx7K0Vny8dHElCIiIk1JYclDOTpLvr665YmIiEhTUljyUI6w5OgsaZC3iIhI01BY8lCO03Amk72ldOqUO6sRERFpvRSWPJSjs2Qy2VtK9dz3V0RERC6BwpKHcnSWamrsnaWjR91ZjYiISOulsOShHJ2lykp7Z+nYMXdWIyIi0nopLHkoR2epvDwfsKmzJCIi0kQUljyUIyzZb+FSoM6SiIhIE1FY8lC+vr4EBAT88CxXnSUREZEmorDkwWrf8qSoCIqL3VqOiIhIq9SosPTvf/+bTz/91Pn8t7/9LaGhoaSkpHD48GGXFSf1c5yKs1o1yFtERKSpNCosvfjii/j5+QGwceNGZs+ezSuvvEJERAQTJ050aYFyfo7OUmioffoAhSURERHX827MRllZWXTp0gWAJUuWMGLECMaPH0+/fv245ZZbXFmf1MPRWQoKyuP4cc21JCIi0hQa1VkKDAzk9OnTAKxYsYLbbrsNAKvVSllZmeuqk3o5OktWqzpLIiIiTaVRnaXbbruNRx55hKuvvpp9+/YxaNAgAHbt2kXHjh1dWZ/Uw9FZ8vW1j1lSZ0lERMT1GtVZmj17NsnJyZw8eZKFCxcSHh4OwNatWxk1apRLC5Tzc3SWvLzUWRIREWkqjeoshYaGMmvWrLNef+655y65IGm4MxNTqrMkIiLSVBrVWfrss89Yt26d8/ns2bO56qqruP/++8nLy3NZcVK/M/eHU2dJRESkqTQqLE2ZMoXCwkIAduzYwaRJkxg0aBCHDh3iqaeecmmBcn5n7g+nzpKIiEhTadRpuEOHDtGjRw8AFi5cyF133cWLL77IN9984xzsLU3P0VkqLrZ3lgoLoaQEnHdBERERkUvWqM6Sr68vpaWlAHzxxRfcfvvtgP2Pt6PjJE3PEZby8/OcAUmn4kRERFyrUWHpxhtv5KmnnuL555/n66+/ZvDgwQDs27ePDh06uLRAOT/HabjS0lKioysAhSURERFXa1RYmjVrFt7e3ixYsIC//vWvtG/fHoDly5dzxx13uLRAOb/g4GC8vOy/wshIjVsSERFpCo0asxQXF8cnn3xy1utvvvnmJRckDefl5UVoaCi5ubmEheUC0QpLIiIiLtaosARQU1PDkiVL2LNnDwBXXHEFQ4cOxWw2u6w4ubA2bdqQm5tLUJCmDxAREWkKjQpLBw4cYNCgQWRnZ9O9e3cAZsyYQWxsLJ9++ikJCQkuLVLOzzFuKSBAp+FERESaQqPGLD3++OMkJCSQlZXFN998wzfffENmZiadOnXi8ccfd3WNUo/IyEgAvLxOAOosiYiIuFqjOktr1qxh06ZNzkvXAcLDw3nppZfo16+fy4qTC4uLiwOgsjITUGdJRETE1RrVWbJYLBQVFZ31enFxMb6+vpdclDScIywVFdnDkjpLIiIirtWosHTXXXcxfvx4Nm/ejGEYGIbBpk2b+NWvfsXQoUNdXaPUwxGWTp+2h6WCAvhhvlARERFxgUaFpbfeeouEhASSk5OxWq1YrVZSUlLo0qULM2fOdHGJUh9HWDp6NBN/f/tr6i6JiIi4TqPCUmhoKEuXLmXfvn0sWLCABQsWsG/fPhYvXkxoaKiLS7QbOnQocXFxWK1WYmJiGDNmDEcbOEDHMAzuvPNOTCYTS5Yscb7+7bffMmrUKGJjY/Hz8yMpKYk///nPTVJ/U3GEpaysLGJibIDGLYmIiLhSgwd4P/XUU/W+/9VXXzl/fuONNxpf0Xn079+f3/3ud8TExJCdnc3kyZO555572LBhwwW3nTlzJiaT6azXt27dStu2bXn33XeJjY1lw4YNjB8/HrPZzGOPPeby79AU2rdvj5eXF5WVlUREnCAjI0adJRERERdqcFjatm1bg9Y7VyhxhYkTJzp/jo+PJzU1leHDh1NVVYWPj895t9u+fTuvv/46W7ZsISYmps57Dz/8cJ3nnTt3ZuPGjSxatMhjwpK3tzft27cnKyuLoKBMIEadJRERERdqcFiq3Tlyt9zcXObPn09KSkq9Qam0tJT777+f2bNnEx0d3aB9FxQU1JkS4VwqKiqoqKhwPi8sLGxY4U0kLi6OrKwsfH0zgevVWRIREXGhRo1ZcpepU6cSEBBAeHg4mZmZLF26tN71J06cSEpKCsOGDWvQ/jds2MAHH3zA+PHj611vxowZhISEOJfY2NgGf4em4Bi3ZDJpriURERFXc2tYSk1NxWQy1bvs3bvXuf6UKVPYtm0bK1aswGw2M3bsWAzDOOe+ly1bxqpVqxp8dd7OnTsZNmwYf/jDH7j99tvrXXfatGkUFBQ4l6ysrAZ/56bgCEtVVZprSURExNUafSNdV5g0aRIPPfRQvet07tzZ+XNERAQRERF069aNpKQkYmNj2bRpE8nJyWdtt2rVKjIyMs66Om/EiBHcdNNNrF692vna7t27GTBgAOPHj+eZZ565YN0WiwWLxXLB9ZpLfHw8AMXFhwF1lkRERFzJrWEpMjLSeW+zi2Wz2S+Trz12qLbU1FQeeeSROq/17NmTN998kyFDhjhf27VrFz/96U958MEHeeGFFxpVi7s5Okt5eeosiYiIuJpbw1JDbd68mbS0NG688UbCwsLIyMhg+vTpzokxAbKzsxkwYADz5s2jb9++REdHn3NQd1xcHJ06dQLsp95++tOfMnDgQJ566imOHz8OgNlsbnSIcwdHWDpxwh6W8vOhrAz8/NxYlIiISCvhEQO8/f39WbRoEQMGDKB79+6MGzeOXr16sWbNGufpsKqqKtLT0ym9iHt9LFiwgJMnT/Luu+8SExPjXPr06dNUX6VJOMJSbu5prNYSQN0lERERVzEZ5xshLQ1WWFhISEgIBQUFBAcHu6WGkJAQCgsLiY3dQ1ZWImvXwo03uqUUERERj9DQv98e0VmSC3MM8g4Otg/yVmdJRETENRSWWgnHqTirVXMtiYiIuJLCUivhCEtms66IExERcSWFpVbCEZaqq9VZEhERcSWFpVbCEZbKytRZEhERcSWFpVbCMcA7L0+zeIuIiLiSwlIr4egsnT59BKjh8GHQpBAiIiKXTmGplYiJicFsNlNVVYWX1wlKStRdEhERcQWFpVbC29ub9u3bA9CunX3cUnq6OysSERFpHRSWWhHHqbi2bRWWREREXEVhqRVxDPIODLQP8lZYEhERuXQKS63IjyemVFgSERG5dApLrYgjLFVWKiyJiIi4isJSK+IISwUF9rD0/fdQVubGgkRERFoBhaVWxBGWjh7NJDTUPs/SgQPurUlERMTTKSy1Io6wlJubS5cuxYBOxYmIiFwqhaVWJDg4mNDQUEBzLYmIiLiKwlIr4+guhYUpLImIiLiCwlIr4whLVqvCkoiIiCsoLLUyjokpbbYzYUk31BUREWk8haVWxtFZKi4+jMkEBQWQk+PmokRERDyYwlIr4whL2dmZdOxof02n4kRERBpPYamVcYSl77//nu7d7a8pLImIiDSewlIrk5iYCEBmZiYdOxYACksiIiKXQmGplWnTpo1zkLfVuh2AvXvdWJCIiIiHU1hqha655hoAKiq+AdRZEhERuRQKS63Q1VdfDcCJE/awdOgQVFa6syIRERHPpbDUCjk6S+np2wgMhJoayMhwc1EiIiIeSmGpFXKEpT179tC1aymgU3EiIiKNpbDUCsXExBAVFYXNZiMy8jtAYUlERKSxFJZaKUd3ycdnG6CwJCIi0lgKS62UIyyVlemKOBERkUuhsNRKOa6IO3ZMYUlERORSKCy1Uo7OUkbGTqCS06fh9Gn31iQiIuKJFJZaqY4dOxIaGkplZSVRUbsBdZdEREQaw2PC0tChQ4mLi8NqtRITE8OYMWM4evRog7Y1DIM777wTk8nEkiVLzrnO6dOn6dChAyaTifz8fNcV7iYmk8l5Kq5NG/upON32RERE5OJ5TFjq378/H374Ienp6SxcuJCMjAzuueeeBm07c+ZMTCZTveuMGzeOXr16uaLUFsNxKs5isV8Rt3WrO6sRERHxTN7uLqChJk6c6Pw5Pj6e1NRUhg8fTlVVFT4+Pufdbvv27bz++uts2bKFmJiYc67z17/+lfz8fH7/+9+zfPnyC9ZSUVFBRUWF83lhYeFFfJPm4+gsOa6IW7vWndWIiIh4Jo/pLNWWm5vL/PnzSUlJqTcolZaWcv/99zN79myio6PPuc7u3bv54x//yLx58/DyatjhmDFjBiEhIc4lNja2Ud+jqTk6S5mZ24Eadu6EvDy3liQiIuJxPCosTZ06lYCAAMLDw8nMzGTp0qX1rj9x4kRSUlIYNmzYOd+vqKhg1KhRvPrqq8TFxTW4jmnTplFQUOBcsrKyLup7NJdu3brh7+9PWVkp8fH7MQzYsMHdVYmIiHgWt4al1NRUTCZTvcveWqOSp0yZwrZt21ixYgVms5mxY8diGMY5971s2TJWrVrFzJkzz/v506ZNIykpiQceeOCi6rZYLAQHB9dZWiKz2Uzv3r0B6NhRp+JEREQaw61jliZNmsRDDz1U7zqdO3d2/hwREUFERATdunUjKSmJ2NhYNm3aRHJy8lnbrVq1ioyMDEJDQ+u8PmLECG666SZWr17NqlWr2LFjBwsWLABwBq+IiAiefvppnnvuuUv7gi3ANddcw8aNG7FavwHuZ906d1ckIiLiWdwaliIjI4mMjGzUtjabDaDOQOvaUlNTeeSRR+q81rNnT958802GDBkCwMKFCykrK3O+n5aWxsMPP8zatWtJSEhoVF0tjWOQd2Gh/Yq4tDQoLwer1Z1ViYiIeA6PuBpu8+bNpKWlceONNxIWFkZGRgbTp08nISHB2VXKzs5mwIABzJs3j759+xIdHX3OQd1xcXF06tQJ4KxAdOrUKQCSkpLO6kh5Kscg7z17viEqyuDECRNffw0/+YmbCxMREfEQHjHA29/fn0WLFjFgwAC6d+/unBNpzZo1WCwWAKqqqkhPT6e0tNTN1bYsV1xxBT4+PuTn53PNNd8D6FSciIjIRfCIzlLPnj1ZtWpVvet07NjxvIO9HS70/i233HLBdTyNr68vV155Jdu2bSMmZhvQSYO8RURELoJHdJbk0vTp0weAkpLVgH36gJoaNxYkIiLiQRSWLgODBw8GYOPGpQQGGhQWwo4dbi5KRETEQygsXQZuu+02/P39yczM5Mor7VfFadySiIhIwygsXQb8/PwYOHAgAFbrEkCTU4qIiDSUwtJlYvjw4QBkZi4B7GGplY1lFxERaRIKS5eJu+66C7PZzMGDO/D2zuDYMTh0yN1ViYiItHwKS5eJNm3acPPNNwPQvr39BsQ6FSciInJhCkuXEcepuJqaJYAGeYuIiDSEwtJlZNiwYQAcPboeyFFnSUREpAEUli4jcXFxXHPNNdhsNkymT0hPhwMH3F2ViIhIy6awdJlxnIqLiFgCwLvvuq8WERERT6CwdJlxhKX8/BVAMe++qykERERE6qOwdJm58sor6dy5M1VVFVgsK8jIgE2b3F2ViIhIy6WwdJkxmUzO7lL79ksA+N//dV89IiIiLZ3C0mXIEZZycj4GSvngA6isdGtJIiIiLZbC0mUoJSWFzp07U1ycT3DwP8nNheXL3V2ViIhIy6SwdBkym82kpqYCYLO9ApTrVJyIiMh5KCxdpsaOHUuHDh0oLj4KzOXjjyEvz91ViYiItDwKS5cpi8XC1KlTAfDxeYnKyioWLHBzUSIiIi2QwtJlbNy4cURFRVFVdRiYr1NxIiIi56CwdBnz8/Nj8uTJPzx7kbVra/j+e3dWJCIi0vIoLF3mfvWrXxEeHg7sBz5kzhx3VyQiItKyKCxd5gIDA5k4ceIPz15g5kybBnqLiIjUorAkPPbYY4SEhAC7KCxczGuvubsiERGRlkNhSQgJCeHxxx//4dkkZs4sISfHrSWJiIi0GApLAsBvf/tb4uLigMOUlv6Rl15yd0UiIiItg8KSAPaxS7Nnz/7h2evMmvUtR464tSQREZEWQWFJnO666y7uueceoIaqqkd5/vkad5ckIiLidgpLUsef//xnAgKCgc38619/5+BBd1ckIiLiXgpLUke7du145ZUZANhs05g69aibKxIREXEvhSU5y6OPPsqVV14PFLJgweNs3OjuikRERNxHYUnOYjabmT//H5hMZmAhw4b9k+Jid1clIiLiHgpLck69evXimWeeB+Dkycd44IHNbq5IRETEPRSW5Lyeey6Vm276GVDJ0qUjmDfvhLtLEhERaXYeE5aGDh1KXFwcVquVmJgYxowZw9GjDRt8bBgGd955JyaTiSVLlpz1/ty5c+nVqxdWq5W2bdsyYcIEF1fvmUwmE59+Opc2bZKAbB555OdkZ1e5uywREZFm5TFhqX///nz44Yekp6ezcOFCMjIyfpgT6MJmzpyJyWQ653tvvPEGTz/9NKmpqezatYsvvviCgQMHurJ0jxYUFMTq1Yvx8gqmqmotN900GcNwd1UiIiLNx2QYnvmnb9myZQwfPpyKigp8fHzOu9727du566672LJlCzExMSxevJjhw4cDkJeXR/v27fn4448ZMGBAo2spLCwkJCSEgoICgoODG72fluytt5bxxBPDABg9+h+8++4v3VyRiIjIpWno32+P6SzVlpuby/z580lJSak3KJWWlnL//fcze/ZsoqOjz3p/5cqV2Gw2srOzSUpKokOHDowcOZKsrKx6P7+iooLCwsI6S2v3+ONDufXW6QDMnz+eX/1qpnsLEhERaSYeFZamTp1KQEAA4eHhZGZmsnTp0nrXnzhxIikpKQwbNuyc7x88eBCbzcaLL77IzJkzWbBgAbm5udx2221UVlaed78zZswgJCTEucTGxl7S9/IUy5c/S2LiRAD+/veJPPLIdDy0MSkiItJgbg1LqampmEymepe9e/c6158yZQrbtm1jxYoVmM1mxo4de94/1suWLWPVqlXMnDnzvJ9vs9moqqrirbfeYuDAgdxwww2899577N+/n6+++uq8202bNo2CggLncqFOVGvh7e3F9u2v07nznwB4550/MXbsY9hsNjdXJiIi0nS83fnhkyZN4qGHHqp3nc6dOzt/joiIICIigm7dupGUlERsbCybNm0iOTn5rO1WrVpFRkYGoaGhdV4fMWIEN910E6tXryYmJgaAHj16ON+PjIwkIiKCzMzM89ZksViwWCwN+Iatj8Vi4ptvnuaKK9qQnT2Bd999m5KSXN57b+5le0xERKR1c2tYioyMJDIyslHbOroZFRUV53w/NTWVRx55pM5rPXv25M0332TIkCEA9OvXD4D09HQ6dOgA2MdDnTp1ivj4+EbVdTkICYHNm39N795hnD49hsWL36d//yMsXbqo0b9PERGRlsojrobbvHkzaWlp3HjjjYSFhZGRkcH06dM5ceIEu3btwmKxkJ2dzYABA5g3bx59+/Y9535MJlOdq+EAhg8fzoEDB/jHP/5BcHAw06ZN4+DBg2zfvr3eweO1XQ5Xw53Lrl1w/fUrKSn5OVBAfHwn/u//PqnTqRMREWmpWtXVcP7+/ixatIgBAwbQvXt3xo0bR69evVizZo3z1E9VVRXp6emUlpZe1L7nzZvH9ddfz+DBg7n55pvx8fHhs88+a3BQupxdcQV88cVtBAVtBDpz+PAhbrghmc8++8zdpYmIiLiMR3SWWrrLtbPk8O23cOutpzh1agTwX7y8vJgxYwaTJk3CbDa7uzwREZFzalWdJWnZeveGdesiaN9+JfALbDYbU6dO5Sc/+Qnp6enuLk9EROSSKCyJS3TvDuvX+5KQ8A7wT0ymIDZs2MBVV13F66+/Tk1NjbtLFBERaRSFJXGZ+HhYt87E1Vc/gmHsxGS6nfLyciZPnsxNN93Ejh073F2iiIjIRVNYEpeKjoa1a+FnP4vDMD4D/omvbxAbN27k6quvZsqUKRQXF7u7TBERkQZTWBKXCwiAjz6CadNMwCNUVu6iXbufUVNTw2uvvUZiYiILFizQrVJERMQjKCxJk/DyghdfhH//G3x9Yzl6dCExMZ/Svn1nsrOz+fnPf86dd97J/v373V2qiIhIvRSWpEmNHQtffQWxsXDs2CBOnNjJbbf9Hl9fXz7//HOuvPJKfv/731NWVubuUkVERM5JYUmaXEqKfS6mESOgutqPlSufo2/fnfTvP5DKykqef/55evTowccff+zuUkVERM6isCTNIizMPo7p738HPz9Yt64rO3YsZ8qUBXTo0IHvv/+eoUOHMnz4cLKystxdroiIiJPCkjQbkwnGj4ctW6BXLzh1ysSrr47gppv28Pjjv8Xb25ulS5eSlJTEm2++SXV1tbtLFhERUViS5tejB3z9NaSm2geCv/deIIsWvczbb28jJSWFkpISnnrqKfr06cOmTZvcXa6IiFzmFJbELSwWmDHDPidTQgIcOQLjx19Jt25reeONfxAWFsb27dtJTk7m7rvvZufOne4uWURELlMKS+JWjsHf/+//2Z/PnevFn/70S555Zi8PPfQLvLy8WLJkCb169WL06NEcOHDAvQWLiMhlR2FJ3C4gAGbPhvXr7WOZcnNh0qS27N37P3z44U7uueceDMPgP//5D4mJiUyYMIGTJ0+6u2wREblMKCxJi5GSAlu3whtvQGAgbNoEP/95EhbLRyxdupVBgwZRU1PD22+/TZcuXXjllVcoLy93d9kiItLKKSxJi+LtDRMnwt69cN99YBgwfz6MGHENHTt+yoIFX3H11VdTWFjI1KlTSUxM5J133uHYsWPuLl1ERFopk6EbdF2ywsJCQkJCKCgoIDg42N3ltCrffAO/+x18/rn9ub8//PKXNuLj3+X1139Hdna2c93ExET69+/PT3/6UwYNGoS/v7+bqhYREU/Q0L/fCksuoLDU9FavhmnT7KfmwN6BGjmylMjIP7N27QK2bdtW58a8bdu2ZfLkyfz6178mMDDQPUWLiEiLprDUjBSWmodhwIoV8PLL9vvNOdxxB4wYkUtQ0H9Zv34Vy5Yt4/DhwwCEh4czadIkJkyYoN+NiIjUobDUjBSWmt/XX8Mrr8CiRfYQBfZB4SNGwH33VXH06HxefPFPZGRkABAcHMzw4cMZOXIkt912G76+vm6sXkREWgKFpWaksOQ++/fDvHnw7rvw/fdnXo+IgMGDq2nT5j0+/fQF9u1Ld74XGhrK3Xffze23306fPn3o3LkzJpOp+YsXERG3UlhqRgpL7mcYsGGDPTR9+KF9riYHf38b1167AZPpA3bvXsCpU8frbBsWFsZ1111H3759uf3220lOTsbHx6eZv4GIiDQ3haVmpLDUslRXw3//C0uWwOLF9lupnFFDcPBaIiIWUV29iWPHvqWqqrLO9kFBQQwYMIA77riDlJQUunbtitVqbc6vICIizUBhqRkpLLVchmGf6HLFCvsVdevXQ2lp7TUqsVp3EB+/BR+f/5KZuYLCwlN19mEymejYsSOJiYkkJiaSlJREjx49SEpKok2bNs35dURExIUUlpqRwpLnqKqCLVtgzRpYt84envLza69hA7YRHv4Z3t4rKSz8lrKy/HPuCyAqKorExES6devmXLp3706XLl0wm81N+2VEROSSKCw1I4Ulz2Wzwe7dZ4LT5s32QeNnGMBJYC8hIXsJDd2DybSHoqLdnD6ddd79BgUFOcdB9e3blx49ehASEkJISAh+fn4aUC4i0gIoLDUjhaXW5fRp+9QEmzbBt9/Cjh1w8OC51izCx2cvMTH7CAraB+yjpGQfx4/vpby89FwbAODj40NoaCiJiYlcc801XH311VxzzTUkJSXh7e3dVF9LRER+RGGpGSkstX5FRbBrF3z3nT1AOZbi4nOtXQPspk2br/H3/5ry8s2UlHxPeXkh9f3fzWw2065dO+Li4oiNjSU2NpZu3bpxxRVXODtTIiLiOgpLzUhh6fJks9k7Tnv2QHq6fdm7176cOnXOLYBiLJYCOnQ4RZs2OzCZtlFY+A1ZWdspKSms9/Pat29PYmIi7dq1IyYmhujoaGJiYoiNjaVjx47ExMTg5aV7Y4uINJTCUjNSWJIfO33aHqIcS3q6fSzUoUP2qQ3OZgOOERqaRUREFoGBmfj4HKa0dC85Obs5eTL7XBvV4ePjQ3x8PJ06deLaa691jpdq3769q7+eiEiroLDUjBSWpKGqquDwYdi3r24nau9eyMmpb8t8/Pz2EBGxn6Cg4/j6HsNkOkZFxTEKCrI4fjyTmpqac27Zvn17rrjiCjp06FBnSUhIoFOnTpqAU0QuWwpLzUhhSVyhoMB+Ws+xZGTAgQP2JTPzzD3wzsVkqiY29ijt239PUFA6paVpHDv2NYcO7cBms513O7PZTMeOHenatSsJCQl07NixzhIeHq4r90Sk1VJYakYKS9LUKirsp/AyMuydqe+/tz8eOmTvUhUUnG/LEnx9txEZmUFY2BEsliMYxhGKig6TnZ1Baen5r9oD+61gHJNxOuaTio+PJz4+nrCwMAUpEfFoCkvNSGFJ3Mkw4MQJ+6m8PXvsj46O1MGD5xsjBWAQFXWMqKj9BAXtx8fnENXV31Nc/D3Hjx/i+PFj9X5uYGAgcXFxJCQk0KVLF7p06UJCQgJxcXH4+flhsVici7+/v4KViLQ4rS4sDR06lO3bt5OTk0NYWBi33norL7/8Mu3atbvgtoZhMGjQID777DMWL17M8OHDne+lpaWRmprK1q1bMZlM9O3bl1deeYXevXs3uDaFJWmpqqvtHaj0dHuQ2r37zKDzujOXn61duzLatdtPcPBevLz2Ula2l/z8A+TkHObkyXoHWJ0lMjKS6667jmuvvZbrrruOq6++mvbt22uWcxFxq1YXlt58802Sk5OJiYkhOzubyZMnA7Bhw4YGbbty5UqWL19eJywVFxcTHx/P0KFDSU1Npbq6mj/84Q+sW7eOrKysBg98VVgST5Sbe6YDtX//mcd9+yAvr/5to6LK6NAhizZtvsdqzaC6+gBFRQfIyTnAiRPZVFRUUFFRccF5paKjo2nfvj3t27evMyVC7aVt27YahC4iTaLVhaUfW7ZsGcOHD6eioqLef0i3b9/OXXfdxZYtW4iJiakTlrZs2UKfPn3IzMwkNjYWgB07dtCrVy/2799Ply5dGlSLwpK0NqdPnwlQtQea799vf68+Viu0bw/t2hnExFTTtm0Z3t57KSnZyvHjWzhwYAv79u0679V75xIeHk50dDRRUVFERkbStm1b52NcXBydO3cmPj4eq9V6id9cRC4nDf377ZH3VsjNzWX+/PmkpKTUG5RKS0u5//77mT17NtHR0We93717d8LDw3nnnXf43e9+R01NDe+88w5JSUl07NjxvPt1/FezQ2Fh/ZMJinia8HD7csMNZ7+Xn28PUI4wdfCgfaD5wYNw5AiUl9vfz8gwAT4/LH1/WOy8vWvo1OkE0dHZhIYewc8vGzhGeflxSkqOk59/nJycY+Tk5FBTU8Pp06c5ffo0u3btqrfudu3a0alTp7OW9u3bEx0dTWBgoMZOichF86jO0tSpU5k1axalpaXccMMNfPLJJ4SHh593/UcffZSamhr+9a9/AWAymc4as7Rz506GDx/OoUOHAOjatSuff/458fHx593vs88+y3PPPXfW6+osyeWushKys89ejhyBrCz7cvSoffbzC/HxgdhYGx06nCYy8jjBwcfx88vBMHKoqTlJeflJ8vJOcPjwYQ4ePEjxue89U4efnx9RUVG0bdvWeWPjkJAQgoODiYqKon379nTo0MF5atDf398FR0VEWiqPOA2XmprKyy+/XO86e/bsITExEYBTp06Rm5vL4cOHee655wgJCeGTTz45538pLlu2jEmTJrFt2zYCAwOBs8NSWVkZt9xyC4mJiTz22GPU1NTw2muvsXfvXtLS0vDz8ztnTefqLMXGxiosiTRAdbU9MNm7T2eW7Gz7xJw5OdDQZq2/P8TFQXy8QVTUaQIDD+HldZCyskMUFBzi1KlDZGd/z7FjxxoUpn4sIiKCuLg44uPjiYuLo0OHDmeNqdJcVCKeyyPC0smTJzl9gQEQnTt3xtfX96zXjxw5QmxsLBs2bCA5Ofms95988kneeuutOvfKqqmpwcvLi5tuuonVq1c7T78dO3bMuV5lZSVhYWG888473HfffQ36HhqzJOJa5eVw/PiZuaQcy5Ej9qB17FjDA5W3N0RFQXR0CeHhJwgMPI6//yms1gJ8fQvx8ioACigoOM7Ro9kcOXKEI0eOUFJS0qD9+/r60q5duzqD1KOiopwdLMfPUVFRGlMl0sJ4xJilyMhIIiMjG7WtY1bi2h2e2lJTU3nkkUfqvNazZ0/efPNNhgwZAtjHNHl5edX5r0LH8/pmPRaRpmW1QseO9uXmm8+9TmmpPTjVnqTz++/tp/pOnLAvubn2Tpb9dGAA0PmH5Ww+PvaB6R06wFVXGURE5OPnl4mXVyZVVYcpKTlMUdExcnKOc/y4fTl9+jSVlZV8//33fP/99xf8XiEhIURFRREaGur8d8dkMmEymWjfvj3du3ene/fuzsk/zWYzJpPJ+e9SYGCgrgwUcQOPGLO0efNm0tLSuPHGGwkLCyMjI4Pp06dz4sQJdu3ahcViITs7mwEDBjBv3jz69u17zv38+DTc3r17ueqqq3j44Yf5zW9+g81m46WXXuLjjz9mz549xMTENKg+dZZEWqbKSvtpvWPHzoyfOnr0THfKsZw82fB9hodDdLS9WxUZWUFw8HGs1mzM5mxqarKprDxBSckJcnNPkJOTw4kTJzhx4gSVlZUu+U6hoaFEREQQERFB27ZtSUhIoGvXrs4lJibmnN14ETmbR3SWGsrf359Fixbxhz/8gZKSEmJiYrjjjjt45plnsFgsAFRVVZGenn7B2zfUlpiYyMcff8xzzz1HcnIyXl5eXH311Xz22WcNDkoi0nL5+to7RR06QJ8+51+vqsp+2u/IkbqLY3C6I2RVVtqnTjh9GuwX5lmA+B+WugIC7IEqNhb69DEIDy8gIOA4FssJ/PwKCQkxCA2F4GADw6jm8OHD7Nu3j/T0dNLT0zl27NwzqOfn55Ofn8+BAwfO+328vb0JCAggMDDQ+RgUFERgYCCBgYGEhYURExPjXKKjowkJCamzjSYMFTnDIzpLLZ06SyKtn81mn6zz+PEzi6ND5ehWZWfbX7+I/2bDZLJ3q6KiHGOr7I9t20JYmEFoqI02bQzCwmz4+BRis52ipOQUp06d4ujRoxw4cID9+/ezf/9+Dh48SFVVlUu+r5+fH23atCEsLMz5aLFYMJvNeHt7YzabsVqtZ8175Vg3LCxMUzVIi+cRA7xbC4UlEamtuNg+ZurHwcqxHD9uf//kyYZNo/BjVuuZubDCw6FNG/sSGlpDYGARAQEl+PkVY7GU4OtbjJdXMYZRTGVlMUVFRZw+fZpjx445lxMnTlBUVERxcbFLx2uazWbn1AzBwcEEBQU5H3+8BAQE4O/v73z09/fHarU6Fz8/P2eHzHFGQeRSKSw1I4UlEWmMmhr7KT1HeKq95OTYO1m5ufbFcfrv/DdGvjCLBcLC7EtEhD1oOR7DwiAkxCAwsAKLpRhv70JMpjxqanKpqsqjoCCPyspKampqqK6upqamhtLSUk6ePMnJkyfJyckhJyeHvLw88vLyXDZG61x8fX3rhKzapxtrL0FBQfj5+WE2m52Ll5cXAQEBzgAXHBxcZ84tPz8/dcMuIwpLzUhhSUSag2FAUdGZ4HT69Jkw5QhUjsdTp86sU1Bg3/ZShITYx2FZrfbFz88+z5UjfLVpY38MDobAQAOLpRzIwzDyMZmKgEIMo4iqqkJKSoooLCykqKjIuZSWllJSUuJ8LCsro7y83LmUlpZSXl7uisNYL29vb0JCQpxjvGqHMV9fX3x9ffHx8cHX1xc/P7+zOmVWqxWLxYKvry8WiwWTyeT8LmVlZVRUVNCmTRtiY2Pp0KEDERERCmdupLDUjBSWRKQls9nsISs/396tyss7E6ZOnbIv+fln3q+9XiPm8rygwMAzS1CQ/TE09MwSEmJ/zWI5s1it4O9fjbd3MWZzISZTEYZRRHV1CVVVJZSXF1NcXExJSQnFxcXO04qlpaXYbDZqamqw2WxUV1dTWlpKYWGhcykoKKCgoMAtU8ZYLBYiIyPrTBPh5eXlDGOOU5B+fn74+/s7Hx1L7dOXjomUDcNw3sTaMWu9Y94vjSOrq1VdDSciIo3n5WUPICEhUM+dnM6pqupMeCothbIy+6ShZWX2IOV4LzfX/lhUdPZSWGjvbjmmxSsubmwI8wZCf1jqMpvrdr1qd7+Cguwdr6Ag+2lHR0BzPNo7ZgaGUUxNTQGVlflAMTZbMYZRQk1NCRUVJVRWVlJVVUVlZSUVFRWUlZVRVFS3S+a4w4NjHcMwnGHHarXi6+vLqVOnyMrK4sSJE1RUVHDkyJHGHIxGcQzSNwwDm82GYRiYzWYCAgLqdNEsFouzg+bj44O3t7czhDlCZUBAAKGhoc4lMDCwztyFJpMJb29vLBaLc7FarXXCnr+//1kXDnh7e+Pr69uirshUWBIRkfPy8YHISPtyqSoq7KGpqOhMYCouPhOm8vPtj3l5UFJiX9+xOMJZQYF9/cJC+zoONTX25w2ceP0cTEDQD0uHs9718akbwhw/116CguxXM9bunPn727f18bFPZeHreybE+fpWUlp6lPLyU5hMNsxmA7BhMtnw9q7Ey6uMqir76TvHUlpaWueUZe3nZWVl9m/yw0SnJpOJ4uJiTpywz/lVUlJy3omcy8rKOHXqVGMPXpMwm811gtaaNWvo1q2bW2pRWBIRkWZhsdinRGjb1jX7s9nsc1/V7nY5Hh1LaemZ7pbjsXZQcyzl5WcWx3bFxWe6YVVV9qWht9lpGF+g4w/LuQUF2TuCwcH2wOXtbV8cAcwRwqxW+zq+vnVPX9Y+xWm1lmAYp/D2tuHt7YWPjwlvbxNeXtUYRomzi2Y/tVnh7KJVVlZSXV1dZzZ5sN8FwzHvV35+PkVFRc7Tf44uVHV1NRUVFZSXlzsfHaGvpKSEkpKS80534biIwDF/Yu3blzU3hSUREfFIXl5nujpNparK3q0qLj4TwGoHsx+HLMe6jqW01L6Pysozj471SkrOnNqsqbFf6VhdfWY9OHMq0zUCflguzMur7uLtXbeL5uiu+fufebRa7adDvbzOPFosddfx87O/VrvT5u1tYDbb8PKqwWyuAarw8qrCZKoAKjAM+2N0dKyrDsRFU1gSERE5Dx+fM52Z5lRVdebUZH6+PTBVVdUNVNXV9lDlWCoqzjw6lpKSM/twnOasvZ+amrrdOAeb7ew5wJpisL+dCTD/sAD4nXOt3bshKampaqifwpKIiEgL4+NjH4weEdF8n2mznTkFWVNzJjDZbPaA9eMummNdx2N5uX07x7Y1NWfGm9Vez9E5cwQ6R3D7cWetduirqLB3pNxFYUlERETw8rJfGRjQsDN1lxX3jZYSERER8QAKSyIiIiL1UFgSERERqYfCkoiIiEg9FJZERERE6qGwJCIiIlIPhSURERGReigsiYiIiNRDYUlERESkHgpLIiIiIvVQWBIRERGph8KSiIiISD0UlkRERETqobAkIiIiUg9vdxfQGhiGAUBhYaGbKxEREZGGcvzddvwdPx+FJRcoKioCIDY21s2ViIiIyMUqKioiJCTkvO+bjAvFKbkgm83G0aNHCQoKwmQyuWy/hYWFxMbGkpWVRXBwsMv2K2fTsW4+OtbNR8e6eel4Nx9XHWvDMCgqKqJdu3Z4eZ1/ZJI6Sy7g5eVFhw4dmmz/wcHB+j9eM9Gxbj461s1Hx7p56Xg3H1cc6/o6Sg4a4C0iIiJSD4UlERERkXooLLVgFouFP/zhD1gsFneX0urpWDcfHevmo2PdvHS8m09zH2sN8BYRERGphzpLIiIiIvVQWBIRERGph8KSiIiISD0UlkRERETqobDUgs2ePZuOHTtitVq5/vrr+frrr91dksebMWMGffr0ISgoiLZt2zJ8+HDS09PrrFNeXs6ECRMIDw8nMDCQESNGcOLECTdV3Dq89NJLmEwmnnzySedrOs6ulZ2dzQMPPEB4eDh+fn707NmTLVu2ON83DIPf//73xMTE4Ofnx6233sr+/fvdWLFnqqmpYfr06XTq1Ak/Pz8SEhJ4/vnn69xbTMe6cf773/8yZMgQ2rVrh8lkYsmSJXXeb8hxzc3NZfTo0QQHBxMaGsq4ceMoLi6+5NoUllqoDz74gKeeeoo//OEPfPPNN/Tu3ZuBAweSk5Pj7tI82po1a5gwYQKbNm1i5cqVVFVVcfvtt1NSUuJcZ+LEiXz88cd89NFHrFmzhqNHj/Kzn/3MjVV7trS0NP7+97/Tq1evOq/rOLtOXl4e/fr1w8fHh+XLl7N7925ef/11wsLCnOu88sorvPXWW/ztb39j8+bNBAQEMHDgQMrLy91Yued5+eWX+etf/8qsWbPYs2cPL7/8Mq+88gp/+ctfnOvoWDdOSUkJvXv3Zvbs2ed8vyHHdfTo0ezatYuVK1fyySef8N///pfx48dfenGGtEh9+/Y1JkyY4HxeU1NjtGvXzpgxY4Ybq2p9cnJyDMBYs2aNYRiGkZ+fb/j4+BgfffSRc509e/YYgLFx40Z3lemxioqKjK5duxorV640br75ZuOJJ54wDEPH2dWmTp1q3Hjjjed932azGdHR0carr77qfC0/P9+wWCzGe++91xwlthqDBw82Hn744Tqv/exnPzNGjx5tGIaOtasAxuLFi53PG3Jcd+/ebQBGWlqac53ly5cbJpPJyM7OvqR61FlqgSorK9m6dSu33nqr8zUvLy9uvfVWNm7c6MbKWp+CggIA2rRpA8DWrVupqqqqc+wTExOJi4vTsW+ECRMmMHjw4DrHE3ScXW3ZsmVcd911/PznP6dt27ZcffXV/POf/3S+f+jQIY4fP17neIeEhHD99dfreF+klJQUvvzyS/bt2wfAt99+y7p167jzzjsBHeum0pDjunHjRkJDQ7nuuuuc69x66614eXmxefPmS/p83Ui3BTp16hQ1NTVERUXVeT0qKoq9e/e6qarWx2az8eSTT9KvXz+uvPJKAI4fP46vry+hoaF11o2KiuL48eNuqNJzvf/++3zzzTekpaWd9Z6Os2sdPHiQv/71rzz11FP87ne/Iy0tjccffxxfX18efPBB5zE9178pOt4XJzU1lcLCQhITEzGbzdTU1PDCCy8wevRoAB3rJtKQ43r8+HHatm1b531vb2/atGlzycdeYUkuWxMmTGDnzp2sW7fO3aW0OllZWTzxxBOsXLkSq9Xq7nJaPZvNxnXXXceLL74IwNVXX83OnTv529/+xoMPPujm6lqXDz/8kPnz5/Of//yHK664gu3bt/Pkk0/Srl07HetWTKfhWqCIiAjMZvNZVwadOHGC6OhoN1XVujz22GN88sknfPXVV3To0MH5enR0NJWVleTn59dZX8f+4mzdupWcnByuueYavL298fb2Zs2aNbz11lt4e3sTFRWl4+xCMTEx9OjRo85rSUlJZGZmAjiPqf5NuXRTpkwhNTWV++67j549ezJmzBgmTpzIjBkzAB3rptKQ4xodHX3WRVDV1dXk5uZe8rFXWGqBfH19ufbaa/nyyy+dr9lsNr788kuSk5PdWJnnMwyDxx57jMWLF7Nq1So6depU5/1rr70WHx+fOsc+PT2dzMxMHfuLMGDAAHbs2MH27dudy3XXXcfo0aOdP+s4u06/fv3OmgJj3759xMfHA9CpUyeio6PrHO/CwkI2b96s432RSktL8fKq+6fTbDZjs9kAHeum0pDjmpycTH5+Plu3bnWus2rVKmw2G9dff/2lFXBJw8Olybz//vuGxWIx5s6da+zevdsYP368ERoaahw/ftzdpXm0X//610ZISIixevVq49ixY86ltLTUuc6vfvUrIy4uzli1apWxZcsWIzk52UhOTnZj1a1D7avhDEPH2ZW+/vprw9vb23jhhReM/fv3G/Pnzzf8/f2Nd99917nOSy+9ZISGhhpLly41vvvuO2PYsGFGp06djLKyMjdW7nkefPBBo3379sYnn3xiHDp0yFi0aJERERFh/Pa3v3Wuo2PdOEVFRca2bduMbdu2GYDxxhtvGNu2bTMOHz5sGEbDjusdd9xhXH311cbmzZuNdevWGV27djVGjRp1ybUpLLVgf/nLX4y4uDjD19fX6Nu3r7Fp0yZ3l+TxgHMuc+bMca5TVlZm/L//9/+MsLAww9/f37j77ruNY8eOua/oVuLHYUnH2bU+/vhj48orrzQsFouRmJho/OMf/6jzvs1mM6ZPn25ERUUZFovFGDBggJGenu6maj1XYWGh8cQTTxhxcXGG1Wo1OnfubDz99NNGRUWFcx0d68b56quvzvnv84MPPmgYRsOO6+nTp41Ro0YZgYGBRnBwsPGLX/zCKCoquuTaTIZRa9pREREREalDY5ZERERE6qGwJCIiIlIPhSURERGReigsiYiIiNRDYUlERESkHgpLIiIiIvVQWBIRERGph8KSiIiISD0UlkREXGD16tWYTKazbg4sIp5PYUlERESkHgpLIiIiIvVQWBKRVsFmszFjxgw6deqEn58fvXv3ZsGCBcCZU2SffvopvXr1wmq1csMNN7Bz5846+1i4cCFXXHEFFouFjh078vrrr9d5v6KigqlTpxIbG4vFYqFLly688847ddbZunUr1113Hf7+/qSkpJCenu5879tvv6V///4EBQURHBzMtddey5YtW5roiIiIqygsiUirMGPGDObNm8ff/vY3du3axcSJE3nggQdYs2aNc50pU6bw+uuvk5aWRmRkJEOGDKGqqgqwh5yRI0dy3333sWPHDp599lmmT5/O3LlznduPHTuW9957j7feeos9e/bw97//ncDAwDp1PP3007z++uts2bIFb29vHn74Yed7o0ePpkOHDqSlpbF161ZSU1Px8fFp2gMjIpfOEBHxcOXl5Ya/v7+xYcOGOq+PGzfOGDVqlPHVV18ZgPH+++873zt9+rTh5+dnfPDBB4ZhGMb9999v3HbbbXW2nzJlitGjRw/DMAwjPT3dAIyVK1eeswbHZ3zxxRfO1z799FMDMMrKygzDMIygoCBj7ty5l/6FRaRZqbMkIh7vwIEDlJaWcttttxEYGOhc5s2bR0ZGhnO95ORk589t2rShe/fu7NmzB4A9e/bQr1+/Ovvt168f+/fvp6amhu3bt2M2m7n55pvrraVXr17On2NiYgDIyckB4KmnnuKRRx7h1ltv5aWXXqpTm4i0XApLIuLxiouLAfj000/Zvn27c9m9e7dz3NKl8vPza9B6tU+rmUwmwD6eCuDZZ59l165dDB48mFWrVtGjRw8WL17skvpEpOkoLImIx+vRowcWi4XMzEy6dOlSZ4mNjXWut2nTJufPeXl57Nu3j6SkJACSkpJYv359nf2uX7+ebt26YTab6dmzJzabrc4YqMbo1q0bEydOZMWKFfzsZz9jzpw5l7Q/EWl63u4uQETkUgUFBTF58mQmTpyIzWbjxhtvpKCggPXr1xMcHEx8fDwAf/zjHwkPDycqKoqnn36aiIgIhg8fDsCkSZPo06cPzz//PPfeey8bN25k1qxZvP322wB07NiRBx98kIcffpi33nqL3r17c/jwYXJychg5cuQFaywrK2PKlCncc889dOrUiSNHjpCWlsaIESOa7LiIiIu4e9CUiIgr2Gw2Y+bMmUb37t0NHx8fIzIy0hg4cKCxZs0a5+Drjz/+2LjiiisMX19fo2/fvsa3335bZx8LFiwwevToYfj4+BhxcXHGq6++Wuf9srIyY+LEiUZMTIzh6+trdOnSxfif//kfwzDODPDOy8tzrr9t2zYDMA4dOmRUVFQY9913nxEbG2v4+voa7dq1Mx577DHn4G8RablMhmEYbs5rIiJNavXq1fTv35+8vDxCQ0PdXY6IeBiNWRIRERGph8KSiIiISD10Gk5ERESkHuosiYiIiNRDYUlERESkHgpLIiIiIvVQWBIRERGph8KSiIiISD0UlkRERETqobAkIiIiUg+FJREREZF6/H/8wiToGCp3dwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability and\n",
        "## finding indexes of test_input_F_H matrix with the hij set that do not satisfy\n",
        "## constraint on the minimum SINR_P_min rate but satisfy the maximum transmit\n",
        "## power p_max\n",
        "\n",
        "test_input = [test_input_F_H, test_input_A_inv, test_input_X, test_input_beta, test_input_p_hat]\n",
        "# output_P_hat_temp = model.predict(test_input)\n",
        "output_P_hat_temp = np.multiply(p_max, model.predict(test_input))\n",
        "output_P_hat = output_P_hat_temp.reshape((output_P_hat_temp.shape[0], output_P_hat_temp.shape[1], 1)) # test_input_F_H_size X row X column\n",
        "output_P_hat_size = output_P_hat.shape[0]\n",
        "test_data_F_H_abs_sqr = cmplx_abs_sqr(test_data_F_H)\n",
        "\n",
        "indx_n = []\n",
        "count_v = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(output_P_hat[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(output_P_hat[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise[i] + ph - numr\n",
        "    SINR_out = np.divide(numr, dnumr)\n",
        "\n",
        "    if np.round(SINR_out, decimals= 3) < SINR_P_min[i]:\n",
        "      indx_n.append(k)\n",
        "      count_v = count_v + 1\n",
        "      # print(SINR_out)\n",
        "      break\n",
        "\n",
        "violation_prb = (count_v / output_P_hat_size) * 100\n",
        "print(\"Constraints Violation Probability: {:.2f}%\".format(violation_prb))\n",
        "# print(len(indx_n))\n",
        "# print(indx_n)"
      ],
      "metadata": {
        "id": "TppB89ypt7gC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7634d55-d57a-4673-ae78-90546638b2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step\n",
            "Constraints Violation Probability: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to calculate the average sum rate\n",
        "# Here, p_model is the output of DNN, and it is a 2D array.\n",
        "import math\n",
        "\n",
        "def average_sum_rate(hij, p_model, sigma_sqr_noise, K):\n",
        "  R = 0\n",
        "  hij_size = hij.shape[0]\n",
        "  hij_abs_sqr = cmplx_abs_sqr(hij)\n",
        "\n",
        "  for k in range(hij_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      phn = 0\n",
        "      for j in range(K):  # Total columns\n",
        "        phn_j = np.multiply(p_model[k,j], hij_abs_sqr[k,i,j])\n",
        "        phn = phn + phn_j\n",
        "\n",
        "      numr_s = np.multiply(p_model[k,i], hij_abs_sqr[k,i,i])\n",
        "      dnumr_s = sigma_sqr_noise[i] + phn - numr_s\n",
        "      R_temp = math.log2(1 + np.divide(numr_s, dnumr_s))\n",
        "      R = R + R_temp\n",
        "\n",
        "  return (R/hij_size)"
      ],
      "metadata": {
        "id": "Sj6wlXCWuAgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN Sum Rate for test_data_F_H\n",
        "sumrate_F_H = average_sum_rate(test_data_F_H, output_P_hat, sigma_sqr_noise, K)\n",
        "print(\"Average Sum Rate for all H matrices: {:.3f} Bit/Second/Hertz\".format(sumrate_F_H))"
      ],
      "metadata": {
        "id": "uRAZY_RBuCI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64000ec7-57be-4d77-fb73-028eaf0b346f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sum Rate for all H matrices: 3.489 Bit/Second/Hertz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking (A_inv x b), i.e., the power for negative values\n",
        "count_n = 0\n",
        "for c in range(output_P_hat_size):\n",
        "  p_temp = np.matmul(test_input_A_inv[c], test_input_b[c])\n",
        "  if np.any(p_temp < 0):\n",
        "    count_n = count_n + 1\n",
        "    print(c,'\\n')\n",
        "    print(p_temp)\n",
        "\n",
        "print(count_n)"
      ],
      "metadata": {
        "id": "G1JAw6fKjw5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339366ff-4671-4057-cca6-b2891342a355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking P_hat, i.e., the power for test_data_F_H for negative values\n",
        "## and Hit Rate i.e. percentage for 0 <= P_hat <= p_max\n",
        "count_p = 0\n",
        "count_n = 0\n",
        "\n",
        "for n in range(output_P_hat_size):\n",
        "  P_max = np.amax(output_P_hat[n])\n",
        "  if np.round(P_max, decimals = 3) <= 1:\n",
        "    count_p = count_p + 1\n",
        "\n",
        "  if np.any(output_P_hat[n] < 0):\n",
        "    count_n = count_n + 1\n",
        "    print(n,'\\n')\n",
        "    print(output_P_hat)\n",
        "\n",
        "p_hit_rate = (count_p / output_P_hat_size) * 100\n",
        "print(\"Htt Rate for Power : {:.2f}%\".format(p_hit_rate))\n",
        "print(\"Negative power count: \", count_n)"
      ],
      "metadata": {
        "id": "BHZkmotV2NTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e890b3fe-dafd-4edc-8e81-6fe5e6773c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Htt Rate for Power : 100.00%\n",
            "Negative power count:  0\n"
          ]
        }
      ]
    }
  ]
}