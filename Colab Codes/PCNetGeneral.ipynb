{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**2. Codes for analyzing the PCNet/PCNet+ model**"
      ],
      "metadata": {
        "id": "74OdOsgKiQ9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 : PCNet+ Model: For enhanced generalization capacity**"
      ],
      "metadata": {
        "id": "vgnzVksniirt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYB9DspIwMFV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "## Number of transmitter-receiver pairs\n",
        "K = 8\n",
        "\n",
        "## Minimum rate for the achievable SINR of multiple concurrent transmissions\n",
        "SINR_P_min = np.array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "\n",
        "## Maximum transmit power\n",
        "p_max = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading a NumPy array from a CSV file\n",
        "# Loading F_H array from a CSV file\n",
        "from numpy import loadtxt\n",
        "\n",
        "## Reading an array from the file\n",
        "# If we want to read a file from our local drive, we have to first upload it to Collab's session storage.\n",
        "F_H_2D_L_0dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_0dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_10dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_10dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_20dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_20dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_30dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_30dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_40dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_40dB.csv', delimiter = ',', dtype = str)\n",
        "\n",
        "# ## Reshaping the array from 2D to 3D\n",
        "F_H_3D_0dB = F_H_2D_L_0dB.reshape(F_H_2D_L_0dB.shape[0], F_H_2D_L_0dB.shape[1] // K, K)\n",
        "F_H_3D_10dB = F_H_2D_L_10dB.reshape(F_H_2D_L_10dB.shape[0], F_H_2D_L_10dB.shape[1] // K, K)\n",
        "F_H_3D_20dB = F_H_2D_L_20dB.reshape(F_H_2D_L_20dB.shape[0], F_H_2D_L_20dB.shape[1] // K, K)\n",
        "F_H_3D_30dB = F_H_2D_L_30dB.reshape(F_H_2D_L_30dB.shape[0], F_H_2D_L_30dB.shape[1] // K, K)\n",
        "F_H_3D_40dB = F_H_2D_L_40dB.reshape(F_H_2D_L_40dB.shape[0], F_H_2D_L_40dB.shape[1] // K, K)\n",
        "\n",
        "F_H_3D_0dB_size = F_H_3D_0dB.shape[0]\n",
        "F_H_3D_10dB_size = F_H_3D_10dB.shape[0]\n",
        "F_H_3D_20dB_size = F_H_3D_20dB.shape[0]\n",
        "F_H_3D_30dB_size = F_H_3D_30dB.shape[0]\n",
        "F_H_3D_40dB_size = F_H_3D_40dB.shape[0]"
      ],
      "metadata": {
        "id": "lbvUkI3zzYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to convert string data to complex data and to remove the initial whitespace\n",
        "def cnvrt_2_cmplx_data(F_H_3D_size, F_H_3D):\n",
        "  F_H_list = []\n",
        "  for k in range(F_H_3D_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      for j in range(K):  # Total columns\n",
        "        F_H_temp = complex(F_H_3D[k][i][j].strip())\n",
        "        F_H_list.append(F_H_temp)\n",
        "  F_H_array = np.array(F_H_list)\n",
        "  F_H = F_H_array.reshape((F_H_3D_size, K, K)) # H_size X row X column_count\n",
        "  return F_H"
      ],
      "metadata": {
        "id": "GLyhb0ECzo0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting string data to complex data and removing the initial whitespace\n",
        "F_H_0dB = cnvrt_2_cmplx_data(F_H_3D_0dB_size, F_H_3D_0dB)\n",
        "F_H_10dB = cnvrt_2_cmplx_data(F_H_3D_10dB_size, F_H_3D_10dB)\n",
        "F_H_20dB = cnvrt_2_cmplx_data(F_H_3D_20dB_size, F_H_3D_20dB)\n",
        "F_H_30dB = cnvrt_2_cmplx_data(F_H_3D_30dB_size, F_H_3D_30dB)\n",
        "F_H_40dB = cnvrt_2_cmplx_data(F_H_3D_40dB_size, F_H_3D_40dB)\n",
        "\n",
        "print(F_H_0dB.shape)\n",
        "print(F_H_10dB.shape)\n",
        "print(F_H_20dB.shape)\n",
        "print(F_H_30dB.shape)\n",
        "print(F_H_40dB.shape)\n",
        "\n",
        "F_H_0dB_size = F_H_0dB.shape[0]\n",
        "F_H_10dB_size = F_H_10dB.shape[0]\n",
        "F_H_20dB_size = F_H_20dB.shape[0]\n",
        "F_H_30dB_size = F_H_30dB.shape[0]\n",
        "F_H_40dB_size = F_H_40dB.shape[0]\n",
        "\n",
        "# print(F_H_0dB)\n",
        "# print(F_H_10dB)\n",
        "# print(F_H_20dB)\n",
        "# print(F_H_30dB)\n",
        "# print(F_H_40dB)"
      ],
      "metadata": {
        "id": "DRdDkfQD0QB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c96dd7f-9fe7-43d6-9442-afe5fca2396a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba as nb\n",
        "\n",
        "## Function to compute the square of the absolute value of an array of complex numbers\n",
        "@nb.vectorize([nb.float64(nb.complex128),nb.float32(nb.complex64)])\n",
        "def cmplx_abs_sqr(cmplx_var):\n",
        "  return cmplx_var.real**2 + cmplx_var.imag**2"
      ],
      "metadata": {
        "id": "oTHe5I6Y0yFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the matrix A (K x K)\n",
        "def generate_A(F_H_size, K, SINR_P_min, F_H):\n",
        "  Aij_list = []\n",
        "  F_H_abs_sqr = cmplx_abs_sqr(F_H)\n",
        "\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      Aj_list =[]\n",
        "      for j in range(K): # Total columns\n",
        "        if i==j:\n",
        "          A = F_H_abs_sqr[k,i,j]\n",
        "        else:\n",
        "          A = np.multiply(-SINR_P_min[i], F_H_abs_sqr[k,i,j])\n",
        "        Aj_list.append(A)\n",
        "      Aij_list.append(Aj_list)\n",
        "  Aij_array = np.array(Aij_list)\n",
        "  Aij = Aij_array.reshape((F_H_size, K, K)) # H_size X row X column\n",
        "  return Aij"
      ],
      "metadata": {
        "id": "amj7ytZZ0x72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A\n",
        "A_0dB = generate_A(F_H_0dB_size, K, SINR_P_min, F_H_0dB)\n",
        "A_10dB = generate_A(F_H_10dB_size, K, SINR_P_min, F_H_10dB)\n",
        "A_20dB = generate_A(F_H_20dB_size, K, SINR_P_min, F_H_20dB)\n",
        "A_30dB = generate_A(F_H_30dB_size, K, SINR_P_min, F_H_30dB)\n",
        "A_40dB = generate_A(F_H_40dB_size, K, SINR_P_min, F_H_40dB)\n",
        "\n",
        "print(A_0dB.shape)\n",
        "print(A_10dB.shape)\n",
        "print(A_20dB.shape)\n",
        "print(A_30dB.shape)\n",
        "print(A_40dB.shape)\n",
        "\n",
        "# print(A_0dB)\n",
        "# print(A_10dB)\n",
        "# print(A_20dB)\n",
        "# print(A_30dB)\n",
        "# print(A_40dB)"
      ],
      "metadata": {
        "id": "r_VgMSnO0rFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458b681e-21dc-48a9-d1a3-c2293fd4ad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Variances for noise signals\n",
        "sigma_sqr_noise_0dB = np.array([1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0], dtype = float)\n",
        "sigma_sqr_noise_10dB = np.array([1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1], dtype = float)\n",
        "sigma_sqr_noise_20dB = np.array([1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2], dtype = float)\n",
        "sigma_sqr_noise_30dB = np.array([1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3], dtype = float)\n",
        "sigma_sqr_noise_40dB = np.array([1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4], dtype = float)"
      ],
      "metadata": {
        "id": "vXoRqZRh8xGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the vector b (K x 1)\n",
        "def generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H):\n",
        "  bi_list = []\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows, i.e., total transmitters\n",
        "      b = np.multiply(SINR_P_min[i], sigma_sqr_noise[i])\n",
        "      bi_list.append(b)\n",
        "  bi_array = np.array(bi_list)\n",
        "  bi = bi_array.reshape((F_H_size, K, 1)) # H_size X row X column\n",
        "  return bi"
      ],
      "metadata": {
        "id": "efG88rpO8yIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create vector b\n",
        "b_0dB = generate_b(F_H_0dB_size, K, SINR_P_min, sigma_sqr_noise_0dB, F_H_0dB)\n",
        "b_10dB = generate_b(F_H_10dB_size, K, SINR_P_min, sigma_sqr_noise_10dB, F_H_10dB)\n",
        "b_20dB = generate_b(F_H_20dB_size, K, SINR_P_min, sigma_sqr_noise_20dB, F_H_20dB)\n",
        "b_30dB = generate_b(F_H_30dB_size, K, SINR_P_min, sigma_sqr_noise_30dB, F_H_30dB)\n",
        "b_40dB = generate_b(F_H_40dB_size, K, SINR_P_min, sigma_sqr_noise_40dB, F_H_40dB)\n",
        "\n",
        "print(b_0dB.shape)\n",
        "print(b_10dB.shape)\n",
        "print(b_20dB.shape)\n",
        "print(b_30dB.shape)\n",
        "print(b_40dB.shape)\n",
        "\n",
        "# print(b_0dB)\n",
        "# print(b_10dB)\n",
        "# print(b_20dB)\n",
        "# print(b_30dB)\n",
        "# print(b_40dB)"
      ],
      "metadata": {
        "id": "CXRm--qV87yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8fc1d1-a6ba-4c5c-9ad9-93990f47daf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A_inv, i.e., the pseudo inverse of matrix A\n",
        "A_inv_0dB = np.linalg.pinv(A_0dB)\n",
        "A_inv_10dB = np.linalg.pinv(A_10dB)\n",
        "A_inv_20dB = np.linalg.pinv(A_20dB)\n",
        "A_inv_30dB = np.linalg.pinv(A_30dB)\n",
        "A_inv_40dB = np.linalg.pinv(A_40dB)\n",
        "\n",
        "A_inv_0dB[A_inv_0dB<0] = 0\n",
        "A_inv_10dB[A_inv_10dB<0] = 0\n",
        "A_inv_20dB[A_inv_20dB<0] = 0\n",
        "A_inv_30dB[A_inv_30dB<0] = 0\n",
        "A_inv_40dB[A_inv_40dB<0] = 0\n",
        "\n",
        "print(A_inv_0dB.shape)\n",
        "print(A_inv_10dB.shape)\n",
        "print(A_inv_20dB.shape)\n",
        "print(A_inv_30dB.shape)\n",
        "print(A_inv_40dB.shape)\n",
        "\n",
        "# print(A_inv_0dB)\n",
        "# print(A_inv_10dB)\n",
        "# print(A_inv_20dB)\n",
        "# print(A_inv_30dB)\n",
        "# print(A_inv_40dB)"
      ],
      "metadata": {
        "id": "qXM1cion9FBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1aaeaf4-2e6c-4a11-ae3a-d06663a813ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector p_hat = (A_inv x b)\n",
        "p_hat_0dB = np.matmul(A_inv_0dB, b_0dB)\n",
        "p_hat_10dB = np.matmul(A_inv_10dB, b_10dB)\n",
        "p_hat_20dB = np.matmul(A_inv_20dB, b_20dB)\n",
        "p_hat_30dB = np.matmul(A_inv_30dB, b_30dB)\n",
        "p_hat_40dB = np.matmul(A_inv_40dB, b_40dB)\n",
        "\n",
        "print(p_hat_0dB.shape)\n",
        "print(p_hat_10dB.shape)\n",
        "print(p_hat_20dB.shape)\n",
        "print(p_hat_30dB.shape)\n",
        "print(p_hat_40dB.shape)\n",
        "\n",
        "# print(p_hat_0dB)\n",
        "# print(p_hat_10dB)\n",
        "# print(p_hat_20dB)\n",
        "# print(p_hat_30dB)\n",
        "# print(p_hat_40dB)"
      ],
      "metadata": {
        "id": "lNdQtSCF9L-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247347eb-6878-4d99-a6e6-b6ee31c79fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split datasets for training, validation, and testing.\n",
        "def split(np_array):\n",
        "  # data_size = np_array.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_array.shape[1]\n",
        "  column_count = np_array.shape[2]\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = complex, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_array[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_array[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_array[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "Zw-tmBrL9hvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split F_H matrix\n",
        "F_H_S_0dB = split(F_H_0dB)\n",
        "train_input_F_H_0dB = F_H_S_0dB[0]\n",
        "valid_input_F_H_0dB = F_H_S_0dB[1]\n",
        "test_input_F_H_0dB = F_H_S_0dB[2]\n",
        "test_data_F_H_0dB = F_H_S_0dB[3]\n",
        "\n",
        "F_H_S_10dB = split(F_H_10dB)\n",
        "train_input_F_H_10dB = F_H_S_10dB[0]\n",
        "valid_input_F_H_10dB = F_H_S_10dB[1]\n",
        "test_input_F_H_10dB = F_H_S_10dB[2]\n",
        "test_data_F_H_10dB = F_H_S_10dB[3]\n",
        "\n",
        "F_H_S_20dB = split(F_H_20dB)\n",
        "train_input_F_H_20dB = F_H_S_20dB[0]\n",
        "valid_input_F_H_20dB = F_H_S_20dB[1]\n",
        "test_input_F_H_20dB = F_H_S_20dB[2]\n",
        "test_data_F_H_20dB = F_H_S_20dB[3]\n",
        "\n",
        "F_H_S_30dB = split(F_H_30dB)\n",
        "train_input_F_H_30dB = F_H_S_30dB[0]\n",
        "valid_input_F_H_30dB = F_H_S_30dB[1]\n",
        "test_input_F_H_30dB = F_H_S_30dB[2]\n",
        "test_data_F_H_30dB = F_H_S_30dB[3]\n",
        "\n",
        "F_H_S_40dB = split(F_H_40dB)\n",
        "train_input_F_H_40dB = F_H_S_40dB[0]\n",
        "valid_input_F_H_40dB = F_H_S_40dB[1]\n",
        "test_input_F_H_40dB = F_H_S_40dB[2]\n",
        "test_data_F_H_40dB = F_H_S_40dB[3]"
      ],
      "metadata": {
        "id": "aothP-rf9pTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a7b90a-6b7d-415a-d737-1c3ef2ae716f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split p_hat vector\n",
        "p_hat_S_0dB = split(p_hat_0dB)\n",
        "train_input_p_hat_0dB = p_hat_S_0dB[0]\n",
        "valid_input_p_hat_0dB = p_hat_S_0dB[1]\n",
        "test_input_p_hat_0dB = p_hat_S_0dB[2]\n",
        "test_data_p_hat_0dB = p_hat_S_0dB[3]\n",
        "\n",
        "p_hat_S_10dB = split(p_hat_10dB)\n",
        "train_input_p_hat_10dB = p_hat_S_10dB[0]\n",
        "valid_input_p_hat_10dB = p_hat_S_10dB[1]\n",
        "test_input_p_hat_10dB = p_hat_S_10dB[2]\n",
        "test_data_p_hat_10dB = p_hat_S_10dB[3]\n",
        "\n",
        "p_hat_S_20dB = split(p_hat_20dB)\n",
        "train_input_p_hat_20dB = p_hat_S_20dB[0]\n",
        "valid_input_p_hat_20dB = p_hat_S_20dB[1]\n",
        "test_input_p_hat_20dB = p_hat_S_20dB[2]\n",
        "test_data_p_hat_20dB = p_hat_S_20dB[3]\n",
        "\n",
        "p_hat_S_30dB = split(p_hat_30dB)\n",
        "train_input_p_hat_30dB = p_hat_S_30dB[0]\n",
        "valid_input_p_hat_30dB = p_hat_S_30dB[1]\n",
        "test_input_p_hat_30dB = p_hat_S_30dB[2]\n",
        "test_data_p_hat_30dB = p_hat_S_30dB[3]\n",
        "\n",
        "p_hat_S_40dB = split(p_hat_40dB)\n",
        "train_input_p_hat_40dB = p_hat_S_40dB[0]\n",
        "valid_input_p_hat_40dB = p_hat_S_40dB[1]\n",
        "test_input_p_hat_40dB = p_hat_S_40dB[2]\n",
        "test_data_p_hat_40dB = p_hat_S_40dB[3]"
      ],
      "metadata": {
        "id": "RydWI6f0909G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885d9fb2-afa6-4956-82df-ebbf02532813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create EsN0 vector\n",
        "EsN0_array_0dB = np.full(shape = F_H_0dB_size, fill_value = 0, dtype = int)\n",
        "EsN0_array_10dB = np.full(shape = F_H_10dB_size, fill_value = 10, dtype = int)\n",
        "EsN0_array_20dB = np.full(shape = F_H_20dB_size, fill_value = 20, dtype = int)\n",
        "EsN0_array_30dB = np.full(shape = F_H_30dB_size, fill_value = 30, dtype = int)\n",
        "EsN0_array_40dB = np.full(shape = F_H_40dB_size, fill_value = 40, dtype = int)\n",
        "\n",
        "EsN0_vector_0dB = EsN0_array_0dB.reshape((F_H_0dB_size, 1)) # row X column\n",
        "EsN0_vector_10dB = EsN0_array_10dB.reshape((F_H_10dB_size, 1)) # row X column\n",
        "EsN0_vector_20dB = EsN0_array_20dB.reshape((F_H_20dB_size, 1)) # row X column\n",
        "EsN0_vector_30dB = EsN0_array_30dB.reshape((F_H_30dB_size, 1)) # row X column\n",
        "EsN0_vector_40dB = EsN0_array_40dB.reshape((F_H_40dB_size, 1)) # row X column\n",
        "\n",
        "print(EsN0_vector_0dB.shape)\n",
        "print(EsN0_vector_10dB.shape)\n",
        "print(EsN0_vector_20dB.shape)\n",
        "print(EsN0_vector_30dB.shape)\n",
        "print(EsN0_vector_40dB.shape)"
      ],
      "metadata": {
        "id": "2vi9kd0q96ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df0e4f3-65d9-481a-ecd2-b3212e233d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split EsN0 vector for training, validation, and testing.\n",
        "def split_EsN0(np_vector):\n",
        "  # data_size = np_vector.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_vector.shape[1]\n",
        "  column_count = 1\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = int, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = int, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = int, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_vector[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_vector[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_vector[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "GX-tNkUi-BuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split EsN0 vector\n",
        "EsN0_S_0dB = split_EsN0(EsN0_vector_0dB)\n",
        "train_input_EsN0_0dB = EsN0_S_0dB[0]\n",
        "valid_input_EsN0_0dB = EsN0_S_0dB[1]\n",
        "test_input_EsN0_0dB = EsN0_S_0dB[2]\n",
        "test_data_EsN0_0dB = EsN0_S_0dB[3]\n",
        "\n",
        "EsN0_S_10dB = split_EsN0(EsN0_vector_10dB)\n",
        "train_input_EsN0_10dB = EsN0_S_10dB[0]\n",
        "valid_input_EsN0_10dB = EsN0_S_10dB[1]\n",
        "test_input_EsN0_10dB = EsN0_S_10dB[2]\n",
        "test_data_EsN0_10dB = EsN0_S_10dB[3]\n",
        "\n",
        "EsN0_S_20dB = split_EsN0(EsN0_vector_20dB)\n",
        "train_input_EsN0_20dB = EsN0_S_20dB[0]\n",
        "valid_input_EsN0_20dB = EsN0_S_20dB[1]\n",
        "test_input_EsN0_20dB = EsN0_S_20dB[2]\n",
        "test_data_EsN0_20dB = EsN0_S_20dB[3]\n",
        "\n",
        "EsN0_S_30dB = split_EsN0(EsN0_vector_30dB)\n",
        "train_input_EsN0_30dB = EsN0_S_30dB[0]\n",
        "valid_input_EsN0_30dB = EsN0_S_30dB[1]\n",
        "test_input_EsN0_30dB = EsN0_S_30dB[2]\n",
        "test_data_EsN0_30dB = EsN0_S_30dB[3]\n",
        "\n",
        "EsN0_S_40dB = split_EsN0(EsN0_vector_40dB)\n",
        "train_input_EsN0_40dB = EsN0_S_40dB[0]\n",
        "valid_input_EsN0_40dB = EsN0_S_40dB[1]\n",
        "test_input_EsN0_40dB = EsN0_S_40dB[2]\n",
        "test_data_EsN0_40dB = EsN0_S_40dB[3]"
      ],
      "metadata": {
        "id": "HIobBjLP-GfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93c8804-2dd8-4866-bcf8-1112809f7018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating datasets for training\n",
        "train_input_F_H = np.concatenate((train_input_F_H_0dB, train_input_F_H_10dB,\n",
        "                                  train_input_F_H_20dB, train_input_F_H_30dB,\n",
        "                                  train_input_F_H_40dB,), axis=0)\n",
        "\n",
        "train_input_EsN0 = np.concatenate((train_input_EsN0_0dB, train_input_EsN0_10dB,\n",
        "                                   train_input_EsN0_20dB, train_input_EsN0_30dB,\n",
        "                                   train_input_EsN0_40dB), axis=0)\n",
        "\n",
        "print(train_input_F_H.shape)\n",
        "print(train_input_EsN0.shape)"
      ],
      "metadata": {
        "id": "8w4q9fl5-PHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0023c5-3b06-4c88-ca01-3e7ef550c9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 8, 8)\n",
            "(1000000, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating datasets for validation\n",
        "valid_input_F_H = np.concatenate((valid_input_F_H_0dB, valid_input_F_H_10dB,\n",
        "                                  valid_input_F_H_20dB, valid_input_F_H_30dB,\n",
        "                                  valid_input_F_H_40dB,), axis=0)\n",
        "\n",
        "valid_input_EsN0 = np.concatenate((valid_input_EsN0_0dB, valid_input_EsN0_10dB,\n",
        "                                   valid_input_EsN0_20dB, valid_input_EsN0_30dB,\n",
        "                                   valid_input_EsN0_40dB), axis=0)\n",
        "\n",
        "print(valid_input_F_H.shape)\n",
        "print(valid_input_EsN0.shape)"
      ],
      "metadata": {
        "id": "zomUf1OiBjdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a688eb-deeb-45dc-851b-21c1310db2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000, 8, 8)\n",
            "(125000, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Shuffling the training datasets\n",
        "train_shuffler = np.random.permutation(len(train_input_F_H))\n",
        "train_input_F_H_shuffled = train_input_F_H[train_shuffler]\n",
        "train_input_EsN0_shuffled = train_input_EsN0[train_shuffler]"
      ],
      "metadata": {
        "id": "GAWqe2-uBuKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Shuffling the validation datasets\n",
        "valid_shuffler = np.random.permutation(len(valid_input_F_H))\n",
        "valid_input_F_H_shuffled = valid_input_F_H[valid_shuffler]\n",
        "valid_input_EsN0_shuffled = valid_input_EsN0[valid_shuffler]"
      ],
      "metadata": {
        "id": "iXv0rvBqB0oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Reshaping train_input_F_H_shuffled and adding train_input_EsN0_shuffled\n",
        "const = K*K\n",
        "len1 = train_input_F_H_shuffled.shape[0]\n",
        "train_input_F_H_shuffled_reshaped = train_input_F_H_shuffled.reshape((len1, 1, const)) # size X row X column\n",
        "train_y_true = np.concatenate((train_input_F_H_shuffled_reshaped, train_input_EsN0_shuffled), axis=2)\n",
        "print(train_y_true.shape)"
      ],
      "metadata": {
        "id": "z0MPsAnfxUiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694d26f9-1a97-40b0-9bd1-d781e0c4700c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 1, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reshaping train_input_F_H_shuffled and adding train_input_EsN0_shuffled\n",
        "len2 = valid_input_F_H_shuffled.shape[0]\n",
        "valid_input_F_H_shuffled_reshaped = valid_input_F_H_shuffled.reshape((len2, 1, const)) # size X row X column\n",
        "valid_y_true = np.concatenate((valid_input_F_H_shuffled_reshaped, valid_input_EsN0_shuffled), axis=2)\n",
        "print(valid_y_true.shape)"
      ],
      "metadata": {
        "id": "kQVy0l2wxYYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ca2cc6-02a3-4772-ed14-be4dda61766b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000, 1, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the DNN model - The Functional API\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "## from tensorflow.keras import layers # shows warning\n",
        "from keras.api._v2.keras import layers\n",
        "from keras.layers import Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "hij_inputs = keras.Input(shape=(K,K), name = \"hij_inputs\")\n",
        "f1 = layers.Flatten(name = \"flatten_layer_hij\")(hij_inputs)\n",
        "\n",
        "EsN0_inputs = keras.Input(shape=(1,1), name = \"EsN0_inputs\")\n",
        "f2 = layers.Flatten(name = \"flatten_layer_EsN0\")(EsN0_inputs)\n",
        "\n",
        "concat_layers = concatenate([f1, f2])\n",
        "\n",
        "d1 = layers.Dense(2*K*K, activation=\"relu\", name = \"dense_layer_1\")(concat_layers)\n",
        "b1 = layers.BatchNormalization(name = \"batch_norm_layer_1\")(d1)\n",
        "\n",
        "d2 = layers.Dense(K*K, activation=\"relu\", name = \"dense_layer_2\")(b1)\n",
        "b2 = layers.BatchNormalization(name = \"batch_norm_layer_2\")(d2)\n",
        "\n",
        "# meu = layers.Dense(K, activation=\"relu\", name = \"meu\")(b2)\n",
        "P_hat = layers.Dense(K, activation=\"sigmoid\", name = \"P_hat\")(b2)\n",
        "\n",
        "model = keras.Model(inputs = [hij_inputs, EsN0_inputs], outputs = P_hat, name = \"functional_api\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "fb6pWn8oCl3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bd3a6e-eb82-4727-acc4-60227544b112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_api\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " hij_inputs (InputLayer)        [(None, 8, 8)]       0           []                               \n",
            "                                                                                                  \n",
            " EsN0_inputs (InputLayer)       [(None, 1, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " flatten_layer_hij (Flatten)    (None, 64)           0           ['hij_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " flatten_layer_EsN0 (Flatten)   (None, 1)            0           ['EsN0_inputs[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 65)           0           ['flatten_layer_hij[0][0]',      \n",
            "                                                                  'flatten_layer_EsN0[0][0]']     \n",
            "                                                                                                  \n",
            " dense_layer_1 (Dense)          (None, 128)          8448        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_norm_layer_1 (BatchNorma  (None, 128)         512         ['dense_layer_1[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " dense_layer_2 (Dense)          (None, 64)           8256        ['batch_norm_layer_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_norm_layer_2 (BatchNorma  (None, 64)          256         ['dense_layer_2[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " P_hat (Dense)                  (None, 8)            520         ['batch_norm_layer_2[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,992\n",
            "Trainable params: 17,608\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the model as a graph\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model.png\")"
      ],
      "metadata": {
        "id": "NAk0TFs-DSbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Display the input and output shapes of each layer\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "mgwfFNuJDTWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert SINR_P_min from numpy array to tensor\n",
        "SINR_P_min_t = tf.convert_to_tensor(SINR_P_min, dtype = float)\n",
        "tf.print(SINR_P_min_t)"
      ],
      "metadata": {
        "id": "K3HoXz5YEALV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5926d62f-0151-4892-e619-86976f7d8451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2 0.2 0.2 ... 0.2 0.2 0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The customized loss function that penalizes the constraint violation\n",
        "def custom_loss(y_true, y_pred):\n",
        "  # p = y_pred\n",
        "  p = tf.math.multiply(p_max, y_pred)\n",
        "\n",
        "  mtrx_elmnt = K*K\n",
        "  EsN0_val = y_true[0][0][mtrx_elmnt]\n",
        "  y_true_updt = y_true[:,:,:-1]\n",
        "\n",
        "  if EsN0_val < 10:\n",
        "    sigma_sqr_noise_lf = 1e-0\n",
        "  elif EsN0_val >= 10 and EsN0_val < 20:\n",
        "    sigma_sqr_noise_lf = 1e-1\n",
        "  elif EsN0_val >= 20 and EsN0_val < 30:\n",
        "    sigma_sqr_noise_lf = 1e-2\n",
        "  elif EsN0_val >= 30 and EsN0_val < 40:\n",
        "    sigma_sqr_noise_lf = 1e-3\n",
        "  else:\n",
        "    sigma_sqr_noise_lf = 1e-4\n",
        "\n",
        "  hij = tf.reshape(y_true_updt[:,0:K*K], (-1,K,K))\n",
        "  hij_abs_sqr = tf.math.square(tf.math.abs(hij))\n",
        "\n",
        "  lambda_l = 5.0\n",
        "  R_P = 0.0\n",
        "  pnlty_f_CV = 0.0\n",
        "\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0.0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = tf.math.multiply(p[:,j], hij_abs_sqr[:,i,j])\n",
        "      ph = tf.math.add(ph, ph_j)\n",
        "\n",
        "    numr = tf.math.multiply(p[:,i], hij_abs_sqr[:,i,i])\n",
        "    dnumr = tf.math.add(sigma_sqr_noise_lf, tf.math.subtract(ph, numr))\n",
        "    SINR_i = tf.math.divide(numr, dnumr)\n",
        "    R_P = tf.math.add(R_P, (tf.math.log(1 + SINR_i)/tf.math.log(2.0)))\n",
        "    pnlty_f_CV = tf.math.add(pnlty_f_CV,\n",
        "                             tf.nn.relu((tf.math.log(1 + SINR_P_min_t[i])/tf.math.log(2.0))\n",
        "                                      - (tf.math.log(1 + SINR_i)/tf.math.log(2.0))))\n",
        "\n",
        "  loss = tf.math.add(-R_P, tf.math.multiply(lambda_l, pnlty_f_CV))\n",
        "  loss = tf.reduce_mean(loss) # batch mean\n",
        "  return loss"
      ],
      "metadata": {
        "id": "8v2h8S8UEurO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build and compile the DNN model\n",
        "## Training and Testing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "optA = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer = optA, loss = custom_loss)\n",
        "\n",
        "train_input = [train_input_F_H_shuffled, train_input_EsN0_shuffled]\n",
        "valid_input = [valid_input_F_H_shuffled, valid_input_EsN0_shuffled]\n",
        "\n",
        "history = model.fit(train_input, train_y_true, epochs = 50,\n",
        "                    validation_data = (valid_input, valid_y_true), batch_size = 1000)\n",
        "\n",
        "plt.plot(history.epoch, history.history['loss'], color = \"blue\", label = \"Training\")\n",
        "plt.plot(history.epoch, history.history['val_loss'], color=\"black\", label = \"Validation\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFqAjH-0E0tM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308a696a-b89e-498f-fd9e-eeb3897d0be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1000/1000 [==============================] - 12s 8ms/step - loss: -1.8945 - val_loss: -2.1444\n",
            "Epoch 2/50\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: -2.1530 - val_loss: -2.2123\n",
            "Epoch 3/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.1948 - val_loss: -2.2456\n",
            "Epoch 4/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.2111 - val_loss: -2.2754\n",
            "Epoch 5/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3059 - val_loss: -2.3016\n",
            "Epoch 6/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.2966 - val_loss: -2.3193\n",
            "Epoch 7/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3036 - val_loss: -2.3348\n",
            "Epoch 8/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3128 - val_loss: -2.3452\n",
            "Epoch 9/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3341 - val_loss: -2.3535\n",
            "Epoch 10/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3358 - val_loss: -2.3601\n",
            "Epoch 11/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3179 - val_loss: -2.3653\n",
            "Epoch 12/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3301 - val_loss: -2.3712\n",
            "Epoch 13/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3137 - val_loss: -2.3757\n",
            "Epoch 14/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3509 - val_loss: -2.3803\n",
            "Epoch 15/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3425 - val_loss: -2.3843\n",
            "Epoch 16/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3926 - val_loss: -2.3892\n",
            "Epoch 17/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3808 - val_loss: -2.3923\n",
            "Epoch 18/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3553 - val_loss: -2.3962\n",
            "Epoch 19/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3948 - val_loss: -2.3997\n",
            "Epoch 20/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3741 - val_loss: -2.4036\n",
            "Epoch 21/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3730 - val_loss: -2.4060\n",
            "Epoch 22/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3897 - val_loss: -2.4083\n",
            "Epoch 23/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3852 - val_loss: -2.4112\n",
            "Epoch 24/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3835 - val_loss: -2.4146\n",
            "Epoch 25/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3845 - val_loss: -2.4167\n",
            "Epoch 26/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3794 - val_loss: -2.4189\n",
            "Epoch 27/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3841 - val_loss: -2.4207\n",
            "Epoch 28/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4146 - val_loss: -2.4234\n",
            "Epoch 29/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4043 - val_loss: -2.4243\n",
            "Epoch 30/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3726 - val_loss: -2.4252\n",
            "Epoch 31/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3892 - val_loss: -2.4263\n",
            "Epoch 32/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3707 - val_loss: -2.4273\n",
            "Epoch 33/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3878 - val_loss: -2.4276\n",
            "Epoch 34/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4014 - val_loss: -2.4293\n",
            "Epoch 35/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3798 - val_loss: -2.4298\n",
            "Epoch 36/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4367 - val_loss: -2.4307\n",
            "Epoch 37/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4084 - val_loss: -2.4319\n",
            "Epoch 38/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3964 - val_loss: -2.4309\n",
            "Epoch 39/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4117 - val_loss: -2.4330\n",
            "Epoch 40/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3962 - val_loss: -2.4332\n",
            "Epoch 41/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3899 - val_loss: -2.4330\n",
            "Epoch 42/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3979 - val_loss: -2.4350\n",
            "Epoch 43/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4004 - val_loss: -2.4347\n",
            "Epoch 44/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3950 - val_loss: -2.4362\n",
            "Epoch 45/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.3874 - val_loss: -2.4363\n",
            "Epoch 46/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4223 - val_loss: -2.4373\n",
            "Epoch 47/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4053 - val_loss: -2.4376\n",
            "Epoch 48/50\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: -2.3672 - val_loss: -2.4369\n",
            "Epoch 49/50\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: -2.4077 - val_loss: -2.4382\n",
            "Epoch 50/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.4014 - val_loss: -2.4386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability and\n",
        "## finding indexes of test_input_F_H matrix with the hij set that do not satisfy\n",
        "## constraint on the minimum SINR_P_min rate but satisfy the maximum transmit\n",
        "## power p_max\n",
        "\n",
        "test_input = [test_input_F_H_0dB, test_input_EsN0_0dB]\n",
        "# output_P_hat_temp = model.predict(test_input)\n",
        "output_P_hat_temp = np.multiply(p_max, model.predict(test_input))\n",
        "output_P_hat = output_P_hat_temp.reshape((output_P_hat_temp.shape[0], output_P_hat_temp.shape[1], 1)) # test_input_F_H_size X row X column\n",
        "output_P_hat_size = output_P_hat.shape[0]\n",
        "test_data_F_H_abs_sqr = cmplx_abs_sqr(test_data_F_H_0dB)\n",
        "\n",
        "indx_n = []\n",
        "count_v = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(output_P_hat[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(output_P_hat[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise_0dB[i] + ph - numr\n",
        "    SINR_out = np.divide(numr, dnumr)\n",
        "\n",
        "    if np.round(SINR_out, decimals= 3) < SINR_P_min[i]:\n",
        "      indx_n.append(k)\n",
        "      count_v = count_v + 1\n",
        "      # print(SINR_out)\n",
        "      break\n",
        "\n",
        "violation_prb = (count_v / output_P_hat_size) * 100\n",
        "print(\"Constraints Violation Probability: {:.2f}%\".format(violation_prb))\n",
        "# print(len(indx_n))\n",
        "# print(indx_n)"
      ],
      "metadata": {
        "id": "gJnXE2BKGK7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823557d3-c7af-4c08-cc1f-844ee653c324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 1ms/step\n",
            "Constraints Violation Probability: 97.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to calculate the average sum rate\n",
        "# Here, p_model is the output of DNN, and it is a 2D array.\n",
        "import math\n",
        "\n",
        "def average_sum_rate(hij, p_model, sigma_sqr_noise, K):\n",
        "  R = 0\n",
        "  hij_size = hij.shape[0]\n",
        "  hij_abs_sqr = cmplx_abs_sqr(hij)\n",
        "\n",
        "  for k in range(hij_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      phn = 0\n",
        "      for j in range(K):  # Total columns\n",
        "        phn_j = np.multiply(p_model[k,j], hij_abs_sqr[k,i,j])\n",
        "        phn = phn + phn_j\n",
        "\n",
        "      numr_s = np.multiply(p_model[k,i], hij_abs_sqr[k,i,i])\n",
        "      dnumr_s = sigma_sqr_noise[i] + phn - numr_s\n",
        "      R_temp = math.log2(1 + np.divide(numr_s, dnumr_s))\n",
        "      R = R + R_temp\n",
        "\n",
        "  return (R/hij_size)"
      ],
      "metadata": {
        "id": "qoO5u0E5GwVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the curated power vector p_tilda\n",
        "# p_tilda = test_input_p_hat when SINR_P_min is not met\n",
        "# p_tilda = output_P_hat when SINR_P_min is met\n",
        "\n",
        "p_tilda = np.empty((output_P_hat_size, K, 1), dtype = float, order = 'C')\n",
        "\n",
        "i = 0\n",
        "for j in range(output_P_hat_size):\n",
        "  if (i < len(indx_n)) and (j == indx_n[i]):\n",
        "    p_tilda[j] = (test_input_p_hat_0dB[j] * p_max) / np.amax(test_input_p_hat_0dB[j])\n",
        "    i = i + 1\n",
        "  else:\n",
        "    p_tilda[j] = output_P_hat[j]\n",
        "\n",
        "print(p_tilda.shape)\n",
        "# print(p_tilda)"
      ],
      "metadata": {
        "id": "ImkvmQJ-HAl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd072b9b-f9af-4173-f81a-2bf3e1260211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking p_tilda, i.e., the power for test_data_F_H for negative values\n",
        "## and Hit Rate i.e. percentage for 0 <= p_tilda <= p_max\n",
        "count_p_t = 0\n",
        "count_n_t = 0\n",
        "\n",
        "for n in range(output_P_hat_size):\n",
        "  P_max = np.amax(p_tilda[n])\n",
        "  if np.round(P_max, decimals = 3) <= 1:\n",
        "    count_p_t = count_p_t + 1\n",
        "\n",
        "  if np.any(p_tilda[n] < 0):\n",
        "    count_n_t = count_n_t + 1\n",
        "    print(n,'\\n')\n",
        "    print(p_tilda)\n",
        "\n",
        "p_tilda_hit_rate = (count_p_t / output_P_hat_size) * 100\n",
        "print(\"Hit Rate for Power p_tilda: {:.2f}%\".format(p_tilda_hit_rate))\n",
        "print(\"Negative power count: \", count_n_t)"
      ],
      "metadata": {
        "id": "rnjAMYoXHIh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bcb24b-6ebd-432b-f982-221c2431fc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rate for Power p_tilda: 100.00%\n",
            "Negative power count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability for p_tilda on the SINR_P_min\n",
        "# indx_t = []\n",
        "count_v_t = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(p_tilda[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(p_tilda[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise_0dB[i] + ph - numr\n",
        "    SINR_out_t = np.divide(numr, dnumr)\n",
        "\n",
        "    if np.round(SINR_out_t, decimals = 2) < SINR_P_min[i]:\n",
        "      # indx_t.append(k)\n",
        "      count_v_t = count_v_t + 1\n",
        "      break\n",
        "\n",
        "violation_prb_t = (count_v_t / output_P_hat_size) * 100\n",
        "print(\"SINR_P_min Constraints Violation Probability for p_tilda: {:.2f}%\".format(violation_prb_t))"
      ],
      "metadata": {
        "id": "0M97OZsMHPT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90d8a25-a8f3-47eb-c036-5a979fd95a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SINR_P_min Constraints Violation Probability for p_tilda: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DNN Sum Rate for test_data_F_H\n",
        "sumrate_F_H = average_sum_rate(test_data_F_H_0dB, p_tilda, sigma_sqr_noise_0dB, K)\n",
        "print(\"Average Sum Rate for all H matrices: {:.3f} Bit/Second/Hertz\".format(sumrate_F_H))"
      ],
      "metadata": {
        "id": "9n23SF6dG1in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8ca658-d872-4736-dcd8-bc57c566d9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sum Rate for all H matrices: 2.324 Bit/Second/Hertz\n"
          ]
        }
      ]
    }
  ]
}