{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3. Codes for analyzing the Proposed Model**"
      ],
      "metadata": {
        "id": "ljI2dKdWaNcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 For enhanced generalization capacity**"
      ],
      "metadata": {
        "id": "rOepIkQwaQre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0PufNuZTcK3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "## Number of transmitter-receiver pairs\n",
        "K = 8\n",
        "\n",
        "## Minimum rate for the achievable SINR of multiple concurrent transmissions\n",
        "SINR_P_min = np.array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "\n",
        "## Maximum transmit power\n",
        "p_max = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading a NumPy array from a CSV file\n",
        "# Loading F_H array from a CSV file\n",
        "from numpy import loadtxt\n",
        "\n",
        "## Reading an array from the file\n",
        "# If we want to read a file from our local drive, we have to first upload it to Collab's session storage.\n",
        "F_H_2D_L_0dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_0dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_10dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_10dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_20dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_20dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_30dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_30dB.csv', delimiter = ',', dtype = str)\n",
        "F_H_2D_L_40dB = np.loadtxt('/content/drive/MyDrive/F_H_2D_40dB.csv', delimiter = ',', dtype = str)\n",
        "\n",
        "# ## Reshaping the array from 2D to 3D\n",
        "F_H_3D_0dB = F_H_2D_L_0dB.reshape(F_H_2D_L_0dB.shape[0], F_H_2D_L_0dB.shape[1] // K, K)\n",
        "F_H_3D_10dB = F_H_2D_L_10dB.reshape(F_H_2D_L_10dB.shape[0], F_H_2D_L_10dB.shape[1] // K, K)\n",
        "F_H_3D_20dB = F_H_2D_L_20dB.reshape(F_H_2D_L_20dB.shape[0], F_H_2D_L_20dB.shape[1] // K, K)\n",
        "F_H_3D_30dB = F_H_2D_L_30dB.reshape(F_H_2D_L_30dB.shape[0], F_H_2D_L_30dB.shape[1] // K, K)\n",
        "F_H_3D_40dB = F_H_2D_L_40dB.reshape(F_H_2D_L_40dB.shape[0], F_H_2D_L_40dB.shape[1] // K, K)\n",
        "\n",
        "F_H_3D_0dB_size = F_H_3D_0dB.shape[0]\n",
        "F_H_3D_10dB_size = F_H_3D_10dB.shape[0]\n",
        "F_H_3D_20dB_size = F_H_3D_20dB.shape[0]\n",
        "F_H_3D_30dB_size = F_H_3D_30dB.shape[0]\n",
        "F_H_3D_40dB_size = F_H_3D_40dB.shape[0]"
      ],
      "metadata": {
        "id": "FakIovnzTqq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to convert string data to complex data and to remove the initial whitespace\n",
        "def cnvrt_2_cmplx_data(F_H_3D_size, F_H_3D):\n",
        "  F_H_list = []\n",
        "  for k in range(F_H_3D_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      for j in range(K):  # Total columns\n",
        "        F_H_temp = complex(F_H_3D[k][i][j].strip())\n",
        "        F_H_list.append(F_H_temp)\n",
        "  F_H_array = np.array(F_H_list)\n",
        "  F_H = F_H_array.reshape((F_H_3D_size, K, K)) # H_size X row X column_count\n",
        "  return F_H"
      ],
      "metadata": {
        "id": "jbek6jpGcUzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting string data to complex data and removing the initial whitespace\n",
        "F_H_0dB = cnvrt_2_cmplx_data(F_H_3D_0dB_size, F_H_3D_0dB)\n",
        "F_H_10dB = cnvrt_2_cmplx_data(F_H_3D_10dB_size, F_H_3D_10dB)\n",
        "F_H_20dB = cnvrt_2_cmplx_data(F_H_3D_20dB_size, F_H_3D_20dB)\n",
        "F_H_30dB = cnvrt_2_cmplx_data(F_H_3D_30dB_size, F_H_3D_30dB)\n",
        "F_H_40dB = cnvrt_2_cmplx_data(F_H_3D_40dB_size, F_H_3D_40dB)\n",
        "\n",
        "print(F_H_0dB.shape)\n",
        "print(F_H_10dB.shape)\n",
        "print(F_H_20dB.shape)\n",
        "print(F_H_30dB.shape)\n",
        "print(F_H_40dB.shape)\n",
        "\n",
        "F_H_0dB_size = F_H_0dB.shape[0]\n",
        "F_H_10dB_size = F_H_10dB.shape[0]\n",
        "F_H_20dB_size = F_H_20dB.shape[0]\n",
        "F_H_30dB_size = F_H_30dB.shape[0]\n",
        "F_H_40dB_size = F_H_40dB.shape[0]\n",
        "\n",
        "# print(F_H_0dB)\n",
        "# print(F_H_10dB)\n",
        "# print(F_H_20dB)\n",
        "# print(F_H_30dB)\n",
        "# print(F_H_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNlYDlIYeC5H",
        "outputId": "e7b19735-443c-4fe8-86e7-a60bad191f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numba as nb\n",
        "\n",
        "## Function to compute the square of the absolute value of an array of complex numbers\n",
        "@nb.vectorize([nb.float64(nb.complex128),nb.float32(nb.complex64)])\n",
        "def cmplx_abs_sqr(cmplx_var):\n",
        "  return cmplx_var.real**2 + cmplx_var.imag**2"
      ],
      "metadata": {
        "id": "sbOnVZQLfLFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the matrix A (K x K)\n",
        "def generate_A(F_H_size, K, SINR_P_min, F_H):\n",
        "  Aij_list = []\n",
        "  F_H_abs_sqr = cmplx_abs_sqr(F_H)\n",
        "\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      Aj_list =[]\n",
        "      for j in range(K): # Total columns\n",
        "        if i==j:\n",
        "          A = F_H_abs_sqr[k,i,j]\n",
        "        else:\n",
        "          A = np.multiply(-SINR_P_min[i], F_H_abs_sqr[k,i,j])\n",
        "        Aj_list.append(A)\n",
        "      Aij_list.append(Aj_list)\n",
        "  Aij_array = np.array(Aij_list)\n",
        "  Aij = Aij_array.reshape((F_H_size, K, K)) # H_size X row X column\n",
        "  return Aij"
      ],
      "metadata": {
        "id": "Z7_g8WkMl_ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A\n",
        "A_0dB = generate_A(F_H_0dB_size, K, SINR_P_min, F_H_0dB)\n",
        "A_10dB = generate_A(F_H_10dB_size, K, SINR_P_min, F_H_10dB)\n",
        "A_20dB = generate_A(F_H_20dB_size, K, SINR_P_min, F_H_20dB)\n",
        "A_30dB = generate_A(F_H_30dB_size, K, SINR_P_min, F_H_30dB)\n",
        "A_40dB = generate_A(F_H_40dB_size, K, SINR_P_min, F_H_40dB)\n",
        "\n",
        "print(A_0dB.shape)\n",
        "print(A_10dB.shape)\n",
        "print(A_20dB.shape)\n",
        "print(A_30dB.shape)\n",
        "print(A_40dB.shape)\n",
        "\n",
        "# print(A_0dB)\n",
        "# print(A_10dB)\n",
        "# print(A_20dB)\n",
        "# print(A_30dB)\n",
        "# print(A_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEuECq8YmGgq",
        "outputId": "0a666c29-84df-49a6-bcf2-f8cb1cc2b1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Variances for noise signals\n",
        "sigma_sqr_noise_0dB = np.array([1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0, 1e-0], dtype = float)\n",
        "sigma_sqr_noise_10dB = np.array([1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1], dtype = float)\n",
        "sigma_sqr_noise_20dB = np.array([1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2], dtype = float)\n",
        "sigma_sqr_noise_30dB = np.array([1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3], dtype = float)\n",
        "sigma_sqr_noise_40dB = np.array([1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4], dtype = float)"
      ],
      "metadata": {
        "id": "sIPa54XpmWm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate the vector b (K x 1)\n",
        "def generate_b(F_H_size, K, SINR_P_min, sigma_sqr_noise, F_H):\n",
        "  bi_list = []\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total rows, i.e., total transmitters\n",
        "      b = np.multiply(SINR_P_min[i], sigma_sqr_noise[i])\n",
        "      bi_list.append(b)\n",
        "  bi_array = np.array(bi_list)\n",
        "  bi = bi_array.reshape((F_H_size, K, 1)) # H_size X row X column\n",
        "  return bi"
      ],
      "metadata": {
        "id": "avVk-Onin9-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create vector b\n",
        "b_0dB = generate_b(F_H_0dB_size, K, SINR_P_min, sigma_sqr_noise_0dB, F_H_0dB)\n",
        "b_10dB = generate_b(F_H_10dB_size, K, SINR_P_min, sigma_sqr_noise_10dB, F_H_10dB)\n",
        "b_20dB = generate_b(F_H_20dB_size, K, SINR_P_min, sigma_sqr_noise_20dB, F_H_20dB)\n",
        "b_30dB = generate_b(F_H_30dB_size, K, SINR_P_min, sigma_sqr_noise_30dB, F_H_30dB)\n",
        "b_40dB = generate_b(F_H_40dB_size, K, SINR_P_min, sigma_sqr_noise_40dB, F_H_40dB)\n",
        "\n",
        "print(b_0dB.shape)\n",
        "print(b_10dB.shape)\n",
        "print(b_20dB.shape)\n",
        "print(b_30dB.shape)\n",
        "print(b_40dB.shape)\n",
        "\n",
        "# print(b_0dB)\n",
        "# print(b_10dB)\n",
        "# print(b_20dB)\n",
        "# print(b_30dB)\n",
        "# print(b_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVri8hToEZ3",
        "outputId": "74090ae6-d5b4-4ed8-fae2-19c4d8f2d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create matrix A_inv, i.e., the pseudo inverse of matrix A\n",
        "A_inv_0dB = np.linalg.pinv(A_0dB)\n",
        "A_inv_10dB = np.linalg.pinv(A_10dB)\n",
        "A_inv_20dB = np.linalg.pinv(A_20dB)\n",
        "A_inv_30dB = np.linalg.pinv(A_30dB)\n",
        "A_inv_40dB = np.linalg.pinv(A_40dB)\n",
        "\n",
        "A_inv_0dB[A_inv_0dB<0] = 0\n",
        "A_inv_10dB[A_inv_10dB<0] = 0\n",
        "A_inv_20dB[A_inv_20dB<0] = 0\n",
        "A_inv_30dB[A_inv_30dB<0] = 0\n",
        "A_inv_40dB[A_inv_40dB<0] = 0\n",
        "\n",
        "print(A_inv_0dB.shape)\n",
        "print(A_inv_10dB.shape)\n",
        "print(A_inv_20dB.shape)\n",
        "print(A_inv_30dB.shape)\n",
        "print(A_inv_40dB.shape)\n",
        "\n",
        "# print(A_inv_0dB)\n",
        "# print(A_inv_10dB)\n",
        "# print(A_inv_20dB)\n",
        "# print(A_inv_30dB)\n",
        "# print(A_inv_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SxTKchxo4hE",
        "outputId": "e58d18bb-0174-4711-db80-daea48eda954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n",
            "(250000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector p_hat = (A_inv x b)\n",
        "p_hat_0dB = np.matmul(A_inv_0dB, b_0dB)\n",
        "p_hat_10dB = np.matmul(A_inv_10dB, b_10dB)\n",
        "p_hat_20dB = np.matmul(A_inv_20dB, b_20dB)\n",
        "p_hat_30dB = np.matmul(A_inv_30dB, b_30dB)\n",
        "p_hat_40dB = np.matmul(A_inv_40dB, b_40dB)\n",
        "\n",
        "print(p_hat_0dB.shape)\n",
        "print(p_hat_10dB.shape)\n",
        "print(p_hat_20dB.shape)\n",
        "print(p_hat_30dB.shape)\n",
        "print(p_hat_40dB.shape)\n",
        "\n",
        "# print(p_hat_0dB)\n",
        "# print(p_hat_10dB)\n",
        "# print(p_hat_20dB)\n",
        "# print(p_hat_30dB)\n",
        "# print(p_hat_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol8hdK_Fpyd6",
        "outputId": "2191c827-d90b-4db2-bad2-8d98e4214231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert p_max_array to (K x 1) vector\n",
        "p_max_array = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype = float)\n",
        "p_max_vector = p_max_array.reshape((K, 1)) # row X column\n",
        "print(p_max_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYnWLXbWqWyE",
        "outputId": "2efd0989-4655-49fc-977e-a230caa0ea95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a vector X = (p_max_vector - p_hat)\n",
        "X_0dB = p_max_vector - p_hat_0dB\n",
        "X_10dB = p_max_vector - p_hat_10dB\n",
        "X_20dB = p_max_vector - p_hat_20dB\n",
        "X_30dB = p_max_vector - p_hat_30dB\n",
        "X_40dB = p_max_vector - p_hat_40dB\n",
        "\n",
        "print(X_0dB.shape)\n",
        "print(X_10dB.shape)\n",
        "print(X_20dB.shape)\n",
        "print(X_30dB.shape)\n",
        "print(X_40dB.shape)\n",
        "\n",
        "# print(X_0dB)\n",
        "# print(X_10dB)\n",
        "# print(X_20dB)\n",
        "# print(X_30dB)\n",
        "# print(X_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZkzKIfKqemx",
        "outputId": "a2db9089-e00b-4472-a20b-f8e4c4a55301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to generate a vector beta = MIN[(p_max_vector - p_hat) / A_inv_cv]\n",
        "def generate_beta(F_H_size, A_inv, X):\n",
        "  beta_list = []\n",
        "\n",
        "  for k in range(F_H_size):\n",
        "    for i in range(K):  # Total columns\n",
        "      ak = A_inv[k,:,i]\n",
        "      akr = ak.reshape((K, 1)) # row X column\n",
        "      with np.errstate(divide='ignore'):\n",
        "        beta_w = np.where(akr != 0.0, np.divide(X[k], akr), np.inf)\n",
        "        # beta_w = np.divide(X[k], akr)\n",
        "      # beta_w = np.divide(X[k], akr)\n",
        "      beta_min = np.amin(beta_w)\n",
        "      beta_list.append(beta_min)\n",
        "\n",
        "  beta_array = np.array(beta_list)\n",
        "  beta = beta_array.reshape((F_H_size, K, 1)) # H_size X row X column_count\n",
        "  return beta"
      ],
      "metadata": {
        "id": "pQvU5K3iq_xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate a vector beta = MIN[(p_max_vector - p_hat) / A_inv_cv]\n",
        "beta_0dB = generate_beta(F_H_0dB_size, A_inv_0dB, X_0dB)\n",
        "beta_10dB = generate_beta(F_H_10dB_size, A_inv_10dB, X_10dB)\n",
        "beta_20dB = generate_beta(F_H_20dB_size, A_inv_20dB, X_20dB)\n",
        "beta_30dB = generate_beta(F_H_30dB_size, A_inv_30dB, X_30dB)\n",
        "beta_40dB = generate_beta(F_H_40dB_size, A_inv_40dB, X_40dB)\n",
        "\n",
        "print(beta_0dB.shape)\n",
        "print(beta_10dB.shape)\n",
        "print(beta_20dB.shape)\n",
        "print(beta_30dB.shape)\n",
        "print(beta_40dB.shape)\n",
        "\n",
        "beta_0dB_size = beta_0dB.shape[0]\n",
        "beta_10dB_size = beta_10dB.shape[0]\n",
        "beta_20dB_size = beta_20dB.shape[0]\n",
        "beta_30dB_size = beta_30dB.shape[0]\n",
        "beta_40dB_size = beta_40dB.shape[0]\n",
        "\n",
        "# print(beta_0dB)\n",
        "# print(beta_10dB)\n",
        "# print(beta_20dB)\n",
        "# print(beta_30dB)\n",
        "# print(beta_40dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2qu-ZKnsbZK",
        "outputId": "9d8a3bf9-bb1c-465a-d10a-943b73545b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n",
            "(250000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split datasets for training, validation, and testing.\n",
        "def split(np_array):\n",
        "  # data_size = np_array.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_array.shape[1]\n",
        "  column_count = np_array.shape[2]\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = complex, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = complex, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_array[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_array[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_array[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "boVaWBGouGB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split F_H matrix\n",
        "F_H_S_0dB = split(F_H_0dB)\n",
        "train_input_F_H_0dB = F_H_S_0dB[0]\n",
        "valid_input_F_H_0dB = F_H_S_0dB[1]\n",
        "test_input_F_H_0dB = F_H_S_0dB[2]\n",
        "test_data_F_H_0dB = F_H_S_0dB[3]\n",
        "\n",
        "F_H_S_10dB = split(F_H_10dB)\n",
        "train_input_F_H_10dB = F_H_S_10dB[0]\n",
        "valid_input_F_H_10dB = F_H_S_10dB[1]\n",
        "test_input_F_H_10dB = F_H_S_10dB[2]\n",
        "test_data_F_H_10dB = F_H_S_10dB[3]\n",
        "\n",
        "F_H_S_20dB = split(F_H_20dB)\n",
        "train_input_F_H_20dB = F_H_S_20dB[0]\n",
        "valid_input_F_H_20dB = F_H_S_20dB[1]\n",
        "test_input_F_H_20dB = F_H_S_20dB[2]\n",
        "test_data_F_H_20dB = F_H_S_20dB[3]\n",
        "\n",
        "F_H_S_30dB = split(F_H_30dB)\n",
        "train_input_F_H_30dB = F_H_S_30dB[0]\n",
        "valid_input_F_H_30dB = F_H_S_30dB[1]\n",
        "test_input_F_H_30dB = F_H_S_30dB[2]\n",
        "test_data_F_H_30dB = F_H_S_30dB[3]\n",
        "\n",
        "F_H_S_40dB = split(F_H_40dB)\n",
        "train_input_F_H_40dB = F_H_S_40dB[0]\n",
        "valid_input_F_H_40dB = F_H_S_40dB[1]\n",
        "test_input_F_H_40dB = F_H_S_40dB[2]\n",
        "test_data_F_H_40dB = F_H_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Cr1GItuP0d",
        "outputId": "007ea307-ca84-4af6-a37a-82930f7d3c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split A_inv matrix\n",
        "A_inv_S_0dB = split(A_inv_0dB)\n",
        "train_input_A_inv_0dB = A_inv_S_0dB[0]\n",
        "valid_input_A_inv_0dB = A_inv_S_0dB[1]\n",
        "test_input_A_inv_0dB = A_inv_S_0dB[2]\n",
        "test_data_A_inv_0dB = A_inv_S_0dB[3]\n",
        "\n",
        "A_inv_S_10dB = split(A_inv_10dB)\n",
        "train_input_A_inv_10dB = A_inv_S_10dB[0]\n",
        "valid_input_A_inv_10dB = A_inv_S_10dB[1]\n",
        "test_input_A_inv_10dB = A_inv_S_10dB[2]\n",
        "test_data_A_inv_10dB = A_inv_S_10dB[3]\n",
        "\n",
        "A_inv_S_20dB = split(A_inv_20dB)\n",
        "train_input_A_inv_20dB = A_inv_S_20dB[0]\n",
        "valid_input_A_inv_20dB = A_inv_S_20dB[1]\n",
        "test_input_A_inv_20dB = A_inv_S_20dB[2]\n",
        "test_data_A_inv_20dB = A_inv_S_20dB[3]\n",
        "\n",
        "A_inv_S_30dB = split(A_inv_30dB)\n",
        "train_input_A_inv_30dB = A_inv_S_30dB[0]\n",
        "valid_input_A_inv_30dB = A_inv_S_30dB[1]\n",
        "test_input_A_inv_30dB = A_inv_S_30dB[2]\n",
        "test_data_A_inv_30dB = A_inv_S_30dB[3]\n",
        "\n",
        "A_inv_S_40dB = split(A_inv_40dB)\n",
        "train_input_A_inv_40dB = A_inv_S_40dB[0]\n",
        "valid_input_A_inv_40dB = A_inv_S_40dB[1]\n",
        "test_input_A_inv_40dB = A_inv_S_40dB[2]\n",
        "test_data_A_inv_40dB = A_inv_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0FDocSpvidk",
        "outputId": "9a7ca416-70ad-49f6-881c-f2eeda6b53d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n",
            "(200000, 8, 8) (25000, 8, 8) (25000, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split b vector\n",
        "b_S_0dB = split(b_0dB)\n",
        "train_input_b_0dB = b_S_0dB[0]\n",
        "valid_input_b_0dB = b_S_0dB[1]\n",
        "test_input_b_0dB = b_S_0dB[2]\n",
        "test_data_b_0dB = b_S_0dB[3]\n",
        "\n",
        "b_S_10dB = split(b_10dB)\n",
        "train_input_b_10dB = b_S_10dB[0]\n",
        "valid_input_b_10dB = b_S_10dB[1]\n",
        "test_input_b_10dB = b_S_10dB[2]\n",
        "test_data_b_10dB = b_S_10dB[3]\n",
        "\n",
        "b_S_20dB = split(b_20dB)\n",
        "train_input_b_20dB = b_S_20dB[0]\n",
        "valid_input_b_20dB = b_S_20dB[1]\n",
        "test_input_b_20dB = b_S_20dB[2]\n",
        "test_data_b_20dB = b_S_20dB[3]\n",
        "\n",
        "b_S_30dB = split(b_30dB)\n",
        "train_input_b_30dB = b_S_30dB[0]\n",
        "valid_input_b_30dB = b_S_30dB[1]\n",
        "test_input_b_30dB = b_S_30dB[2]\n",
        "test_data_b_30dB = b_S_30dB[3]\n",
        "\n",
        "b_S_40dB = split(b_40dB)\n",
        "train_input_b_40dB = b_S_40dB[0]\n",
        "valid_input_b_40dB = b_S_40dB[1]\n",
        "test_input_b_40dB = b_S_40dB[2]\n",
        "test_data_b_40dB = b_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u62wDoijvnBd",
        "outputId": "a8d47251-caea-4e95-d03f-a1abc804e9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split X vector\n",
        "X_S_0dB = split(X_0dB)\n",
        "train_input_X_0dB = X_S_0dB[0]\n",
        "valid_input_X_0dB = X_S_0dB[1]\n",
        "test_input_X_0dB = X_S_0dB[2]\n",
        "test_data_X_0dB = X_S_0dB[3]\n",
        "\n",
        "X_S_10dB = split(X_10dB)\n",
        "train_input_X_10dB = X_S_10dB[0]\n",
        "valid_input_X_10dB = X_S_10dB[1]\n",
        "test_input_X_10dB = X_S_10dB[2]\n",
        "test_data_X_10dB = X_S_10dB[3]\n",
        "\n",
        "X_S_20dB = split(X_20dB)\n",
        "train_input_X_20dB = X_S_20dB[0]\n",
        "valid_input_X_20dB = X_S_20dB[1]\n",
        "test_input_X_20dB = X_S_20dB[2]\n",
        "test_data_X_20dB = X_S_20dB[3]\n",
        "\n",
        "X_S_30dB = split(X_30dB)\n",
        "train_input_X_30dB = X_S_30dB[0]\n",
        "valid_input_X_30dB = X_S_30dB[1]\n",
        "test_input_X_30dB = X_S_30dB[2]\n",
        "test_data_X_30dB = X_S_30dB[3]\n",
        "\n",
        "X_S_40dB = split(X_40dB)\n",
        "train_input_X_40dB = X_S_40dB[0]\n",
        "valid_input_X_40dB = X_S_40dB[1]\n",
        "test_input_X_40dB = X_S_40dB[2]\n",
        "test_data_X_40dB = X_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bmtnsB0vpqL",
        "outputId": "0557c42c-c04a-48d4-b0a4-71eb563f6c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split beta vector\n",
        "beta_S_0dB = split(beta_0dB)\n",
        "train_input_beta_0dB = beta_S_0dB[0]\n",
        "valid_input_beta_0dB = beta_S_0dB[1]\n",
        "test_input_beta_0dB = beta_S_0dB[2]\n",
        "test_data_beta_0dB = beta_S_0dB[3]\n",
        "\n",
        "beta_S_10dB = split(beta_10dB)\n",
        "train_input_beta_10dB = beta_S_10dB[0]\n",
        "valid_input_beta_10dB = beta_S_10dB[1]\n",
        "test_input_beta_10dB = beta_S_10dB[2]\n",
        "test_data_beta_10dB = beta_S_10dB[3]\n",
        "\n",
        "beta_S_20dB = split(beta_20dB)\n",
        "train_input_beta_20dB = beta_S_20dB[0]\n",
        "valid_input_beta_20dB = beta_S_20dB[1]\n",
        "test_input_beta_20dB = beta_S_20dB[2]\n",
        "test_data_beta_20dB = beta_S_20dB[3]\n",
        "\n",
        "beta_S_30dB = split(beta_30dB)\n",
        "train_input_beta_30dB = beta_S_30dB[0]\n",
        "valid_input_beta_30dB = beta_S_30dB[1]\n",
        "test_input_beta_30dB = beta_S_30dB[2]\n",
        "test_data_beta_30dB = beta_S_30dB[3]\n",
        "\n",
        "beta_S_40dB = split(beta_40dB)\n",
        "train_input_beta_40dB = beta_S_40dB[0]\n",
        "valid_input_beta_40dB = beta_S_40dB[1]\n",
        "test_input_beta_40dB = beta_S_40dB[2]\n",
        "test_data_beta_40dB = beta_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYk2-v-pvt4-",
        "outputId": "ea0c9816-d11d-4f9f-bba9-f0a8376713f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split p_hat vector\n",
        "p_hat_S_0dB = split(p_hat_0dB)\n",
        "train_input_p_hat_0dB = p_hat_S_0dB[0]\n",
        "valid_input_p_hat_0dB = p_hat_S_0dB[1]\n",
        "test_input_p_hat_0dB = p_hat_S_0dB[2]\n",
        "test_data_p_hat_0dB = p_hat_S_0dB[3]\n",
        "\n",
        "p_hat_S_10dB = split(p_hat_10dB)\n",
        "train_input_p_hat_10dB = p_hat_S_10dB[0]\n",
        "valid_input_p_hat_10dB = p_hat_S_10dB[1]\n",
        "test_input_p_hat_10dB = p_hat_S_10dB[2]\n",
        "test_data_p_hat_10dB = p_hat_S_10dB[3]\n",
        "\n",
        "p_hat_S_20dB = split(p_hat_20dB)\n",
        "train_input_p_hat_20dB = p_hat_S_20dB[0]\n",
        "valid_input_p_hat_20dB = p_hat_S_20dB[1]\n",
        "test_input_p_hat_20dB = p_hat_S_20dB[2]\n",
        "test_data_p_hat_20dB = p_hat_S_20dB[3]\n",
        "\n",
        "p_hat_S_30dB = split(p_hat_30dB)\n",
        "train_input_p_hat_30dB = p_hat_S_30dB[0]\n",
        "valid_input_p_hat_30dB = p_hat_S_30dB[1]\n",
        "test_input_p_hat_30dB = p_hat_S_30dB[2]\n",
        "test_data_p_hat_30dB = p_hat_S_30dB[3]\n",
        "\n",
        "p_hat_S_40dB = split(p_hat_40dB)\n",
        "train_input_p_hat_40dB = p_hat_S_40dB[0]\n",
        "valid_input_p_hat_40dB = p_hat_S_40dB[1]\n",
        "test_input_p_hat_40dB = p_hat_S_40dB[2]\n",
        "test_data_p_hat_40dB = p_hat_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C9ubdzpvwwB",
        "outputId": "27c42eaf-0488-486e-ef8e-3fc9fa3ca33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n",
            "(200000, 8, 1) (25000, 8, 1) (25000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create EsN0 vector\n",
        "EsN0_array_0dB = np.full(shape = F_H_0dB_size, fill_value = 0, dtype = int)\n",
        "EsN0_array_10dB = np.full(shape = F_H_10dB_size, fill_value = 10, dtype = int)\n",
        "EsN0_array_20dB = np.full(shape = F_H_20dB_size, fill_value = 20, dtype = int)\n",
        "EsN0_array_30dB = np.full(shape = F_H_30dB_size, fill_value = 30, dtype = int)\n",
        "EsN0_array_40dB = np.full(shape = F_H_40dB_size, fill_value = 40, dtype = int)\n",
        "\n",
        "EsN0_vector_0dB = EsN0_array_0dB.reshape((F_H_0dB_size, 1)) # row X column\n",
        "EsN0_vector_10dB = EsN0_array_10dB.reshape((F_H_10dB_size, 1)) # row X column\n",
        "EsN0_vector_20dB = EsN0_array_20dB.reshape((F_H_20dB_size, 1)) # row X column\n",
        "EsN0_vector_30dB = EsN0_array_30dB.reshape((F_H_30dB_size, 1)) # row X column\n",
        "EsN0_vector_40dB = EsN0_array_40dB.reshape((F_H_40dB_size, 1)) # row X column\n",
        "\n",
        "print(EsN0_vector_0dB.shape)\n",
        "print(EsN0_vector_10dB.shape)\n",
        "print(EsN0_vector_20dB.shape)\n",
        "print(EsN0_vector_30dB.shape)\n",
        "print(EsN0_vector_40dB.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fqMqJpg1lN0",
        "outputId": "a06b32dd-399b-48b2-b505-0b58a51d892d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n",
            "(250000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to split EsN0 vector for training, validation, and testing.\n",
        "def split_EsN0(np_vector):\n",
        "  # data_size = np_vector.shape[0]\n",
        "  # train_data_size = int(data_size * 0.8)\n",
        "  # valid_data_size = int(data_size * 0.1)\n",
        "  # test_data_size = int(data_size * 0.1)\n",
        "\n",
        "  train_data_size = int(200000)\n",
        "  valid_data_size = int(25000)\n",
        "  test_data_size = int(25000)\n",
        "\n",
        "  train_e_indx = train_data_size\n",
        "  valid_e_indx = train_e_indx + valid_data_size\n",
        "  test_e_indx = valid_e_indx + test_data_size\n",
        "  test_data_size_n = test_e_indx - valid_e_indx\n",
        "\n",
        "  row_count = np_vector.shape[1]\n",
        "  column_count = 1\n",
        "\n",
        "  train_data = np.empty((train_data_size, row_count, column_count), dtype = int, order = 'C')\n",
        "  valid_data = np.empty((valid_data_size, row_count, column_count), dtype = int, order = 'C')\n",
        "  test_data = np.empty((test_data_size_n, row_count, column_count), dtype = int, order = 'C')\n",
        "\n",
        "  for i in range(train_e_indx):\n",
        "    train_data[i] = np_vector[i]\n",
        "\n",
        "  xv = 0\n",
        "  for j in range(train_e_indx, valid_e_indx):\n",
        "    valid_data[xv] = np_vector[j]\n",
        "    xv = xv + 1\n",
        "\n",
        "  xt = 0\n",
        "  for k in range(valid_e_indx, test_e_indx):\n",
        "    test_data[xt] = np_vector[k]\n",
        "    xt = xt + 1\n",
        "\n",
        "  # print(train_data.shape, valid_data.shape, test_data.shape)\n",
        "\n",
        "\n",
        "  ## Training input will be the absolute value\n",
        "  train_input = np.absolute(train_data)\n",
        "  valid_input = np.absolute(valid_data)\n",
        "  test_input = np.absolute(test_data)\n",
        "\n",
        "  print(train_input.shape, valid_input.shape, test_input.shape)\n",
        "\n",
        "  return [train_input, valid_input, test_input, test_data]"
      ],
      "metadata": {
        "id": "46_WBIU44DNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split EsN0 vector\n",
        "EsN0_S_0dB = split_EsN0(EsN0_vector_0dB)\n",
        "train_input_EsN0_0dB = EsN0_S_0dB[0]\n",
        "valid_input_EsN0_0dB = EsN0_S_0dB[1]\n",
        "test_input_EsN0_0dB = EsN0_S_0dB[2]\n",
        "test_data_EsN0_0dB = EsN0_S_0dB[3]\n",
        "\n",
        "EsN0_S_10dB = split_EsN0(EsN0_vector_10dB)\n",
        "train_input_EsN0_10dB = EsN0_S_10dB[0]\n",
        "valid_input_EsN0_10dB = EsN0_S_10dB[1]\n",
        "test_input_EsN0_10dB = EsN0_S_10dB[2]\n",
        "test_data_EsN0_10dB = EsN0_S_10dB[3]\n",
        "\n",
        "EsN0_S_20dB = split_EsN0(EsN0_vector_20dB)\n",
        "train_input_EsN0_20dB = EsN0_S_20dB[0]\n",
        "valid_input_EsN0_20dB = EsN0_S_20dB[1]\n",
        "test_input_EsN0_20dB = EsN0_S_20dB[2]\n",
        "test_data_EsN0_20dB = EsN0_S_20dB[3]\n",
        "\n",
        "EsN0_S_30dB = split_EsN0(EsN0_vector_30dB)\n",
        "train_input_EsN0_30dB = EsN0_S_30dB[0]\n",
        "valid_input_EsN0_30dB = EsN0_S_30dB[1]\n",
        "test_input_EsN0_30dB = EsN0_S_30dB[2]\n",
        "test_data_EsN0_30dB = EsN0_S_30dB[3]\n",
        "\n",
        "EsN0_S_40dB = split_EsN0(EsN0_vector_40dB)\n",
        "train_input_EsN0_40dB = EsN0_S_40dB[0]\n",
        "valid_input_EsN0_40dB = EsN0_S_40dB[1]\n",
        "test_input_EsN0_40dB = EsN0_S_40dB[2]\n",
        "test_data_EsN0_40dB = EsN0_S_40dB[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1ekZI663MbZ",
        "outputId": "fed32955-ab1d-4585-9d8b-8db0bb2b6cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n",
            "(200000, 1, 1) (25000, 1, 1) (25000, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating datasets for training\n",
        "train_input_F_H = np.concatenate((train_input_F_H_0dB, train_input_F_H_10dB,\n",
        "                                  train_input_F_H_20dB, train_input_F_H_30dB,\n",
        "                                  train_input_F_H_40dB,), axis=0)\n",
        "\n",
        "train_input_EsN0 = np.concatenate((train_input_EsN0_0dB, train_input_EsN0_10dB,\n",
        "                                   train_input_EsN0_20dB, train_input_EsN0_30dB,\n",
        "                                   train_input_EsN0_40dB), axis=0)\n",
        "\n",
        "train_input_A_inv = np.concatenate((train_input_A_inv_0dB, train_input_A_inv_10dB,\n",
        "                                    train_input_A_inv_20dB, train_input_A_inv_30dB,\n",
        "                                    train_input_A_inv_40dB), axis=0)\n",
        "\n",
        "train_input_X = np.concatenate((train_input_X_0dB, train_input_X_10dB,\n",
        "                                train_input_X_20dB, train_input_X_30dB,\n",
        "                                train_input_X_40dB), axis=0)\n",
        "\n",
        "train_input_beta = np.concatenate((train_input_beta_0dB, train_input_beta_10dB,\n",
        "                                   train_input_beta_20dB, train_input_beta_30dB,\n",
        "                                   train_input_beta_40dB), axis=0)\n",
        "\n",
        "train_input_p_hat = np.concatenate((train_input_p_hat_0dB, train_input_p_hat_10dB,\n",
        "                                    train_input_p_hat_20dB, train_input_p_hat_30dB,\n",
        "                                    train_input_p_hat_40dB), axis=0)\n",
        "\n",
        "print(train_input_F_H.shape)\n",
        "print(train_input_EsN0.shape)\n",
        "print(train_input_A_inv.shape)\n",
        "print(train_input_X.shape)\n",
        "print(train_input_p_hat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skR98dch-MhI",
        "outputId": "872e62f7-a677-4557-9f58-5e4f96fa963e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 8, 8)\n",
            "(1000000, 1, 1)\n",
            "(1000000, 8, 8)\n",
            "(1000000, 8, 1)\n",
            "(1000000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating datasets for validation\n",
        "valid_input_F_H = np.concatenate((valid_input_F_H_0dB, valid_input_F_H_10dB,\n",
        "                                  valid_input_F_H_20dB, valid_input_F_H_30dB,\n",
        "                                  valid_input_F_H_40dB,), axis=0)\n",
        "\n",
        "valid_input_EsN0 = np.concatenate((valid_input_EsN0_0dB, valid_input_EsN0_10dB,\n",
        "                                   valid_input_EsN0_20dB, valid_input_EsN0_30dB,\n",
        "                                   valid_input_EsN0_40dB), axis=0)\n",
        "\n",
        "valid_input_A_inv = np.concatenate((valid_input_A_inv_0dB, valid_input_A_inv_10dB,\n",
        "                                    valid_input_A_inv_20dB, valid_input_A_inv_30dB,\n",
        "                                    valid_input_A_inv_40dB), axis=0)\n",
        "\n",
        "valid_input_X = np.concatenate((valid_input_X_0dB, valid_input_X_10dB,\n",
        "                                valid_input_X_20dB, valid_input_X_30dB,\n",
        "                                valid_input_X_40dB), axis=0)\n",
        "\n",
        "valid_input_beta = np.concatenate((valid_input_beta_0dB, valid_input_beta_10dB,\n",
        "                                   valid_input_beta_20dB, valid_input_beta_30dB,\n",
        "                                   valid_input_beta_40dB), axis=0)\n",
        "\n",
        "valid_input_p_hat = np.concatenate((valid_input_p_hat_0dB, valid_input_p_hat_10dB,\n",
        "                                    valid_input_p_hat_20dB, valid_input_p_hat_30dB,\n",
        "                                    valid_input_p_hat_40dB), axis=0)\n",
        "\n",
        "print(valid_input_F_H.shape)\n",
        "print(valid_input_EsN0.shape)\n",
        "print(valid_input_A_inv.shape)\n",
        "print(valid_input_X.shape)\n",
        "print(valid_input_p_hat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liSFGiTpCwyo",
        "outputId": "d6073da3-a331-45f8-a981-1e3e3e71923a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000, 8, 8)\n",
            "(125000, 1, 1)\n",
            "(125000, 8, 8)\n",
            "(125000, 8, 1)\n",
            "(125000, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Shuffling the training datasets\n",
        "train_shuffler = np.random.permutation(len(train_input_F_H))\n",
        "train_input_F_H_shuffled = train_input_F_H[train_shuffler]\n",
        "train_input_EsN0_shuffled = train_input_EsN0[train_shuffler]\n",
        "train_input_A_inv_shuffled = train_input_A_inv[train_shuffler]\n",
        "train_input_X_shuffled = train_input_X[train_shuffler]\n",
        "train_input_beta_shuffled = train_input_beta[train_shuffler]\n",
        "train_input_p_hat_shuffled = train_input_p_hat[train_shuffler]"
      ],
      "metadata": {
        "id": "0FSYEzt_Fbod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Shuffling the validation datasets\n",
        "valid_shuffler = np.random.permutation(len(valid_input_F_H))\n",
        "valid_input_F_H_shuffled = valid_input_F_H[valid_shuffler]\n",
        "valid_input_EsN0_shuffled = valid_input_EsN0[valid_shuffler]\n",
        "valid_input_A_inv_shuffled = valid_input_A_inv[valid_shuffler]\n",
        "valid_input_X_shuffled = valid_input_X[valid_shuffler]\n",
        "valid_input_beta_shuffled = valid_input_beta[valid_shuffler]\n",
        "valid_input_p_hat_shuffled = valid_input_p_hat[valid_shuffler]"
      ],
      "metadata": {
        "id": "TtGS75iAHN72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Reshaping train_input_F_H_shuffled and adding train_input_EsN0_shuffled\n",
        "const = K*K\n",
        "len1 = train_input_F_H_shuffled.shape[0]\n",
        "train_input_F_H_shuffled_reshaped = train_input_F_H_shuffled.reshape((len1, 1, const)) # size X row X column\n",
        "train_y_true = np.concatenate((train_input_F_H_shuffled_reshaped, train_input_EsN0_shuffled), axis=2)\n",
        "print(train_y_true.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g7RPEkI1egG",
        "outputId": "e7131bde-6160-4556-b622-1ae4e29c9533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 1, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reshaping train_input_F_H_shuffled and adding train_input_EsN0_shuffled\n",
        "len2 = valid_input_F_H_shuffled.shape[0]\n",
        "valid_input_F_H_shuffled_reshaped = valid_input_F_H_shuffled.reshape((len2, 1, const)) # size X row X column\n",
        "valid_y_true = np.concatenate((valid_input_F_H_shuffled_reshaped, valid_input_EsN0_shuffled), axis=2)\n",
        "print(valid_y_true.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtGoJWDfJEcf",
        "outputId": "6d0b900f-44ea-4b72-938e-0597bccbf2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125000, 1, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the DNN model - The Functional API\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "## from tensorflow.keras import layers # shows warning\n",
        "from keras.api._v2.keras import layers\n",
        "from keras.layers import Input, concatenate, Lambda\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "hij_inputs = keras.Input(shape=(K,K), name = \"hij_inputs\")\n",
        "f1 = layers.Flatten(name = \"flatten_layer_hij\")(hij_inputs)\n",
        "\n",
        "EsN0_inputs = keras.Input(shape=(1,1), name = \"EsN0_inputs\")\n",
        "f2 = layers.Flatten(name = \"flatten_layer_EsN0\")(EsN0_inputs)\n",
        "\n",
        "concat_layers = concatenate([f1, f2])\n",
        "\n",
        "d1 = layers.Dense(2*K*K, activation=\"relu\", name = \"dense_layer_1\")(concat_layers)\n",
        "b1 = layers.BatchNormalization(name = \"batch_norm_layer_1\")(d1)\n",
        "\n",
        "d2 = layers.Dense(K*K, activation=\"relu\", name = \"dense_layer_2\")(b1)\n",
        "b2 = layers.BatchNormalization(name = \"batch_norm_layer_2\")(d2)\n",
        "\n",
        "# meu = layers.Dense(K, activation=\"relu\", name = \"meu\")(b2)\n",
        "meu = layers.Dense(K, activation=\"sigmoid\", name = \"meu\")(b2)\n",
        "\n",
        "A_inv_inputs = keras.Input(shape=(K,K), name = \"A_inv_inputs\")\n",
        "f3 = layers.Flatten(name = \"flatten_layer_A_inv\")(A_inv_inputs)\n",
        "\n",
        "X_inputs = keras.Input(shape=(K,1), name = \"X_inputs\")\n",
        "f4 = layers.Flatten(name = \"flatten_layer_X\")(X_inputs)\n",
        "\n",
        "beta_inputs = keras.Input(shape=(K,1), name = \"beta_inputs\")\n",
        "f5 = layers.Flatten(name = \"flatten_layer_beta\")(beta_inputs)\n",
        "\n",
        "p_hat_inputs = keras.Input(shape=(K,1), name = \"p_hat_inputs\")\n",
        "f6 = layers.Flatten(name = \"flatten_layer_p_hat\")(p_hat_inputs)\n",
        "\n",
        "def custom_layer(tensor):\n",
        "  t_A_inv = tensor[0]\n",
        "  t_X = tensor[1]\n",
        "  t_beta = tensor[2]\n",
        "  t_p_hat = tensor[3]\n",
        "  t_meu = tensor[4]\n",
        "\n",
        "  A_inv_cl = tf.reshape(t_A_inv[:,0:K*K], (-1,K,K))\n",
        "  X_cl = tf.reshape(t_X[:,0:K*1], (-1,K,1))\n",
        "  beta_cl = tf.reshape(t_beta[:,0:K*1], (-1,K,1))\n",
        "  p_hat_cl = tf.reshape(t_p_hat[:,0:K*1], (-1,K,1))\n",
        "  meu_cl = tf.reshape(t_meu[:,0:K*1], (-1,K,1))\n",
        "\n",
        "  meu_ewm = tf.math.multiply(beta_cl, meu_cl)\n",
        "\n",
        "  alpha_dnumr = tf.matmul(A_inv_cl, meu_ewm)\n",
        "  alpha_whole = tf.divide(X_cl, alpha_dnumr)\n",
        "  alpha = tf.reduce_min(alpha_whole, axis = 1, keepdims = True)\n",
        "  max_p = tf.constant([1.0])\n",
        "  alpha = tf.math.minimum(max_p, alpha)\n",
        "  meu_P = tf.multiply(meu_ewm, alpha)\n",
        "\n",
        "  Z_cl = tf.matmul(A_inv_cl, meu_P)\n",
        "  P_hat_cl = tf.add(p_hat_cl, Z_cl)\n",
        "  P_hat_cl_Norm = tf.math.divide(P_hat_cl, tf.reduce_max(P_hat_cl, axis = 1, keepdims = True))\n",
        "\n",
        "  # return P_hat_cl\n",
        "  return P_hat_cl_Norm\n",
        "\n",
        "lambda_layer = tf.keras.layers.Lambda(custom_layer, name=\"lambda_layer\")([f3, f4, f5, f6, meu])\n",
        "f7 = layers.Flatten(name = \"flatten_layer_output\")(lambda_layer)\n",
        "\n",
        "model = keras.Model(inputs = [hij_inputs, EsN0_inputs, A_inv_inputs, X_inputs, beta_inputs, p_hat_inputs], outputs = f7, name = \"functional_api\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mnUNEAIZOT",
        "outputId": "2e4bee9c-4c27-4fe4-fd68-b04407f0fba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_api\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " hij_inputs (InputLayer)        [(None, 8, 8)]       0           []                               \n",
            "                                                                                                  \n",
            " EsN0_inputs (InputLayer)       [(None, 1, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " flatten_layer_hij (Flatten)    (None, 64)           0           ['hij_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " flatten_layer_EsN0 (Flatten)   (None, 1)            0           ['EsN0_inputs[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 65)           0           ['flatten_layer_hij[0][0]',      \n",
            "                                                                  'flatten_layer_EsN0[0][0]']     \n",
            "                                                                                                  \n",
            " dense_layer_1 (Dense)          (None, 128)          8448        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_norm_layer_1 (BatchNorma  (None, 128)         512         ['dense_layer_1[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " dense_layer_2 (Dense)          (None, 64)           8256        ['batch_norm_layer_1[0][0]']     \n",
            "                                                                                                  \n",
            " A_inv_inputs (InputLayer)      [(None, 8, 8)]       0           []                               \n",
            "                                                                                                  \n",
            " X_inputs (InputLayer)          [(None, 8, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " beta_inputs (InputLayer)       [(None, 8, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " p_hat_inputs (InputLayer)      [(None, 8, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " batch_norm_layer_2 (BatchNorma  (None, 64)          256         ['dense_layer_2[0][0]']          \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " flatten_layer_A_inv (Flatten)  (None, 64)           0           ['A_inv_inputs[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_layer_X (Flatten)      (None, 8)            0           ['X_inputs[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_layer_beta (Flatten)   (None, 8)            0           ['beta_inputs[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_layer_p_hat (Flatten)  (None, 8)            0           ['p_hat_inputs[0][0]']           \n",
            "                                                                                                  \n",
            " meu (Dense)                    (None, 8)            520         ['batch_norm_layer_2[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_layer (Lambda)          (None, 8, 1)         0           ['flatten_layer_A_inv[0][0]',    \n",
            "                                                                  'flatten_layer_X[0][0]',        \n",
            "                                                                  'flatten_layer_beta[0][0]',     \n",
            "                                                                  'flatten_layer_p_hat[0][0]',    \n",
            "                                                                  'meu[0][0]']                    \n",
            "                                                                                                  \n",
            " flatten_layer_output (Flatten)  (None, 8)           0           ['lambda_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,992\n",
            "Trainable params: 17,608\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the model as a graph\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model.png\")"
      ],
      "metadata": {
        "id": "S5Qr0hmULdwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Display the input and output shapes of each layer\n",
        "# keras.utils.plot_model(model, \"Functional_API_Model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "a-AvOJ5LLmkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The customized loss function\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  # p = y_pred\n",
        "  p = tf.math.multiply(p_max, y_pred)\n",
        "\n",
        "  mtrx_elmnt = K*K\n",
        "  EsN0_val = y_true[0][0][mtrx_elmnt]\n",
        "  y_true_updt = y_true[:,:,:-1]\n",
        "\n",
        "  if EsN0_val < 10:\n",
        "    sigma_sqr_noise_lf = 1e-0\n",
        "  elif EsN0_val >= 10 and EsN0_val < 20:\n",
        "    sigma_sqr_noise_lf = 1e-1\n",
        "  elif EsN0_val >= 20 and EsN0_val < 30:\n",
        "    sigma_sqr_noise_lf = 1e-2\n",
        "  elif EsN0_val >= 30 and EsN0_val < 40:\n",
        "    sigma_sqr_noise_lf = 1e-3\n",
        "  else:\n",
        "    sigma_sqr_noise_lf = 1e-4\n",
        "\n",
        "  hij = tf.reshape(y_true_updt[:,0:K*K], (-1,K,K))\n",
        "  hij_abs_sqr = tf.math.square(tf.math.abs(hij))\n",
        "\n",
        "  R_P = 0.0\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0.0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = tf.math.multiply(p[:,j], hij_abs_sqr[:,i,j])\n",
        "      ph = tf.math.add(ph, ph_j)\n",
        "\n",
        "    numr = tf.math.multiply(p[:,i], hij_abs_sqr[:,i,i])\n",
        "    dnumr = tf.math.add(sigma_sqr_noise_lf, tf.math.subtract(ph, numr))\n",
        "    SINR_i = tf.math.divide(numr, dnumr)\n",
        "    R_P = tf.math.add(R_P, (tf.math.log(1 + SINR_i)/tf.math.log(2.0)))\n",
        "\n",
        "  loss = -R_P\n",
        "  loss = tf.reduce_mean(loss) # batch mean\n",
        "  return loss"
      ],
      "metadata": {
        "id": "S1Fg_oNSNmgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build and compile the DNN model\n",
        "## Training and Testing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "optA = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "# optA = tf.keras.optimizers.Adam(learning_rate = 0.0001, clipnorm=0.92)\n",
        "model.compile(optimizer = optA, loss = custom_loss)\n",
        "\n",
        "train_input = [train_input_F_H_shuffled, train_input_EsN0_shuffled, train_input_A_inv_shuffled,\n",
        "               train_input_X_shuffled, train_input_beta_shuffled, train_input_p_hat_shuffled]\n",
        "\n",
        "valid_input = [valid_input_F_H_shuffled, valid_input_EsN0_shuffled, valid_input_A_inv_shuffled,\n",
        "               valid_input_X_shuffled, valid_input_beta_shuffled, valid_input_p_hat_shuffled]\n",
        "\n",
        "history = model.fit(train_input, train_y_true, epochs = 50,\n",
        "                    validation_data = (valid_input, valid_y_true), batch_size = 1000)\n",
        "\n",
        "# plt.plot(history.epoch, history.history['loss'], color = \"blue\", label = \"Training\")\n",
        "# plt.plot(history.epoch, history.history['val_loss'], color=\"black\", label = \"Validation\")\n",
        "# plt.xlabel(\"epochs\")\n",
        "# plt.ylabel(\"loss\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPM90d9IOP5G",
        "outputId": "d7192c9f-6af9-4b3d-d45a-0466a0e61fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1000/1000 [==============================] - 14s 10ms/step - loss: -2.5855 - val_loss: -2.6343\n",
            "Epoch 2/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.6801 - val_loss: -2.6506\n",
            "Epoch 3/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.6860 - val_loss: -2.6549\n",
            "Epoch 4/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.6842 - val_loss: -2.6576\n",
            "Epoch 5/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7222 - val_loss: -2.6975\n",
            "Epoch 6/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7248 - val_loss: -2.7028\n",
            "Epoch 7/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7476 - val_loss: -2.7048\n",
            "Epoch 8/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7337 - val_loss: -2.7057\n",
            "Epoch 9/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7528 - val_loss: -2.7064\n",
            "Epoch 10/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7409 - val_loss: -2.7066\n",
            "Epoch 11/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7678 - val_loss: -2.7068\n",
            "Epoch 12/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7511 - val_loss: -2.7075\n",
            "Epoch 13/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7463 - val_loss: -2.7077\n",
            "Epoch 14/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7498 - val_loss: -2.7079\n",
            "Epoch 15/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7308 - val_loss: -2.7082\n",
            "Epoch 16/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7489 - val_loss: -2.7081\n",
            "Epoch 17/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7526 - val_loss: -2.7083\n",
            "Epoch 18/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7296 - val_loss: -2.7084\n",
            "Epoch 19/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7460 - val_loss: -2.7087\n",
            "Epoch 20/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7507 - val_loss: -2.7087\n",
            "Epoch 21/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7521 - val_loss: -2.7087\n",
            "Epoch 22/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7529 - val_loss: -2.7088\n",
            "Epoch 23/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7399 - val_loss: -2.7091\n",
            "Epoch 24/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7359 - val_loss: -2.7090\n",
            "Epoch 25/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.7534 - val_loss: -2.7091\n",
            "Epoch 26/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7604 - val_loss: -2.7091\n",
            "Epoch 27/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7344 - val_loss: -2.7093\n",
            "Epoch 28/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7509 - val_loss: -2.7094\n",
            "Epoch 29/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7539 - val_loss: -2.7094\n",
            "Epoch 30/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7394 - val_loss: -2.7095\n",
            "Epoch 31/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7427 - val_loss: -2.7095\n",
            "Epoch 32/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7471 - val_loss: -2.7096\n",
            "Epoch 33/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7428 - val_loss: -2.7095\n",
            "Epoch 34/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7403 - val_loss: -2.7098\n",
            "Epoch 35/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7483 - val_loss: -2.7097\n",
            "Epoch 36/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7362 - val_loss: -2.7098\n",
            "Epoch 37/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7363 - val_loss: -2.7099\n",
            "Epoch 38/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7519 - val_loss: -2.7099\n",
            "Epoch 39/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7667 - val_loss: -2.7099\n",
            "Epoch 40/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7387 - val_loss: -2.7100\n",
            "Epoch 41/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7458 - val_loss: -2.7101\n",
            "Epoch 42/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7544 - val_loss: -2.7101\n",
            "Epoch 43/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7577 - val_loss: -2.7101\n",
            "Epoch 44/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7543 - val_loss: -2.7101\n",
            "Epoch 45/50\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: -2.7451 - val_loss: -2.7101\n",
            "Epoch 46/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7512 - val_loss: -2.7103\n",
            "Epoch 47/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7500 - val_loss: -2.7103\n",
            "Epoch 48/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7454 - val_loss: -2.7103\n",
            "Epoch 49/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7425 - val_loss: -2.7105\n",
            "Epoch 50/50\n",
            "1000/1000 [==============================] - 8s 8ms/step - loss: -2.7531 - val_loss: -2.7105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Constraint violation probability and\n",
        "## finding indexes of test_input_F_H matrix with the hij set that do not satisfy\n",
        "## constraint on the minimum SINR_P_min rate but satisfy the maximum transmit\n",
        "## power p_max\n",
        "\n",
        "test_input = [test_input_F_H_0dB, test_input_EsN0_0dB, test_input_A_inv_0dB,\n",
        "              test_input_X_0dB, test_input_beta_0dB, test_input_p_hat_0dB]\n",
        "# output_P_hat_temp = model.predict(test_input)\n",
        "output_P_hat_temp = np.multiply(p_max, model.predict(test_input))\n",
        "output_P_hat = output_P_hat_temp.reshape((output_P_hat_temp.shape[0], output_P_hat_temp.shape[1], 1)) # test_input_F_H_size X row X column\n",
        "output_P_hat_size = output_P_hat.shape[0]\n",
        "test_data_F_H_abs_sqr = cmplx_abs_sqr(test_data_F_H_0dB)\n",
        "\n",
        "indx_n = []\n",
        "count_v = 0\n",
        "\n",
        "for k in range(output_P_hat_size):\n",
        "  for i in range(K):  # Total rows\n",
        "    ph = 0\n",
        "    for j in range(K):  # Total columns\n",
        "      ph_j = np.multiply(output_P_hat[k,j], test_data_F_H_abs_sqr[k,i,j])\n",
        "      ph = ph + ph_j\n",
        "\n",
        "    numr = np.multiply(output_P_hat[k,i], test_data_F_H_abs_sqr[k,i,i])\n",
        "    dnumr = sigma_sqr_noise_0dB[i] + ph - numr\n",
        "    SINR_out = np.divide(numr, dnumr)\n",
        "\n",
        "    if np.round(SINR_out, decimals= 3) < SINR_P_min[i]:\n",
        "      indx_n.append(k)\n",
        "      count_v = count_v + 1\n",
        "      # print(SINR_out)\n",
        "      break\n",
        "\n",
        "violation_prb = (count_v / output_P_hat_size) * 100\n",
        "print(\"Constraints Violation Probability: {:.2f}%\".format(violation_prb))\n",
        "# print(len(indx_n))\n",
        "# print(indx_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83sgNEevRLaM",
        "outputId": "04afee0b-f36c-49a3-efc8-4e20fe99d440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 1s 1ms/step\n",
            "Constraints Violation Probability: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to calculate the average sum rate\n",
        "# Here, p_model is the output of DNN, and it is a 2D array.\n",
        "import math\n",
        "\n",
        "def average_sum_rate(hij, p_model, sigma_sqr_noise, K):\n",
        "  R = 0\n",
        "  hij_size = hij.shape[0]\n",
        "  hij_abs_sqr = cmplx_abs_sqr(hij)\n",
        "\n",
        "  for k in range(hij_size):\n",
        "    for i in range(K):  # Total rows\n",
        "      phn = 0\n",
        "      for j in range(K):  # Total columns\n",
        "        phn_j = np.multiply(p_model[k,j], hij_abs_sqr[k,i,j])\n",
        "        phn = phn + phn_j\n",
        "\n",
        "      numr_s = np.multiply(p_model[k,i], hij_abs_sqr[k,i,i])\n",
        "      dnumr_s = sigma_sqr_noise[i] + phn - numr_s\n",
        "      R_temp = math.log2(1 + np.divide(numr_s, dnumr_s))\n",
        "      R = R + R_temp\n",
        "\n",
        "  return (R/hij_size)"
      ],
      "metadata": {
        "id": "qJ97mHqfSEit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DNN Sum Rate for test_data_F_H\n",
        "sumrate_F_H = average_sum_rate(test_data_F_H_0dB, output_P_hat, sigma_sqr_noise_0dB, K)\n",
        "print(\"Average Sum Rate for all H matrices: {:.3f} Bit/Second/Hertz\".format(sumrate_F_H))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01jLM48DSL8v",
        "outputId": "b5b1b895-cb72-4e18-8213-aba09352691b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sum Rate for all H matrices: 2.605 Bit/Second/Hertz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking (A_inv x b), i.e., the power for negative values\n",
        "count_n = 0\n",
        "for c in range(output_P_hat_size):\n",
        "  p_temp = np.matmul(test_input_A_inv_0dB[c], test_input_b_0dB[c])\n",
        "  if np.any(p_temp < 0):\n",
        "    count_n = count_n + 1\n",
        "    print(c,'\\n')\n",
        "    print(p_temp)\n",
        "\n",
        "print(count_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFsHYSdKSfgI",
        "outputId": "050bd301-6c7a-4197-d797-f44aa88bea3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking P_hat, i.e., the power for test_data_F_H for negative values\n",
        "## and Hit Rate i.e. percentage for 0 <= P_hat <= p_max\n",
        "count_p = 0\n",
        "count_n = 0\n",
        "\n",
        "for n in range(output_P_hat_size):\n",
        "  P_max = np.amax(output_P_hat[n])\n",
        "  if np.round(P_max, decimals = 3) <= 1:\n",
        "    count_p = count_p + 1\n",
        "\n",
        "  if np.any(output_P_hat[n] < 0):\n",
        "    count_n = count_n + 1\n",
        "    print(n,'\\n')\n",
        "    print(output_P_hat)\n",
        "\n",
        "p_hit_rate = (count_p / output_P_hat_size) * 100\n",
        "print(\"Htt Rate for Power : {:.2f}%\".format(p_hit_rate))\n",
        "print(\"Negative power count: \", count_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clAr6MktS9e3",
        "outputId": "e3f2e8f9-05db-4ae3-bddc-bc29980beb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Htt Rate for Power : 100.00%\n",
            "Negative power count:  0\n"
          ]
        }
      ]
    }
  ]
}